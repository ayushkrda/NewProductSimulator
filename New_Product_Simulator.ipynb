{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center> New Product Simulator <br>\n",
    "<center>To measure the impact of introducing new product using the attributes of existing product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================= CODE HEADER ==============================================================\n",
    "# CODE DESCRIPTION: Get the New Product sales by changing attributes of existing products\n",
    "# CONTACT: Mu Sigma Inc. 3400 Dundee Rd, Suite 160 Northbrook, IL â€“ 60062 | www.mu-sigma.com\n",
    "# LAST MODIFIED DATE: 06-12-2021\n",
    "\n",
    "# ======================================= INSTRUCTIONS  ==============================================================\n",
    "# 1. Run below 3 cells to get \"Import Data\" widget\n",
    "# 2. Click on \"Import Data\" widget to import data\n",
    "# 3. Select required features/filters to be changed using widgets provided in this tool & get the New Products Simulated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png img {\n",
       "    display: block;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "}\n",
       " \n",
       ".loader {\n",
       "  border: 5px solid #f3f3f3;\n",
       "  border-radius: 50%;\n",
       "  border-top: 5px solid teal;\n",
       "  border-right: 5px solid grey;\n",
       "  border-bottom: 5px solid maroon;\n",
       "  border-left: 5px solid tan;\n",
       "  width: 20px;\n",
       "  height: 20px;\n",
       "  -webkit-animation: spin 1s linear infinite;\n",
       "  animation: spin 1s linear infinite;\n",
       "  float: left;\n",
       "}\n",
       "\n",
       "@-webkit-keyframes spin {\n",
       "  0% { -webkit-transform: rotate(0deg); }\n",
       "  100% { -webkit-transform: rotate(360deg); }\n",
       "}\n",
       "\n",
       "@keyframes spin {\n",
       "  0% { transform: rotate(0deg); }\n",
       "  100% { transform: rotate(360deg); }\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setting the notebook output to be in the center and creating loading symbol\n",
    "\n",
    "from IPython.display import HTML, display, Markdown, clear_output\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".output_png img {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "}\n",
    " \n",
    ".loader {\n",
    "  border: 5px solid #f3f3f3;\n",
    "  border-radius: 50%;\n",
    "  border-top: 5px solid teal;\n",
    "  border-right: 5px solid grey;\n",
    "  border-bottom: 5px solid maroon;\n",
    "  border-left: 5px solid tan;\n",
    "  width: 20px;\n",
    "  height: 20px;\n",
    "  -webkit-animation: spin 1s linear infinite;\n",
    "  animation: spin 1s linear infinite;\n",
    "  float: left;\n",
    "}\n",
    "\n",
    "@-webkit-keyframes spin {\n",
    "  0% { -webkit-transform: rotate(0deg); }\n",
    "  100% { -webkit-transform: rotate(360deg); }\n",
    "}\n",
    "\n",
    "@keyframes spin {\n",
    "  0% { transform: rotate(0deg); }\n",
    "  100% { transform: rotate(360deg); }\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "table.dataframe td, table.dataframe th {\n",
       "    border: 2px  black solid !important;\n",
       "  color: black !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 2px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Importing libraries</h2></div>'))\n",
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime as datetime\n",
    "import matplotlib\n",
    "import matplotlib.ticker as mtick\n",
    "import io\n",
    "import joblib\n",
    "\n",
    "# Importing widgets\n",
    "try:\n",
    "  import ipywidgets as widgets\n",
    "  from ipywidgets import interactive, Layout, Button, Box,VBox,interactive_output\n",
    "except:\n",
    "  !pip install ipywidgets\n",
    "  import ipywidgets as widgets\n",
    "  from ipywidgets import interactive, Layout, Button, Box,VBox\n",
    "\n",
    "# Importing IPython\n",
    "try:\n",
    "  from IPython.display import Javascript, display\n",
    "except:\n",
    "  !pip install ipython\n",
    "  from IPython.display import Javascript, display, HTML\n",
    "\n",
    "# Importing sklearn\n",
    "try:\n",
    "  from sklearn.ensemble import RandomForestRegressor\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "except:\n",
    "  !pip install -U scikit-learn\n",
    "  from sklearn.ensemble import RandomForestRegressor\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import random\n",
    "from termcolor import colored\n",
    "from collections import Counter\n",
    "\n",
    "# Importing widgets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import Javascript, display\n",
    "def run_all(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index(), IPython.notebook.ncells()-32)'))\n",
    "\n",
    "# Converting into year and week number column\n",
    "from datetime import datetime\n",
    "def year_week(y, w):\n",
    "    return datetime.strptime(f'{y} {w} 7', '%G %V %u')\n",
    "\n",
    "clear_output()\n",
    "    \n",
    "# Import data widget    \n",
    "button = widgets.Button(description=\"Import Data\")\n",
    "button.on_click(run_all)\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide code cells\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(r\"\"\"<style id=hide>div.input{display:none;}</style><button type=\"button\"onclick=\"var myStyle = document.getElementById('hide').sheet;myStyle.insertRule('div.input{display:inherit !important;}', 0);\">Show/hide inputs</button>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Importing Dataset</h2></div>'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Reading the required files | Files processed in \"Seasonality Index/seasonality_index_calculation.R\" deliverable file\n",
    "seasonality_table = pd.read_csv(\"Seasonality_Index_data.csv\")\n",
    "#Reading the required files | Files from \"Data Treatment/UPC Data Treatment/UPC_Data_Treatment.R\" deliverable file\n",
    "data_all_UPC = pd.read_csv(\"Treated_data/Treated_data_UPC.csv\") \n",
    "#Reading the required files | Files from \"Data Treatment/Banner Product Data Treatment/POS_ADCAL_SPC_DataTreatment.R\" deliverable file\n",
    "data_all = pd.read_csv('Treated_data/Treated_data_BP.csv')\n",
    "#Reading the required files | Files from Coca Cola (present under \"New Product Simulator\" folder)\n",
    "covid_stages = pd.read_csv(\"Covid_Stages_data.csv\") \n",
    "new_product = pd.read_excel(\"Innovation_Product_Mapping.xlsx\")\n",
    "product_feat = pd.read_csv(\"Product_features_data.csv\")\n",
    "\n",
    "# Reading input files | Files processed in Jupyterhub for new product stages (present under \"New Product Simulator\" folder)\n",
    "new_prod_stage = pd.read_csv(\"New_Product_cat_stage.csv\")\n",
    "\n",
    "# Removing EVIAN Products as partnership with EVIAN product is over as informed by the client\n",
    "data_all = data_all.loc[~data_all['Product'].str.contains('EVIAN')]\n",
    "\n",
    "# Merging datasets to get product features\n",
    "data_all = pd.merge(data_all,product_feat,how = 'left',left_on = 'Product' , right_on = 'TPO_PRODUCT_GROUP')\n",
    "\n",
    "# Grouping UPC data at Product level\n",
    "data_all_UPC_grouped = data_all_UPC.groupby(['Region','Channel','Store.Chain','Banner','Product','Week']).agg({'UPC.PRODUCT':'nunique','UPC_FLAVOUR':'nunique','UPC_SWEETNER':'nunique','UPC_TYPE':'nunique'}).reset_index()\n",
    "\n",
    "# Changing column names\n",
    "data_all_UPC_grouped.rename(columns={'UPC.PRODUCT':'No_of_brands','UPC_FLAVOUR':'No_of_flavors','UPC_SWEETNER':'No_of_sweetners','UPC_TYPE':'No_of_types'}, inplace=True)\n",
    "\n",
    "# Getting Brand, Flavor, Sweetner, Type information in Product level data\n",
    "data_all = pd.merge(data_all, data_all_UPC_grouped, how='inner', on=['Region','Channel','Store.Chain','Banner','Product','Week'])\n",
    "\n",
    "# Files present in FTP server under \"New Product Simulator/Processed_data\" \n",
    "# Training data files\n",
    "combined_dataset_adcal_temp_EAST = pd.read_csv('Processed_data/combined_dataset_adcal_EAST.csv')\n",
    "combined_dataset_edv_temp_EAST = pd.read_csv('Processed_data/combined_dataset_edv_EAST.csv')\n",
    "combined_dataset_other_promo_temp_EAST = pd.read_csv('Processed_data/combined_dataset_other_promo_EAST.csv')\n",
    "combined_dataset_self_promo_temp_EAST = pd.read_csv('Processed_data/combined_dataset_self_promo_EAST.csv')\n",
    "combined_dataset_adcal_temp_ONTARIO = pd.read_csv('Processed_data/combined_dataset_adcal_ONTARIO.csv')\n",
    "combined_dataset_edv_temp_ONTARIO = pd.read_csv('Processed_data/combined_dataset_edv_ONTARIO.csv')\n",
    "combined_dataset_other_promo_temp_ONTARIO = pd.read_csv('Processed_data/combined_dataset_other_promo_ONTARIO.csv')\n",
    "combined_dataset_self_promo_temp_ONTARIO = pd.read_csv('Processed_data/combined_dataset_self_promo_ONTARIO.csv')\n",
    "combined_dataset_adcal_temp_QUEBEC = pd.read_csv('Processed_data/combined_dataset_adcal_QUEBEC.csv')\n",
    "combined_dataset_edv_temp_QUEBEC = pd.read_csv('Processed_data/combined_dataset_edv_QUEBEC.csv')\n",
    "combined_dataset_other_promo_temp_QUEBEC = pd.read_csv('Processed_data/combined_dataset_other_promo_QUEBEC.csv')\n",
    "combined_dataset_self_promo_temp_QUEBEC = pd.read_csv('Processed_data/combined_dataset_self_promo_QUEBEC.csv')\n",
    "combined_dataset_adcal_temp_WEST = pd.read_csv('Processed_data/combined_dataset_adcal_WEST.csv')\n",
    "combined_dataset_edv_temp_WEST = pd.read_csv('Processed_data/combined_dataset_edv_WEST.csv')\n",
    "combined_dataset_other_promo_temp_WEST = pd.read_csv('Processed_data/combined_dataset_other_promo_WEST.csv')\n",
    "combined_dataset_self_promo_temp_WEST = pd.read_csv('Processed_data/combined_dataset_self_promo_WEST.csv')\n",
    "\n",
    "# Files present in FTP server under \"New Product Simulator/Tool_Model_Objects_Net_Gain\" \n",
    "model_object_EAST_2019Q1 = joblib.load('Tool_Model_Objects_Net_Gain/RF_EAST_2019Q1.pkl')\n",
    "model_object_EAST_2019Q2 = joblib.load('Tool_Model_Objects_Net_Gain/RF_EAST_2019Q2.pkl')\n",
    "model_object_EAST_2019Q3 = joblib.load('Tool_Model_Objects_Net_Gain/RF_EAST_2019Q3.pkl')\n",
    "model_object_EAST_2019Q4 = joblib.load('Tool_Model_Objects_Net_Gain/RF_EAST_2019Q4.pkl')\n",
    "model_object_ONTARIO_2019Q1 = joblib.load('Tool_Model_Objects_Net_Gain/RF_ONTARIO_2019Q1.pkl')\n",
    "model_object_ONTARIO_2019Q2 = joblib.load('Tool_Model_Objects_Net_Gain/RF_ONTARIO_2019Q2.pkl')\n",
    "model_object_ONTARIO_2019Q3 = joblib.load('Tool_Model_Objects_Net_Gain/RF_ONTARIO_2019Q3.pkl')\n",
    "model_object_ONTARIO_2019Q4 = joblib.load('Tool_Model_Objects_Net_Gain/RF_ONTARIO_2019Q4.pkl')\n",
    "model_object_QUEBEC_2019Q1 = joblib.load('Tool_Model_Objects_Net_Gain/RF_QUEBEC_2019Q1.pkl')\n",
    "model_object_QUEBEC_2019Q2 = joblib.load('Tool_Model_Objects_Net_Gain/RF_QUEBEC_2019Q2.pkl')\n",
    "model_object_QUEBEC_2019Q3 = joblib.load('Tool_Model_Objects_Net_Gain/RF_QUEBEC_2019Q3.pkl')\n",
    "model_object_QUEBEC_2019Q4 = joblib.load('Tool_Model_Objects_Net_Gain/RF_QUEBEC_2019Q4.pkl')\n",
    "model_object_WEST_2019Q1 = joblib.load('Tool_Model_Objects_Net_Gain/RF_WEST_2019Q1.pkl')\n",
    "model_object_WEST_2019Q2 = joblib.load('Tool_Model_Objects_Net_Gain/RF_WEST_2019Q2.pkl')\n",
    "model_object_WEST_2019Q3 = joblib.load('Tool_Model_Objects_Net_Gain/RF_WEST_2019Q3.pkl')\n",
    "model_object_WEST_2019Q4 = joblib.load('Tool_Model_Objects_Net_Gain/RF_WEST_2019Q4.pkl')\n",
    "# Files present in FTP server under \"New Product Simulator/Tool_Model_Objects_Net_Gain\" \n",
    "model_object_EAST_2019Q1_v2 = joblib.load('Tool_Model_Objects/RF_EAST_2019Q1_v2.pkl')\n",
    "model_object_EAST_2019Q2_v2 = joblib.load('Tool_Model_Objects/RF_EAST_2019Q2_v2.pkl')\n",
    "model_object_EAST_2019Q3_v2 = joblib.load('Tool_Model_Objects/RF_EAST_2019Q3_v2.pkl')\n",
    "model_object_EAST_2019Q4_v2 = joblib.load('Tool_Model_Objects/RF_EAST_2019Q4_v2.pkl')\n",
    "model_object_ONTARIO_2019Q1_v2 = joblib.load('Tool_Model_Objects/RF_ONTARIO_2019Q1_v2.pkl')\n",
    "model_object_ONTARIO_2019Q2_v2 = joblib.load('Tool_Model_Objects/RF_ONTARIO_2019Q2_v2.pkl')\n",
    "model_object_ONTARIO_2019Q3_v2 = joblib.load('Tool_Model_Objects/RF_ONTARIO_2019Q3_v2.pkl')\n",
    "model_object_ONTARIO_2019Q4_v2 = joblib.load('Tool_Model_Objects/RF_ONTARIO_2019Q4_v2.pkl')\n",
    "model_object_QUEBEC_2019Q1_v2 = joblib.load('Tool_Model_Objects/RF_QUEBEC_2019Q1_v2.pkl')\n",
    "model_object_QUEBEC_2019Q2_v2 = joblib.load('Tool_Model_Objects/RF_QUEBEC_2019Q2_v2.pkl')\n",
    "model_object_QUEBEC_2019Q3_v2 = joblib.load('Tool_Model_Objects/RF_QUEBEC_2019Q3_v2.pkl')\n",
    "model_object_QUEBEC_2019Q4_v2 = joblib.load('Tool_Model_Objects/RF_QUEBEC_2019Q4_v2.pkl')\n",
    "model_object_WEST_2019Q1_v2 = joblib.load('Tool_Model_Objects/RF_WEST_2019Q1_v2.pkl')\n",
    "model_object_WEST_2019Q2_v2 = joblib.load('Tool_Model_Objects/RF_WEST_2019Q2_v2.pkl')\n",
    "model_object_WEST_2019Q3_v2 = joblib.load('Tool_Model_Objects/RF_WEST_2019Q3_v2.pkl')\n",
    "model_object_WEST_2019Q4_v2 = joblib.load('Tool_Model_Objects/RF_WEST_2019Q4_v2.pkl')\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Choose New Product Attributes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Loading Filters</h2></div>'))\n",
    "\n",
    "# Defining variables for prediction\n",
    "Region_List = ['ONTARIO','QUEBEC','EAST','WEST']\n",
    "Max_POS = \"2019-52\" # Restricting the analysis for 2019 year\n",
    "holidayflag = \"Yes\"\n",
    "\n",
    "# Updating the dependent column\n",
    "data_all[\"Eq.Unit.Sales\"] = data_all[\"Std.Unit.Sales\"]\n",
    "\n",
    "# Selecting variables for training\n",
    "if holidayflag == \"Yes\":\n",
    "    ## Revenue Change    \n",
    "    cols = [\"Dollar.Sales\",\"Region\",\"Week.ID\",\"Week\",\"Year\",\"brand\",\"Banner\",\"Product\",\"Channel\",\"Store.Chain\",\"Category\",\"Pack.Subtype\",\"Adcal_Price\",\"EDV.Price\",\"Adcal_DD\",\"Week.Name\",\"Eq.Unit.Sales\",\"Pantry1\",\"Pantry2\",\"Pantry3\",\"Pre.Holiday.Week\",\"Holiday.Week\",\"Post.Holiday.Week\",\"Front.Page\", \"Middle.Page\",\"Back.Page\",\"Brand\",'PACK_CONTENT','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types']\n",
    "    Volume_dataset_all_reg = data_all[cols]\n",
    "\n",
    "# Getting seasonality and covid information\n",
    "Volume_dataset_all_reg = Volume_dataset_all_reg.merge(seasonality_table)\n",
    "Volume_dataset_all_reg = Volume_dataset_all_reg.merge(covid_stages, how=\"left\")\n",
    "\n",
    "# Replacing nulls in Corona stages\n",
    "Volume_dataset_all_reg.loc[Volume_dataset_all_reg[\"Corona_Stages\"].isnull(),\"Corona_Stages\"] = \"No Stage\"\n",
    "Volume_dataset_all_reg = Volume_dataset_all_reg.loc[Volume_dataset_all_reg[\"Adcal_Price\"] > 0]\n",
    "\n",
    "# Defining model train and test periods\n",
    "\n",
    "Test_week_list = {\"2019Q1\" : [\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-10\",\"2019-11\",\"2019-12\",\"2019-13\"],\n",
    "     \"2019Q2\" : [\"2019-14\",\"2019-15\",\"2019-16\",\"2019-17\",\"2019-18\",\"2019-19\",\"2019-20\",\"2019-21\",\"2019-22\",\"2019-23\",\"2019-24\",\"2019-25\",\"2019-26\"],\n",
    "     \"2019Q3\" : [\"2019-27\",\"2019-28\",\"2019-29\",\"2019-30\",\"2019-31\",\"2019-32\",\"2019-33\",\"2019-34\",\"2019-35\",\"2019-36\",\"2019-37\",\"2019-38\",\"2019-39\"],\n",
    "     \"2019Q4\" : [\"2019-40\",\"2019-41\",\"2019-42\",\"2019-43\",\"2019-44\",\"2019-45\",\"2019-46\",\"2019-47\",\"2019-48\",\"2019-49\",\"2019-50\",\"2019-51\",\"2019-52\"],\n",
    "                  \n",
    "     \"2020Q1\" : [\"2020-01\",\"2020-02\",\"2020-03\",\"2020-04\",\"2020-05\",\"2020-06\",\"2020-07\",\"2020-08\",\"2020-09\",\"2020-10\",\"2020-11\",\"2020-12\",\"2020-13\"],\n",
    "     \"2020Q2\" : [\"2020-14\",\"2020-15\",\"2020-16\",\"2020-17\",\"2020-18\",\"2020-19\",\"2020-20\",\"2020-21\",\"2020-22\",\"2020-23\",\"2020-24\",\"2020-25\",\"2020-26\"],\n",
    "     \"2020Q3\" : [\"2020-27\",\"2020-28\",\"2020-29\",\"2020-30\",\"2020-31\",\"2020-32\",\"2020-33\",\"2020-34\",\"2020-35\",\"2020-36\",\"2020-37\",\"2020-38\",\"2020-39\"],\n",
    "     \"2020Q4\" : [\"2020-40\",\"2020-41\",\"2020-42\",\"2020-43\",\"2020-44\",\"2020-45\",\"2020-46\",\"2020-47\",\"2020-48\",\"2020-49\",\"2020-50\",\"2020-51\",\"2020-52\"],\n",
    "     \"fulltrain\" : []}\n",
    "\n",
    "Test_periods = [\"2019Q4\",\"2019Q1\",\"2019Q2\",\"2019Q3\",\"2020Q4\",\"2020Q1\",\"2020Q2\",\"2020Q3\"]\n",
    "\n",
    "\n",
    "# Grouping category\n",
    "Volume_dataset_all_reg[\"Category\"] = [\"ALTERNATIVE BEVERAGES\" if ((x==\"DROPS\") or (x==\"ENERGY\") or (x==\"ISOTONICS\") or (x==\"NON CARBONATED BEVERAGES\"))\n",
    "                                 else x for x in Volume_dataset_all_reg[\"Category\"]]\n",
    "\n",
    "# Adding Test period column\n",
    "Volume_dataset_all_reg[\"Test_period\"] = [\"2019Q1\" if ((x >= '2019-01') & (x <= '2019-13')) else \"2019Q2\" if ((x >= '2019-14') & (x <= '2019-26')) else \"2019Q3\" if ((x >= '2019-27') & (x <= '2019-39')) else \"2019Q4\" if ((x >= '2019-40') & (x <= '2019-52')) else \n",
    "                                        \"2020Q1\" if ((x >= '2020-01') & (x <= '2020-13')) else \"2020Q2\" if ((x >= '2020-14') & (x <= '2020-26')) else \"2020Q3\" if ((x >= '2020-27') & (x <= '2020-39')) else \"2020Q4\" if ((x >= '2020-40') & (x <= '2020-52')) else\n",
    "                                        \"2017Q1\" if ((x >= '2017-01') & (x <= '2017-13')) else \"2017Q2\" if ((x >= '2017-14') & (x <= '2017-26')) else \"2017Q3\" if ((x >= '2017-27') & (x <= '2017-39')) else \"2017Q4\" if ((x >= '2017-40') & (x <= '2017-52')) else \n",
    "                                        \"2018Q1\" if ((x >= '2018-01') & (x <= '2018-13')) else \"2018Q2\" if ((x >= '2018-14') & (x <= '2018-26')) else \"2018Q3\" if ((x >= '2018-27') & (x <= '2018-39')) else \"2018Q4\" if ((x >= '2018-40') & (x <= '2018-52')) else \n",
    "                                        \"2021Q1\" if ((x >= '2021-01') & (x <= '2021-13')) else \"2021Q2\" if ((x >= '2021-14') & (x <= '2021-26')) else \"2021Q3\" if ((x >= '2021-27') & (x <= '2021-39')) else \"2021Q4\" if ((x >= '2021-40') & (x <= '2021-52')) else x \n",
    "                                        for x in Volume_dataset_all_reg[\"Week\"]]\n",
    "\n",
    "# CORE POWER 414 ML BTTL has duplicate row entries\n",
    "Volume_dataset_all_reg_11 = Volume_dataset_all_reg[Volume_dataset_all_reg['Product']=='TCCC CORE POWER 414 ML BTTL'].drop_duplicates().copy()\n",
    "Volume_dataset_all_reg = Volume_dataset_all_reg[Volume_dataset_all_reg['Product']!='TCCC CORE POWER 414 ML BTTL']\n",
    "Volume_dataset_all_reg = Volume_dataset_all_reg.append(Volume_dataset_all_reg_11, ignore_index = True)\n",
    "\n",
    "################# NEW PRODUCT 1 #################\n",
    "\n",
    "# Filters\n",
    "layout1 = widgets.Layout(width='280px', height='auto')\n",
    "layout3 = widgets.Layout(width='240px', height='auto')\n",
    "layout2 = widgets.Layout(width='106px', height='auto')\n",
    "layout_auto = widgets.Layout(width='auto', height='auto')\n",
    "\n",
    "# Dropdown widgets for choosing new product features\n",
    "region = widgets.Dropdown(options=list(np.sort(Volume_dataset_all_reg.Region.unique())) + ['All'], value='ONTARIO', description='Region', layout = layout3)\n",
    "channel = widgets.Dropdown(options=list(np.sort(Volume_dataset_all_reg.Channel.unique())) + ['All'], value='All', description='Channel', layout = layout3)\n",
    "size = widgets.BoundedFloatText(value=340, min = 50, max = 2630, description='New Size(ml)',disabled=False,layout = layout3) # Max & Min values are taken as per existing product data\n",
    "count = widgets.BoundedFloatText(value=8, min = 1, max = 32, description='New Count',disabled=False,layout = layout3)  # Max & Min values are taken as per existing product data\n",
    "size2 = widgets.BoundedFloatText(value=340, min = 50, max = 2630, description='New Size(ml)',disabled=False,layout = layout3)  # Max & Min values are taken as per existing product data\n",
    "count2 = widgets.BoundedFloatText(value=8, min = 1, max = 32, description='New Count',disabled=False,layout = layout3)  # Max & Min values are taken as per existing product data\n",
    "\n",
    "# New Category dropdown\n",
    "category_drop = widgets.Dropdown(options=list(np.sort(Volume_dataset_all_reg.Category.unique())), value='CARBONATED SOFT DRINKS',layout = layout_auto)\n",
    "category_drop2 = widgets.Dropdown(options=list(np.sort(Volume_dataset_all_reg.Category.unique())), value='CARBONATED SOFT DRINKS',layout = layout_auto)\n",
    "\n",
    "# New pack subtype dropdown\n",
    "pack_subtype_drop = widgets.Dropdown(options=list(np.sort(Volume_dataset_all_reg['Pack.Subtype'].unique())),layout = layout_auto)\n",
    "pack_subtype_drop2 = widgets.Dropdown(options=list(np.sort(Volume_dataset_all_reg['Pack.Subtype'].unique())),layout = layout_auto)\n",
    "\n",
    "# New pack content dropdown\n",
    "pack_content_drop = widgets.Dropdown(options=list(np.sort(Volume_dataset_all_reg['PACK_CONTENT'].unique())),layout = layout_auto)\n",
    "pack_content_drop2 = widgets.Dropdown(options=list(np.sort(Volume_dataset_all_reg['PACK_CONTENT'].unique())),layout = layout_auto)\n",
    "\n",
    "# Reactive filter for product\n",
    "def product_filter(Region_filter,Channel_filter):\n",
    "    if Channel_filter == 'All':\n",
    "        channel_list_prod = Volume_dataset_all_reg.Channel.unique()\n",
    "    else:\n",
    "        channel_list_prod = [Channel_filter]\n",
    "\n",
    "    if Region_filter == 'All':\n",
    "        Region_List_prod = Volume_dataset_all_reg.Region.unique()\n",
    "    else:\n",
    "        Region_List_prod = [Region_filter] \n",
    "\n",
    "    # Filtering for chosen region-channel\n",
    "    try:         \n",
    "        product = widgets.Dropdown(options = list(np.sort(Volume_dataset_all_reg[(Volume_dataset_all_reg.Region.isin(Region_List_prod)) & (Volume_dataset_all_reg.Channel.isin(channel_list_prod))].Product.unique())))\n",
    "        display(product)\n",
    "        return product\n",
    "    \n",
    "    except:\n",
    "        print('Product not found in this Region-Channel')\n",
    "\n",
    "reg_chan_prod = interactive(product_filter, Region_filter=region, Channel_filter = channel)\n",
    "reg_chan_prod.update()\n",
    "\n",
    "# Quarter filters\n",
    "start_week = widgets.HBox([widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\" \"),widgets.Label(value=\" \"),widgets.Label(value=\" \"),widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\"Start Quarter\"),widgets.Dropdown(options=['2019Q1','2019Q2','2019Q3','2019Q4'],value='2019Q1',layout = layout2)])\n",
    "end_week = widgets.HBox([widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\" \"),widgets.Label(value=\" \"),widgets.Label(value=\" \"),widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\"End Quarter\"),widgets.Dropdown(options=['2019Q1','2019Q2','2019Q3','2019Q4'],value='2019Q4', layout = layout2)])\n",
    "\n",
    "# Display filters\n",
    "# Bordering filters\n",
    "clear_output()\n",
    "box_layout = Layout(display='flex',flex_flow='rows',align_items='stretch',border='solid 2px',width='auto')\n",
    "\n",
    "# Ensemble or normal view\n",
    "toggle_ensemble_normal = widgets.ToggleButtons(options=['Multiple Product','Single Product'],value='Single Product',description='Do you want to use single product or multiple product as existing model pack?',style= {'description_width': 'initial'})\n",
    "out = widgets.Output(layout=widgets.Layout(border = 'solid 2px'))\n",
    "\n",
    "# Refresh widget\n",
    "def run_all(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.ncells()-32)'))\n",
    "layout_sim = widgets.Layout(width='auto', height='40px')\n",
    "button_sim = widgets.Button(description=\"Refresh manual filtering/ similarity score filtering\", layout = layout_sim)\n",
    "button_sim.on_click(run_all)\n",
    "\n",
    "# Widget for single, multiple product as existing product\n",
    "def fun(obj):\n",
    "    if toggle_ensemble_normal.value == 'Single Product':          \n",
    "        clear_output()\n",
    "        display(toggle_ensemble_normal)\n",
    "        display(VBox([widgets.HBox((reg_chan_prod.children[0], reg_chan_prod.children[1],widgets.HBox((widgets.Label(value = ' '), widgets.Label(value = ' '), widgets.Label(value = ' '),widgets.Label(value = ' '),widgets.Label(value = 'Model Pack'), reg_chan_prod.children[-1])))), widgets.Label(value=\"$New Product 1$\"),widgets.HBox((widgets.Label(value=\"New Category\"), category_drop,widgets.Label(value=\"New Pack Subtype\"), pack_subtype_drop,widgets.Label(value=\"New Pack Content\"), pack_content_drop)),widgets.HBox((count, size,start_week,end_week))], layout=box_layout))\n",
    "        display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.ncells()-32)'))\n",
    "    else:\n",
    "        clear_output()\n",
    "        display(toggle_ensemble_normal)\n",
    "        display(VBox([widgets.HBox((reg_chan_prod.children[0], reg_chan_prod.children[1],widgets.HBox((widgets.Label(value = ' '), widgets.Label(value = ' '), widgets.Label(value = ' '),widgets.Label(value = ' '))))), widgets.Label(value=\"$New Product 1$\"),widgets.HBox((widgets.Label(value=\"New Category\"), category_drop,widgets.Label(value=\"New Pack Subtype\"), pack_subtype_drop,widgets.Label(value=\"New Pack Content\"), pack_content_drop)),widgets.HBox((count, size,start_week,end_week))], layout=box_layout))\n",
    "        display(button_sim)\n",
    "        display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.ncells()-32)'))\n",
    "        \n",
    "display(toggle_ensemble_normal)\n",
    "# Normal view\n",
    "display(VBox([widgets.HBox((reg_chan_prod.children[0], reg_chan_prod.children[1],widgets.HBox((widgets.Label(value = ' '), widgets.Label(value = ' '), widgets.Label(value = ' '),widgets.Label(value = ' '),widgets.Label(value = 'Model Pack'), reg_chan_prod.children[-1])))), widgets.Label(value=\"$New Product 1$\"),widgets.HBox((widgets.Label(value=\"New Category\"), category_drop,widgets.Label(value=\"New Pack Subtype\"), pack_subtype_drop,widgets.Label(value=\"New Pack Content\"), pack_content_drop)),widgets.HBox((count, size,start_week,end_week))], layout=box_layout))\n",
    "toggle_ensemble_normal.observe(fun, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Storing filter output\n",
    "region_fil = region.value\n",
    "channel_fil = channel.value\n",
    "product_fil = reg_chan_prod.result.value\n",
    "size_fil = size.value\n",
    "count_fil = count.value\n",
    "size_fil2 = size2.value\n",
    "count_fil2 = count2.value\n",
    "\n",
    "# Initialising list for all/specific channel, region\n",
    "if channel.value == 'All':\n",
    "    channel_list = Volume_dataset_all_reg.Channel.unique()\n",
    "else:\n",
    "    channel_list = [channel.value]\n",
    "    \n",
    "if region.value == 'All':\n",
    "    Region_List = Volume_dataset_all_reg.Region.unique()\n",
    "else:\n",
    "    Region_List = [region.value]   \n",
    "    \n",
    "# Run prices cell\n",
    "def run_all(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index() + 1,IPython.notebook.ncells()-27)'))\n",
    "\n",
    "layout = widgets.Layout(width='auto', height='40px')\n",
    "button_check = widgets.Button(description=\"Get Existing Prices & Discounts\", layout = layout)\n",
    "button_check.on_click(run_all)\n",
    "\n",
    "#######################################################################################################################    \n",
    "                        # Model Pack selection for ensemble model\n",
    "#######################################################################################################################    \n",
    "\n",
    "def similarity_score_calculation():\n",
    "    # Checking existing product region, channel & its presence in more than 1 channel    \n",
    "    required_volume_dataset = Volume_dataset_all_reg[(Volume_dataset_all_reg.Region.isin(Region_List)) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value) & (Volume_dataset_all_reg.Category == category_drop.value)][[\"Product\",\"Channel\",\"Category\",\"Pack.Subtype\", 'SIZE_ML', 'COUNT', 'PACK_CONTENT']]\n",
    "    required_volume_dataset_check = required_volume_dataset.groupby('Product')['Channel'].nunique().reset_index()\n",
    "    if len(channel_list) != 1:\n",
    "        existing_prod_list = required_volume_dataset_check[required_volume_dataset_check.Channel > 1].Product.unique()\n",
    "    else:\n",
    "        existing_prod_list = required_volume_dataset_check.Product.unique()\n",
    "\n",
    "    # Existing data features\n",
    "    data_existing_feature = required_volume_dataset[required_volume_dataset.Product.isin(existing_prod_list)][[\"Product\",\"Category\",\n",
    "                              \"Pack.Subtype\", 'SIZE_ML', 'COUNT', 'PACK_CONTENT']].drop_duplicates().reset_index(drop=True)\n",
    "    data_existing_feature['TOTAL_LITERS'] = data_existing_feature['SIZE_ML']*data_existing_feature['COUNT']\n",
    "\n",
    "    # New Product Features\n",
    "    data_existing_feature['New_SIZE_ML'] = size.value\n",
    "    data_existing_feature['New_COUNT'] = count.value\n",
    "    data_existing_feature['New_TOTAL_LITERS'] = size.value * count.value\n",
    "    data_existing_feature['New_PACK_CONTENT'] = pack_content_drop.value\n",
    "    data_existing_feature['New_Pack.Subtype'] = pack_subtype_drop.value\n",
    "    data_existing_feature['New_Category'] = category_drop.value\n",
    "\n",
    "    # Processing numerical scores\n",
    "    data_existing_feature[\"size_dif\"]=abs(data_existing_feature[\"New_SIZE_ML\"]- data_existing_feature[\"SIZE_ML\"])\n",
    "    data_existing_feature[\"count_dif\"]=abs(data_existing_feature[\"New_COUNT\"]- data_existing_feature[\"COUNT\"])\n",
    "    data_existing_feature[\"total_liters_dif\"]=abs(data_existing_feature[\"New_TOTAL_LITERS\"]- data_existing_feature[\"TOTAL_LITERS\"])\n",
    "\n",
    "    data_existing_feature[\"size_score\"]=1- ((data_existing_feature[\"size_dif\"]-data_existing_feature[\"size_dif\"].min())/\n",
    "                               (( data_existing_feature[\"size_dif\"].max()-data_existing_feature[\"size_dif\"].min() )))\n",
    "    data_existing_feature[\"count_score\"]=1- ((data_existing_feature[\"count_dif\"]-data_existing_feature[\"count_dif\"].min())/\n",
    "                               (( data_existing_feature[\"count_dif\"].max()-data_existing_feature[\"count_dif\"].min() )))\n",
    "    data_existing_feature[\"total_liters_score\"]=1- ((data_existing_feature[\"total_liters_dif\"]-data_existing_feature[\"total_liters_dif\"].min())/\n",
    "                               (( data_existing_feature[\"total_liters_dif\"].max()-data_existing_feature[\"total_liters_dif\"].min() )))\n",
    "\n",
    "\n",
    "    # Processing categorical scores\n",
    "    data_existing_feature[\"Category_score\"]= np.where(data_existing_feature[\"New_Category\"]==data_existing_feature[\"Category\"],1,0)\n",
    "    data_existing_feature[\"Pack_content_score\"]= np.where(data_existing_feature[\"New_PACK_CONTENT\"]==data_existing_feature[\"PACK_CONTENT\"],1,0)\n",
    "    data_existing_feature[\"Pack_Subtype_score\"]= np.where(data_existing_feature[\"New_Pack.Subtype\"]==data_existing_feature[\"Pack.Subtype\"],1,0)\n",
    "\n",
    "    # Aggregation of Category_score,Pack_content_score,Pack_Subtype_score\n",
    "    data_existing_feature[\"Categorical_Score\"]= (data_existing_feature[\"Category_score\"]+\n",
    "                                                 data_existing_feature[\"Pack_content_score\"]+\n",
    "                                                 data_existing_feature[\"Pack_Subtype_score\"])/3\n",
    "\n",
    "    score_data=data_existing_feature[[\"Product\",\"size_score\",\"count_score\",\"total_liters_score\",\"Categorical_Score\"]]                    \n",
    "\n",
    "    score_data = score_data.fillna(0)\n",
    "    score_data[\"Similarity_Score\"]=(score_data[\"size_score\"]+\n",
    "                                score_data[\"count_score\"]+  \n",
    "                                score_data[\"total_liters_score\"] +\n",
    "                                score_data[\"Categorical_Score\"])/4\n",
    "    \n",
    "    return score_data\n",
    "\n",
    "# Ensemble: Manual or with similarity score calculation\n",
    "if toggle_ensemble_normal.value == 'Multiple Product':           \n",
    "    toggle_manual_simscore = widgets.ToggleButtons(options=['Manual','Similarity Score'],value='Similarity Score',description='Select model packs manually or using similarity scores',style= {'description_width': 'initial'})    \n",
    "    \n",
    "    # Existing model pack selected to be present in more than 1 channel\n",
    "    required_volume_dataset = Volume_dataset_all_reg[(Volume_dataset_all_reg.Region.isin(Region_List)) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value) & (Volume_dataset_all_reg.Category == category_drop.value)][[\"Product\",\"Channel\",\"Category\",\"Pack.Subtype\", 'SIZE_ML', 'COUNT', 'PACK_CONTENT']]\n",
    "    required_volume_dataset_check = required_volume_dataset.groupby('Product')['Channel'].nunique().reset_index()\n",
    "    if len(channel_list) != 1:        \n",
    "        existing_prod_list = required_volume_dataset_check[required_volume_dataset_check.Channel > 1].Product.unique()    \n",
    "    else:\n",
    "        existing_prod_list = required_volume_dataset_check.Product.unique()    \n",
    "        \n",
    "    prod_ensemble = widgets.SelectMultiple(options = list(np.sort(Volume_dataset_all_reg[(Volume_dataset_all_reg.Region.isin(Region_List)) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value) & (Volume_dataset_all_reg.Category == category_drop.value) & (Volume_dataset_all_reg.Product.isin(existing_prod_list))].Product.unique())))\n",
    "    \n",
    "# Manual/Similarity score changes functions    \n",
    "def fun(obj):\n",
    "    # Display manual model pack selection\n",
    "    if toggle_manual_simscore.value == 'Manual':          \n",
    "        clear_output()       \n",
    "        score_data = similarity_score_calculation()\n",
    "        display(toggle_manual_simscore)\n",
    "        display(prod_ensemble)          \n",
    "        print(colored('Exactly 3 products should be selected', 'red',attrs=['bold']))\n",
    "        display(button_check)\n",
    "\n",
    "    # Display similarity score model pack selection\n",
    "    else:    \n",
    "        clear_output()        \n",
    "        score_data = similarity_score_calculation()\n",
    "        display(toggle_manual_simscore)\n",
    "        display(score_data.sort_values('Similarity_Score', ascending = False)[['Product','Similarity_Score']][:3]) \n",
    "        display(button_check)\n",
    "\n",
    "if toggle_ensemble_normal.value == 'Multiple Product':        \n",
    "    display(toggle_manual_simscore)\n",
    "    score_data = similarity_score_calculation()\n",
    "    display(score_data.sort_values('Similarity_Score', ascending = False)[['Product','Similarity_Score']][:3])\n",
    "    toggle_manual_simscore.observe(fun, 'value')\n",
    "                \n",
    "# New Product 2 feature not available for ensemble model\n",
    "if toggle_ensemble_normal.value != 'Multiple Product': \n",
    "    # Toggle for new product 2 attribute selection\n",
    "    toggle_np2 = widgets.ToggleButtons(options=['Yes','No'],value='No',description='Do you want to change attributes for 2nd New Product?',style= {'description_width': 'initial'})\n",
    "\n",
    "    out = widgets.Output(layout=widgets.Layout(border = 'solid 2px'))\n",
    "\n",
    "    def fun(obj):\n",
    "        if toggle_np2.value == 'Yes':          \n",
    "            with out:        \n",
    "                display(widgets.Label(value=\"$New Product 2$\"),widgets.HBox((widgets.Label(value=\"New Category\"), category_drop2,widgets.Label(value=\"New Pack Subtype\"), pack_subtype_drop2,widgets.Label(value=\"New Pack Content\"), pack_content_drop2)),widgets.HBox((count2, size2)))\n",
    "        else:\n",
    "            out.clear_output()\n",
    "\n",
    "    display(toggle_np2)\n",
    "\n",
    "    display(out)\n",
    "    toggle_np2.observe(fun, 'value')   \n",
    "    \n",
    "    \n",
    "# Run prices cell\n",
    "def run_all(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1,IPython.notebook.ncells()-27)'))\n",
    "\n",
    "layout = widgets.Layout(width='auto', height='40px')\n",
    "button = widgets.Button(description=\"Get Existing Prices & Discounts\", layout = layout)\n",
    "button.on_click(run_all)\n",
    "\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(colored('The Category, Pack Subtype, Pack Content of selected Model Pack is', 'red',attrs=['bold']),colored(Volume_dataset_all_reg[Volume_dataset_all_reg.Product == reg_chan_prod.result.value].Category.unique()[0], 'red',attrs=['bold']),colored(',', 'red',attrs=['bold']),colored(Volume_dataset_all_reg[Volume_dataset_all_reg.Product == reg_chan_prod.result.value]['Pack.Subtype'].unique()[0], 'red',attrs=['bold']),colored('&', 'red',attrs=['bold']),colored(Volume_dataset_all_reg[Volume_dataset_all_reg.Product == reg_chan_prod.result.value].PACK_CONTENT.unique()[0], 'red',attrs=['bold']),colored('respectively', 'red',attrs=['bold']))\n",
    "except:\n",
    "    print('Product is not present in that Region/Channel for selected period')\n",
    "    product_fil = []\n",
    "\n",
    "    \n",
    "# Storing filter output\n",
    "region_fil = region.value\n",
    "channel_fil = channel.value\n",
    "product_fil = reg_chan_prod.result.value\n",
    "size_fil = size.value\n",
    "count_fil = count.value\n",
    "size_fil2 = size2.value\n",
    "count_fil2 = count2.value\n",
    "\n",
    "# Initialising list for all/specific channel, region\n",
    "if channel.value == 'All':\n",
    "    channel_list = Volume_dataset_all_reg.Channel.unique()\n",
    "else:\n",
    "    channel_list = [channel.value]\n",
    "    \n",
    "if region.value == 'All':\n",
    "    Region_List = Volume_dataset_all_reg.Region.unique()\n",
    "else:\n",
    "    Region_List = [region.value]   \n",
    "    \n",
    "# Ensemble list of products\n",
    "if toggle_ensemble_normal.value == 'Multiple Product':\n",
    "    if toggle_manual_simscore.value == 'Manual':    \n",
    "        prod_ensemble_list = list(prod_ensemble.value)\n",
    "    else:    \n",
    "        prod_ensemble_list = list(score_data.sort_values('Similarity_Score', ascending = False)[['Product','Similarity_Score']][:3].Product.unique())\n",
    "\n",
    "        \n",
    "uploader = widgets.FileUpload(multiple=False, description = 'Upload Adcal Data of New Product', layout = widgets.Layout(width='300px'))\n",
    "\n",
    "#### Change price cell run\n",
    "def run_all(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1,IPython.notebook.ncells()-15)'))\n",
    "layout = widgets.Layout(width='auto', height='40px')\n",
    "button_proceed = widgets.Button(description=\"Proceed further\",layout = layout)\n",
    "button_proceed.on_click(run_all)\n",
    "\n",
    "# Toggle for new product uploading option\n",
    "toggle_upload = widgets.ToggleButtons(options=['Yes','No'],value='No',description='Do you want to upload adcal data?',style= {'description_width': 'initial'})\n",
    "\n",
    "out_upload = widgets.Output(layout=widgets.Layout(border = 'solid 2px'))\n",
    "\n",
    "def fun2(obj):\n",
    "    if (toggle_upload.value == 'Yes') & (toggle_ensemble_normal.value == 'Single Product'):          \n",
    "        with out_upload:        \n",
    "            display(uploader)\n",
    "            display(button_proceed)\n",
    "\n",
    "    else:\n",
    "        out_upload.clear_output()\n",
    "\n",
    "if toggle_ensemble_normal.value == 'Single Product':\n",
    "    display(toggle_upload)\n",
    "\n",
    "    display(out_upload)\n",
    "    toggle_upload.observe(fun2, 'value')\n",
    "\n",
    "# Function to find the list of cross products\n",
    "def find_affecting_brands(brand_id):\n",
    "        Product_type = brand_mapping.loc[brand_mapping[\"brand\"]==brand_id,\"Brand\"]\n",
    "        Intra_brands = brand_mapping.loc[brand_mapping['Banner'].isin(brand_mapping.loc[(brand_mapping['brand']==brand_id),\"Banner\"]),\"brand\"]\n",
    "        brand_mapping_category = brand_mapping.copy()\n",
    "        alt_beverage = brand_mapping_category.loc[(~brand_mapping_category[\"Category\"].isin([\"WATER\",\"CARBONATED SOFT DRINKS\"]) ) &\n",
    "                       (brand_mapping_category[\"Brand\"]==\"TCCC\"),\"Category\"].unique()\n",
    "        brand_mapping_category['Category'] = ['ALTERNATIVE BEVERAGES' if x in alt_beverage else x for x in brand_mapping_category['Category']]\n",
    "        pack = brand_mapping.loc[brand_mapping['Pack.Subtype'].isin(brand_mapping.loc[(brand_mapping['brand']==brand_id),\"Pack.Subtype\"]),\"brand\"]\n",
    "        cat = brand_mapping.loc[brand_mapping['Category'].isin(brand_mapping.loc[(brand_mapping['brand']==brand_id),\"Category\"]),\"brand\"]\n",
    "        packsubtype = pack[pack.isin(cat)]\n",
    "        Affecting_brands = Intra_brands.append(packsubtype)\n",
    "        Affecting_brands = list(set(Affecting_brands))\n",
    "        mean_sales_10 = 0.1*Volume_dataset.loc[Volume_dataset[\"brand\"]==brand_id,\"Eq.Unit.Sales\"].mean()\n",
    "        Affecting_brands_vol = pd.DataFrame(Volume_dataset.loc[Volume_dataset[\"brand\"].isin(Affecting_brands)].groupby(\"brand\")[\"Eq.Unit.Sales\"].mean()).reset_index()\n",
    "        Affecting_brands = Affecting_brands_vol.loc[Affecting_brands_vol[\"Eq.Unit.Sales\"]>mean_sales_10,\"brand\"]\n",
    "        return Affecting_brands\n",
    "\n",
    "# Changing number format\n",
    "def human_format(num, precision=2, suffixes=['', 'K', 'M', 'G', 'T', 'P']):\n",
    "    m = sum([abs(num/1000.0**x) >= 1 for x in range(1, len(suffixes))])\n",
    "    return f'{num/1000.0**m:.{precision}f}{suffixes[m]}'\n",
    "\n",
    "# Product stage function of new product\n",
    "def prod_stage_check(Product):\n",
    "    # Filtering for new products from data_all\n",
    "    prod_initial = Volume_dataset_all_reg[Volume_dataset_all_reg.Product.isin(list([Product]))]\n",
    "    # Sorting by product, week\n",
    "    prod_initial = prod_initial[['Product','Week','Test_period']].drop_duplicates().sort_values(['Product','Week']) \n",
    "    # Year\n",
    "    prod_initial['Year'] = prod_initial.Week.str.slice(0,4)\n",
    "    # Week number\n",
    "    prod_initial['Week_no'] = prod_initial.Week.str.slice(5,7)\n",
    "\n",
    "    # Converting into year-week-days\n",
    "    prod_initial['year_week_ts'] = prod_initial.apply(lambda row: year_week(row.Year, row.Week_no), axis=1)\n",
    "\n",
    "    # Get updated start week\n",
    "    if start_week.children[8].value < Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil].Test_period.min():    \n",
    "        start_week_prod_stage = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil].Week.min()\n",
    "    else:\n",
    "        start_week_prod_stage = Volume_dataset_all_reg[Volume_dataset_all_reg.Test_period == start_week.children[8].value].Week.min()\n",
    "\n",
    "    # Difference of weeks with the first week for each product to calculate initial weeks    \n",
    "    prod_initial = prod_initial[prod_initial.Week >= start_week_prod_stage]\n",
    "    prod_initial['week_count'] = (prod_initial['year_week_ts'] - prod_initial.groupby('Product')['year_week_ts'].transform('first'))/7\n",
    "    # Transforming days into integer\n",
    "    prod_initial['week_count'] = prod_initial['week_count'].dt.days.astype('int')\n",
    "    # Adding 1 to week_count column\n",
    "    prod_initial['week_count'] = prod_initial.week_count + 1  \n",
    "    # Renaming week_count to intial weeks\n",
    "    prod_initial = prod_initial.rename(columns = {\"week_count\" : \"Intial_weeks\"})\n",
    "    # Droppping unnecessary columns\n",
    "    prod_initial = prod_initial.drop(['Year','Week_no','year_week_ts'], axis = 1)\n",
    "    prod_initial = prod_initial.rename(columns = {\"Intial_weeks\":\"Initial_weeks\"})\n",
    "\n",
    "    # Category of product \n",
    "    if i == 1:\n",
    "        prod_initial['Category'] = category_drop.value\n",
    "    elif i == 2:\n",
    "        prod_initial['Category'] = category_drop2.value\n",
    "    \n",
    "    # For Alt-bev: Intro:1, Growth: 2-12 rows, Stablisation 13 onwards\n",
    "    prod_initial.loc[(prod_initial.Category == 'ALTERNATIVE BEVERAGES') & (prod_initial.Initial_weeks == 1),'Intial_weeks'] = 'Intro'\n",
    "    prod_initial.loc[(prod_initial.Category == 'ALTERNATIVE BEVERAGES') & ((prod_initial.Initial_weeks == 2) |(prod_initial.Initial_weeks == 3)| (prod_initial.Initial_weeks == 4) | (prod_initial.Initial_weeks == 5) | (prod_initial.Initial_weeks == 6) | (prod_initial.Initial_weeks == 7) | (prod_initial.Initial_weeks == 8) | (prod_initial.Initial_weeks == 9) | (prod_initial.Initial_weeks == 10) | (prod_initial.Initial_weeks == 11) | (prod_initial.Initial_weeks == 12)),'Intial_weeks'] = 'Growth'\n",
    "\n",
    "    # For WATER: Intro:1, Growth: 2-8 rows, Stablisation 9 onwards\n",
    "    prod_initial.loc[(prod_initial.Category == 'WATER') & (prod_initial.Initial_weeks == 1),'Intial_weeks'] = 'Intro'\n",
    "    prod_initial.loc[(prod_initial.Category == 'WATER') & ((prod_initial.Initial_weeks == 2) |(prod_initial.Initial_weeks == 3)| (prod_initial.Initial_weeks == 4) | (prod_initial.Initial_weeks == 5) | (prod_initial.Initial_weeks == 6) | (prod_initial.Initial_weeks == 7) | (prod_initial.Initial_weeks == 8) ),'Intial_weeks'] = 'Growth'\n",
    "\n",
    "    # For CSD # Intro:1, Growth: 2-6 rows, Stablisation 7 onwards\n",
    "    prod_initial.loc[(prod_initial.Category == 'CARBONATED SOFT DRINKS') & (prod_initial.Initial_weeks == 1),'Intial_weeks'] = 'Intro'\n",
    "    prod_initial.loc[(prod_initial.Category == 'CARBONATED SOFT DRINKS') & ((prod_initial.Initial_weeks == 2) |(prod_initial.Initial_weeks == 3)| (prod_initial.Initial_weeks == 4) | (prod_initial.Initial_weeks == 5) | (prod_initial.Initial_weeks == 6)),'Intial_weeks'] = 'Growth'\n",
    "\n",
    "    # Fill na with stabilisation\n",
    "    prod_initial['Intial_weeks'] = prod_initial['Intial_weeks'].fillna('Stabilization')\n",
    "    prod_initial = prod_initial.rename(columns = {\"Intial_weeks\":\"Initial_weeks1\"})\n",
    "    prod_initial = prod_initial.drop(['Initial_weeks', 'Category'], axis = 1)\n",
    "    prod_initial = prod_initial[prod_initial.Week <= Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Test_period <= end_week.children[8].value, 'Week'].max()]\n",
    "    \n",
    "    return prod_initial\n",
    "\n",
    "\n",
    "# Product stages for uplaoded data\n",
    "def prod_stage_check_upload():\n",
    "    # Filtering for new products from data_all\n",
    "    prod_initial_upload = upload_data[['Product','Week']]\n",
    "    # Sorting by product, week\n",
    "    prod_initial_upload = prod_initial_upload.sort_values(['Product','Week']) \n",
    "    # Year\n",
    "    prod_initial_upload['Year'] = prod_initial_upload.Week.str.slice(0,4)\n",
    "    # Week number\n",
    "    prod_initial_upload['Week_no'] = prod_initial_upload.Week.str.slice(5,7)\n",
    "\n",
    "    # Converting into year-week-days\n",
    "    prod_initial_upload['year_week_ts'] = prod_initial_upload.apply(lambda row: year_week(row.Year, row.Week_no), axis=1)\n",
    "\n",
    "    # Difference of weeks with the first week for each product to calculate initial weeks    \n",
    "    prod_initial_upload['week_count'] = (prod_initial_upload['year_week_ts'] - prod_initial_upload.groupby('Product')['year_week_ts'].transform('first'))/7\n",
    "    # Transforming days into integer\n",
    "    prod_initial_upload['week_count'] = prod_initial_upload['week_count'].dt.days.astype('int')\n",
    "    # Adding 1 to week_count column\n",
    "    prod_initial_upload['week_count'] = prod_initial_upload.week_count + 1  \n",
    "    # Renaming week_count to intial weeks\n",
    "    prod_initial_upload = prod_initial_upload.rename(columns = {\"week_count\" : \"Intial_weeks\"})\n",
    "    # Droppping unnecessary columns\n",
    "    prod_initial_upload = prod_initial_upload.drop(['Year','Week_no','year_week_ts'], axis = 1)\n",
    "    prod_initial_upload = prod_initial_upload.rename(columns = {\"Intial_weeks\":\"Initial_weeks\"})\n",
    "    prod_initial_upload = prod_initial_upload.drop_duplicates()\n",
    "\n",
    "    # Category of product \n",
    "    prod_initial_upload ['Category'] = upload_data.Category.unique()[0]\n",
    "\n",
    "    # For Alt-bev: Intro:1, Growth: 2-12 rows, Stablisation 13 onwards\n",
    "    prod_initial_upload.loc[(prod_initial_upload.Category == 'ALTERNATIVE BEVERAGES') & (prod_initial_upload.Initial_weeks == 1),'Intial_weeks'] = 'Intro'\n",
    "    prod_initial_upload.loc[(prod_initial_upload.Category == 'ALTERNATIVE BEVERAGES') & ((prod_initial_upload.Initial_weeks == 2) |(prod_initial_upload.Initial_weeks == 3)| (prod_initial_upload.Initial_weeks == 4) | (prod_initial_upload.Initial_weeks == 5) | (prod_initial_upload.Initial_weeks == 6) | (prod_initial_upload.Initial_weeks == 7) | (prod_initial_upload.Initial_weeks == 8) | (prod_initial_upload.Initial_weeks == 9) | (prod_initial_upload.Initial_weeks == 10) | (prod_initial_upload.Initial_weeks == 11) | (prod_initial_upload.Initial_weeks == 12)),'Intial_weeks'] = 'Growth'\n",
    "\n",
    "    # For WATER: Intro:1, Growth: 2-8 rows, Stablisation 9 onwards\n",
    "    prod_initial_upload.loc[(prod_initial_upload.Category == 'WATER') & (prod_initial_upload.Initial_weeks == 1),'Intial_weeks'] = 'Intro'\n",
    "    prod_initial_upload.loc[(prod_initial_upload.Category == 'WATER') & ((prod_initial_upload.Initial_weeks == 2) |(prod_initial_upload.Initial_weeks == 3)| (prod_initial_upload.Initial_weeks == 4) | (prod_initial_upload.Initial_weeks == 5) | (prod_initial_upload.Initial_weeks == 6) | (prod_initial_upload.Initial_weeks == 7) | (prod_initial_upload.Initial_weeks == 8) ),'Intial_weeks'] = 'Growth'\n",
    "\n",
    "    # For CSD # Intro:1, Growth: 2-6 rows, Stablisation 7 onwards\n",
    "    prod_initial_upload.loc[(prod_initial_upload.Category == 'CARBONATED SOFT DRINKS') & (prod_initial_upload.Initial_weeks == 1),'Intial_weeks'] = 'Intro'\n",
    "    prod_initial_upload.loc[(prod_initial_upload.Category == 'CARBONATED SOFT DRINKS') & ((prod_initial_upload.Initial_weeks == 2) |(prod_initial_upload.Initial_weeks == 3)| (prod_initial_upload.Initial_weeks == 4) | (prod_initial_upload.Initial_weeks == 5) | (prod_initial_upload.Initial_weeks == 6)),'Intial_weeks'] = 'Growth'\n",
    "\n",
    "    # Fill na with stabilisation\n",
    "    prod_initial_upload['Intial_weeks'] = prod_initial_upload['Intial_weeks'].fillna('Stabilization')\n",
    "    prod_initial_upload = prod_initial_upload.drop(['Initial_weeks', 'Category'], axis = 1)\n",
    "    return prod_initial_upload\n",
    "\n",
    "# UDF for data treament for uploaded data #CHANGE 101: Integration of uplaoded data with tool\n",
    "def upload_data_treat():\n",
    "    data_mart = upload_data.copy()\n",
    "    data_mart.dropna(inplace=True)\n",
    "    # Files present in FTP server under \"New Product Simulator\"\n",
    "    #Banner Channel dimension table    \n",
    "    base_data_geo_dim = pd.read_csv('base_data_geo_dim.csv')\n",
    "    #Merging Channel data to Adcal data\n",
    "    data_mart = pd.merge(data_mart,base_data_geo_dim, how='inner', left_on='TPO_BANNER_GROUP', right_on='BN_BANNER_GROUP')\n",
    "    data_mart.rename(columns={'BN_CHANNEL':'Channel'}, inplace=True)\n",
    "    del data_mart['BN_BANNER_GROUP']\n",
    "\n",
    "    # Files present in FTP server under \"New Product Simulator\"\n",
    "    #Importing Week data\n",
    "    week_data = pd.read_csv('Week_data_2022.csv')\n",
    "    #Merging Week data with Adcal data for Week Name and Week Type columns\n",
    "    data_mart = pd.merge(data_mart, week_data, how='inner', on=['REGION','TPO_WEEK'])\n",
    "\n",
    "    #Some columns have spaces bewtween the value,removing the spaces\n",
    "    data_mart['REGION'] = data_mart['REGION'].str.strip()\n",
    "    data_mart['Channel'] = data_mart['Channel'].str.strip()\n",
    "    data_mart['FEATURE'] = data_mart['FEATURE'].str.strip()\n",
    "    data_mart['CAL_WEEK_NAME'] = data_mart['CAL_WEEK_NAME'].str.strip()\n",
    "    data_mart['CAL_WEEK_TYPE'] = data_mart['CAL_WEEK_TYPE'].str.strip()\n",
    "\n",
    "    #Renaming the required columns\n",
    "    data_mart.rename(columns={'REGION':'Region', 'STORE_CHAIN':'Store.Chain','TPO_BANNER_GROUP':'Banner',\n",
    "                              'TPO_PRODUCT_GROUP':'Product', 'CATEGORY':'Category', 'PACK_SUBTYPE':'Pack.Subtype',\n",
    "                              'TPO_WEEK':'Week', 'CAL_WEEK_NAME':'Week.Name', 'CAL_WEEK_TYPE':'Week.Type',\n",
    "                              'FEATURE':'Adcal_Page.Position', 'PRICE':'Adcal_Price'}, inplace=True)\n",
    "\n",
    "    # Subsetting required columns\n",
    "    data_mart = data_mart[['Region','Channel','Store.Chain','Banner','Product','Category','PACK_CONTENT','Pack.Subtype','COUNT','SIZE_ML','Week','Week.Name','Week.Type','WEEK','YEAR','QUARTER','Adcal_Page.Position','Adcal_Price']]\n",
    "\n",
    "    # Dropping duplicates\n",
    "    data_mart = data_mart.drop_duplicates()\n",
    "\n",
    "    ### 2)Changing the banner names for Fresh Co, Maxi and Walmart(East and Quebec are changed to East/Quebec)\n",
    "    #Banner group name same as banner name. Change banner group name to distinguish between the two\n",
    "    data_mart['Banner'].replace({'FRESH CO ON':'FRESH CO/PRICE CHOPPER ON'}, inplace=True)\n",
    "    data_mart['Banner'].replace({'MAXI':'MAXI/CIE'}, inplace=True)\n",
    "    data_mart['Banner'].replace({'WALMART EAST':'WALMART EAST/QUEBEC'}, inplace=True)\n",
    "    data_mart['Banner'].replace({'AFFILIATED SOBEYS QC':'AFF SOBEYS QC'}, inplace=True)\n",
    "    data_mart['Banner'].replace({'RCLS':'LCL LIQUOR WEST'}, inplace=True)\n",
    "    data_mart['Banner'].replace({'LOBLAW QC':'LOBLAWS QC'}, inplace=True)\n",
    "\n",
    "    #Sorting the data at Banner Product Week level\n",
    "    data_treatment = data_mart.sort_values(by=['Banner','Product','Week']).copy()\n",
    "\n",
    "    #Extract Year from Week\n",
    "    data_treatment['Year'] = data_treatment['Week'].str.slice(0,4)\n",
    "\n",
    "    ## Calculating EDV Price: \n",
    "    EDV_dataset = data_treatment[['Banner','Product','Year']].drop_duplicates()\n",
    "\n",
    "    data_treatment['EDV.Price'] = data_treatment.groupby(['Banner','Product','Year'])['Adcal_Price'].transform(lambda x: round(np.quantile(x, q=0.9),2))\n",
    "\n",
    "    ### Calculating the Discount Depth\n",
    "    #Discount depth is calculated and the Adcal Prices  that are more than 90% of the EDV Price are replaced by the EDV Price. This is taken to keep a cutoff for promo price and replace all non-promo prices with the EDV price \n",
    "    ##Calculate discount depth\n",
    "    data_treatment['Adcal_DD'] = round(((data_treatment['EDV.Price'] - data_treatment['Adcal_Price']) / data_treatment['EDV.Price']), 4)\n",
    "\n",
    "    #Make negative values zero\n",
    "    data_treatment.loc[data_treatment['Adcal_DD'] < 0, 'Adcal_DD'] = 0\n",
    "\n",
    "    #Making Adcal.Price equals to EDV.Price Wherever the Discount Depth is less than 10%\n",
    "    data_treatment['Adcal_Price'] = round(data_treatment[['Adcal_DD','Adcal_Price','EDV.Price']].apply(lambda x: x['EDV.Price'] if(x['Adcal_DD'] < 0.1) else x['Adcal_Price'], axis=1), 2)\n",
    "\n",
    "    #Repeating the same process to calculate the new discount depth\n",
    "    data_treatment['Adcal_DD'] = round(((data_treatment['EDV.Price'] - data_treatment['Adcal_Price']) / data_treatment['EDV.Price']), 4)\n",
    "\n",
    "    #Make negative values zero\n",
    "    data_treatment.loc[data_treatment['Adcal_DD'] < 0, 'Adcal_DD'] = 0\n",
    "\n",
    "    ### Pantry flag creation\n",
    "    #Pantry flag is calculated for each Banner-product\n",
    "    pantry_treatment = data_treatment.copy()\n",
    "\n",
    "    #Creating Pantry1 and Pantry2 flags\n",
    "    final_df = pd.DataFrame()\n",
    "    product_list = sorted(list(pantry_treatment['Product'].unique()))\n",
    "    for product in product_list:\n",
    "        banner_list = sorted(list(pantry_treatment['Banner'].unique()))\n",
    "        for banner in banner_list:\n",
    "            df = pantry_treatment[(pantry_treatment['Banner']==banner)\n",
    "                                 &(pantry_treatment['Product']==product)]\n",
    "\n",
    "            df['Test.Vol'] = df['Adcal_DD'].apply(lambda x: 1 if(x > 0.25) else 0)\n",
    "            df['Pantry1'] = df['Test.Vol'].shift(1).fillna(0).astype(int)\n",
    "            df['Pantry2_temp'] = df['Test.Vol'].shift(2).fillna(0).astype(int)\n",
    "            df['Pantry2'] = df[['Pantry1','Pantry2_temp']].apply(lambda x: 0 if((x[0]==1) and (x[1]==1)) else x[1], axis=1)\n",
    "            final_df = final_df.append(df)\n",
    "\n",
    "\n",
    "    pantry_treatment = final_df.copy()\n",
    "    pantry_treatment.drop(columns=['Test.Vol','Pantry2_temp'], inplace=True)\n",
    "\n",
    "    ### Feature Vision flag creation\n",
    "    fv_treated = pantry_treatment.copy()\n",
    "    fv_treated.replace({'FRONT PAGE':'Front.Page', 'INSIDE PAGE':'Middle.Page', 'BACK PAGE':'Back.Page'}, inplace=True)\n",
    "\n",
    "    promo_dummies = pd.get_dummies(fv_treated['Adcal_Page.Position'])\n",
    "\n",
    "    promo_dummies = pd.get_dummies(fv_treated['Adcal_Page.Position'])\n",
    "    promo_lst = list(promo_dummies.columns)\n",
    "\n",
    "    # Add promo dummies if not present\n",
    "    if 'Front.Page' not in promo_lst: \n",
    "        promo_dummies['Front.Page'] = 0\n",
    "\n",
    "    if 'Middle.Page' not in promo_lst: \n",
    "        promo_dummies['Middle.Page'] = 0\n",
    "\n",
    "    if 'Back.Page' not in promo_lst: \n",
    "        promo_dummies['Back.Page'] = 0    \n",
    "\n",
    "    fv_treated = pd.concat([fv_treated, promo_dummies], axis=1)\n",
    "    del fv_treated['IN STORE']\n",
    "\n",
    "    # Creating Test period column\n",
    "    fv_treated['YEAR'] = fv_treated['YEAR'].astype(str)\n",
    "    fv_treated['QUARTER'] = fv_treated['QUARTER'].astype(str)\n",
    "    fv_treated['Test_period'] = fv_treated['YEAR'] + fv_treated['QUARTER']\n",
    "\n",
    "    ### Holiday flag creation\n",
    "    holiday_treatment = fv_treated.copy()\n",
    "    holiday_dummies = pd.get_dummies(holiday_treatment['Week.Type'])\n",
    "    holiday_treatment = pd.concat([holiday_treatment, holiday_dummies], axis=1)\n",
    "\n",
    "    # Files present in FTP server under \"New Product Simulator\"\n",
    "    # Loading seasonality table\n",
    "    seasonality_table = pd.read_csv(\"Seasonality_Index_data\")\n",
    "    seasonality_table['Week.Name'] = seasonality_table['Week.Name'].str.strip()\n",
    "\n",
    "    Region_List = sorted(list(holiday_treatment['Region'].unique()))\n",
    "    category = str(holiday_treatment['Category'].unique()[0])\n",
    "    pack_subtype = str(holiday_treatment['Pack.Subtype'].unique()[0])\n",
    "\n",
    "    final_seasonality_table = pd.DataFrame()\n",
    "\n",
    "    for Region_key in Region_List:\n",
    "        #Creating Region Category Pack Subtype variables\n",
    "        region = Region_key\n",
    "\n",
    "        # Filtering seasonality table\n",
    "        seasonality_table_filtered = seasonality_table[(seasonality_table['Region']==Region_key)\n",
    "                                                      &(seasonality_table['Category']==category)\n",
    "                                                      &(seasonality_table['Pack.Subtype']==pack_subtype)]\n",
    "\n",
    "        # Filtering Week data\n",
    "        week_data_filtered = week_data[week_data['REGION']==Region_key]\n",
    "        week_data_filtered['CAL_WEEK_NAME'] = week_data_filtered['CAL_WEEK_NAME'].str.strip()\n",
    "\n",
    "        # Updated seasonality table\n",
    "        seasonality_table_updated = pd.merge(week_data_filtered[['REGION','TPO_WEEK','CAL_WEEK_NAME']], seasonality_table_filtered[['Region','Week.Name','seasonality_index']], how='left', left_on=['REGION','CAL_WEEK_NAME'], right_on=['Region','Week.Name']).fillna(method='ffill')\n",
    "        seasonality_table_updated['CAL_WEEK_NAME'] = seasonality_table_updated['CAL_WEEK_NAME'].str.strip()\n",
    "\n",
    "        final_seasonality_table = final_seasonality_table.append(seasonality_table_updated)\n",
    "\n",
    "    # Merging holiday treatment data with seasonality data\n",
    "    Volume_dataset_all_reg_upload = pd.merge(holiday_treatment,final_seasonality_table[['REGION','CAL_WEEK_NAME','seasonality_index']], how='left', left_on = ['Week.Name','Region'], right_on=['CAL_WEEK_NAME','REGION'])\n",
    "    del Volume_dataset_all_reg_upload['CAL_WEEK_NAME']\n",
    "    Volume_dataset_all_reg_upload.rename(columns={'Post-holiday':'Post.Holiday.Week', 'Pre-holiday':'Pre.Holiday.Week', 'Holiday':'Holiday.Week'}, inplace=True)\n",
    "    Volume_dataset_all_reg_upload.drop(columns = {'REGION'}, inplace = True)\n",
    "    return Volume_dataset_all_reg_upload\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prices, Discounts for existing product/ uploaded product__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "\n",
    "    display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Getting Existing Prices & Discounts</h2></div>'))\n",
    "\n",
    "    ######################### FILTERS & LAYOUTS #########################\n",
    "    layout1 = widgets.Layout(width='200px', height='auto')\n",
    "    layout4 = widgets.Layout(width='100px', height='auto')\n",
    "    test_period = widgets.SelectMultiple(options=list(np.sort(Volume_dataset_all_reg[(Volume_dataset_all_reg.Test_period >= Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product==product_fil,'Test_period'].min()) & (Volume_dataset_all_reg.Test_period <= Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product==product_fil,'Test_period'].max()) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Test_period.unique())) + ['All'], value=['All'], description='Year Quarter', layout = layout1)\n",
    "    test_period2 = widgets.SelectMultiple(options=list(np.sort(Volume_dataset_all_reg[(Volume_dataset_all_reg.Test_period >= Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product==product_fil,'Test_period'].min()) & (Volume_dataset_all_reg.Test_period <= Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product==product_fil,'Test_period'].max()) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Test_period.unique())) + ['All'], value=['All'], description='Year Quarter', layout = layout1)\n",
    "    test_period3 = widgets.SelectMultiple(options=list(np.sort(Volume_dataset_all_reg[(Volume_dataset_all_reg.Test_period >= Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product==product_fil,'Test_period'].min()) & (Volume_dataset_all_reg.Test_period <= Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product==product_fil,'Test_period'].max()) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Test_period.unique())) + ['All'], value=['All'], description='Year Quarter', layout = layout1)    \n",
    "\n",
    "    # Filter\n",
    "    layout = widgets.Layout(width='250px', height='auto')\n",
    "    base_price_change = widgets.HBox([widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\"Baseline Price Change (%)\"), widgets.BoundedFloatText(value=0, min = -99, max = 1000, disabled=False,layout = layout2)])   \n",
    "    base_price_change2 = widgets.HBox([widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\"Baseline Price Change (%)\"), widgets.BoundedFloatText(value=0, min = -99, max = 1000, disabled=False,layout = layout2)])   \n",
    "    base_price_change3 = widgets.HBox([widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\"Baseline Price Change (%)\"), widgets.BoundedFloatText(value=0, min = -99, max = 1000, disabled=False,layout = layout2)])\n",
    "    disc_change = widgets.HBox([widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\"Discount Change (%)\"), widgets.BoundedFloatText(value=0, min = -99, max = 1000, disabled=False,layout = layout2)])\n",
    "    disc_change2 = widgets.HBox([widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\"Discount Change (%)\"), widgets.BoundedFloatText(value=0, min = -99, max = 1000, disabled=False,layout = layout2)])\n",
    "    disc_change3 = widgets.HBox([widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\" \"),widgets.Label(value=\" \"), widgets.Label(value=\" \"), widgets.Label(value=\"Discount Change (%)\"), widgets.BoundedFloatText(value=0, min = -99, max = 1000, disabled=False,layout = layout2)])\n",
    "\n",
    "    # Bordering filters\n",
    "    box_layout = Layout(display='flex',flex_flow='rows',align_items='stretch',border='solid 2px',width='auto')    \n",
    "            \n",
    "    ############## IN CASE OF ENSEMBLE MODEL ################\n",
    "\n",
    "    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "        if len(prod_ensemble_list) != 3:\n",
    "            clear_output()\n",
    "            print('Select 3 products as existing model pack')\n",
    "        else:\n",
    "\n",
    "            ######################### Quarterly Adcal prices for selected combination #########################\n",
    "            qtr_edv_baseline = Volume_dataset_all_reg[(Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value) & (Volume_dataset_all_reg.Region.isin(Region_List)) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Product.isin(prod_ensemble_list))].groupby(['Product','Test_period'])['EDV.Price'].mean().reset_index()\n",
    "\n",
    "            if len(qtr_edv_baseline) != 0 : # If product is present in selected test period \n",
    "\n",
    "                # Renaming required column\n",
    "                qtr_edv_baseline = qtr_edv_baseline.rename(columns = {'Product':'Existing Model Pack','Test_period':'Year Quarter','EDV.Price':'Baseline Pricing'})\n",
    "\n",
    "                # Baseline & Discount pricing merge\n",
    "                qtr_edv = qtr_edv_baseline\n",
    "                qtr_edv_prev = qtr_edv.copy()\n",
    "\n",
    "                ########################## Quarterly Adcal DD ##########################\n",
    "                qtr_disc = Volume_dataset_all_reg[(Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value) & (Volume_dataset_all_reg.Adcal_DD >= 0.1) & (Volume_dataset_all_reg.Region.isin(Region_List)) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Product.isin(prod_ensemble_list))].groupby(['Product','Test_period']).agg({'Adcal_DD':'mean'}).reset_index()\n",
    "                qtr_disc = qtr_disc.rename(columns = {'Product':'Existing Model Pack','Test_period':'Year Quarter','Adcal_DD':'Discount (%)'})\n",
    "                qtr_disc['Discount (%)'] = qtr_disc['Discount (%)']*100\n",
    "                qtr_disc_prev = qtr_disc.copy()\n",
    "\n",
    "                ######################### Tabulating results #########################\n",
    "                from tabulate import tabulate\n",
    "                edv_disc_join = qtr_edv_prev.merge(qtr_disc_prev, on = ['Existing Model Pack','Year Quarter'], how = 'left')\n",
    "                clear_output()\n",
    "                edv_disc_join = edv_disc_join[['Existing Model Pack','Year Quarter','Baseline Pricing','Discount (%)']].sort_values('Existing Model Pack')\n",
    "                display(edv_disc_join.round(2).astype(str).style.set_table_styles([{'selector' : '','props' : [('border','1px solid black')]}]))\n",
    "\n",
    "            else:     # If product is not present in selected test period \n",
    "                clear_output()\n",
    "                print('Product is not present in that Region/Channel for selected period')            \n",
    "\n",
    "    else: # In case of normal approach\n",
    "        \n",
    "        ######################### Quarterly Adcal prices for selected combination #########################\n",
    "        qtr_edv_baseline = Volume_dataset_all_reg[(Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value) & (Volume_dataset_all_reg.Region.isin(Region_List)) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Product.isin([product_fil]))].groupby('Test_period')['EDV.Price'].mean().reset_index()\n",
    "\n",
    "        if len(qtr_edv_baseline) != 0 : # If product is present in selected test period \n",
    "\n",
    "            # Renaming required column\n",
    "            qtr_edv_baseline = qtr_edv_baseline.rename(columns = {'Test_period':'Year Quarter','EDV.Price':'Baseline Pricing'})\n",
    "\n",
    "            # Baseline & Discount pricing merge\n",
    "            qtr_edv = qtr_edv_baseline\n",
    "            qtr_edv_prev = qtr_edv.copy()\n",
    "\n",
    "            ########################## Quarterly Adcal DD ##########################\n",
    "            qtr_disc = Volume_dataset_all_reg[(Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value) & (Volume_dataset_all_reg.Adcal_DD >= 0.1) & (Volume_dataset_all_reg.Region.isin(Region_List)) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Product.isin([product_fil]))].groupby('Test_period').agg({'Adcal_DD':'mean'}).reset_index()\n",
    "            qtr_disc = qtr_disc.rename(columns = {'Test_period':'Year Quarter','Adcal_DD':'Discount (%)'})\n",
    "            qtr_disc['Discount (%)'] = qtr_disc['Discount (%)']*100\n",
    "            qtr_disc_prev = qtr_disc.copy()\n",
    "\n",
    "            ######################### Tabulating results #########################\n",
    "            from tabulate import tabulate\n",
    "            edv_disc_join = qtr_edv_prev.merge(qtr_disc_prev, on = 'Year Quarter', how = 'left')\n",
    "            clear_output()\n",
    "            edv_disc_join.fillna(0, inplace=True)\n",
    "            edv_disc_join['Existing Model Pack'] = product_fil\n",
    "            edv_disc_join = edv_disc_join[['Existing Model Pack','Year Quarter','Baseline Pricing','Discount (%)']].sort_values('Existing Model Pack')\n",
    "            display(edv_disc_join.round(2).astype(str).style.set_table_styles([{'selector' : '','props' : [('border','1px solid black')]}]))\n",
    "\n",
    "        else:    \n",
    "            clear_output()\n",
    "            print('Product is not present in that Region/Channel for selected period')\n",
    "\n",
    "else: # Integration of uploaded data with tool\n",
    "    if len(uploader.value) != 0:\n",
    "        try:\n",
    "            qtr_edv_baseline = Volume_dataset_all_reg[(Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value) & (Volume_dataset_all_reg.Region.isin(Region_List)) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Product.isin([product_fil]))].groupby('Test_period')['EDV.Price'].mean().reset_index()\n",
    "            input_file = list(uploader.value.values())[0]\n",
    "            content = input_file['content']\n",
    "            content = io.StringIO(content.decode('utf-8'))\n",
    "            upload_data = pd.read_csv(content)\n",
    "            pd.set_option('display.max_columns',50)\n",
    "            print('Uploaded Data : ')\n",
    "            display(upload_data.head())        \n",
    "            ## Data Treatment Steps\n",
    "            upload_data_1 = upload_data_treat()\n",
    "            upload_data = upload_data_1.copy()                \n",
    "        except:\n",
    "            print(colored('Please upload the data in proper format with all required columns', 'red',attrs=['bold']))\n",
    "        \n",
    "    else: # Integration of uplaoded data with tool\n",
    "        print(colored('Please upload the file to proceed', 'red',attrs=['bold']))\n",
    "        upload_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "\n",
    "    if len(qtr_edv_baseline) != 0:  # If product is present in selected test period \n",
    "        \n",
    "        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "            if len(prod_ensemble_list) != 3:\n",
    "                print('Select 3 products as existing model pack')\n",
    "            else:\n",
    "                display(VBox([widgets.Label(value=\"$Model Pack 1$\" + ': ' + edv_disc_join['Existing Model Pack'].unique()[0] ),widgets.HBox((test_period,base_price_change,disc_change))], layout=box_layout))\n",
    "                display(VBox([widgets.Label(value=\"$Model Pack 2$\" + ': ' + edv_disc_join['Existing Model Pack'].unique()[1] ),widgets.HBox((test_period2,base_price_change2,disc_change2))], layout=box_layout))\n",
    "                display(VBox([widgets.Label(value=\"$Model Pack 3$\" + ': ' + edv_disc_join['Existing Model Pack'].unique()[2] ),widgets.HBox((test_period3,base_price_change3,disc_change3))], layout=box_layout))\n",
    "          \n",
    "        else: # In case of normal approach\n",
    "            # Display filters\n",
    "            if toggle_np2.value == 'Yes':\n",
    "                display(VBox([widgets.Label(value=\"$New Product 1$\"),widgets.HBox((test_period,base_price_change,disc_change))], layout=box_layout))\n",
    "                display(VBox([widgets.Label(value=\"$New Product 2$\"),widgets.HBox((test_period2,base_price_change2,disc_change2))], layout=box_layout))\n",
    "            elif toggle_np2.value == 'No':\n",
    "                display(VBox([widgets.HBox((test_period,base_price_change,disc_change))], layout=box_layout))\n",
    "    else:\n",
    "        clear_output()\n",
    "        print('Product is not present in that Region/Channel for selected period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "\n",
    "    #### Change price cell run\n",
    "    def run_all(ev):\n",
    "        display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1,IPython.notebook.ncells()-21)'))\n",
    "    layout = widgets.Layout(width='auto', height='40px')\n",
    "    button = widgets.Button(description=\"Change Prices & Discounts\",layout = layout)\n",
    "    button.on_click(run_all)\n",
    "    display(button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Updated Prices & Discounts__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "\n",
    "    ##################################### PRICE & DISCOUNT #####################################\n",
    "    qtr_edv_new = pd.DataFrame()\n",
    "    qtr_disc_new = pd.DataFrame()\n",
    "    qtr_price_disc_new_merge = pd.DataFrame()\n",
    "    \n",
    "    # Storing filter output\n",
    "    # Product 1\n",
    "    test_period_fil = list(test_period.value)\n",
    "    base_price_change_fil = base_price_change.children[10].value/100\n",
    "    disc_change_fil = disc_change.children[10].value/100\n",
    "    # Product 2\n",
    "    test_period_fil2 = list(test_period2.value)\n",
    "    base_price_change_fil2 = base_price_change2.children[10].value/100\n",
    "    disc_change_fil2 = disc_change2.children[10].value/100\n",
    "    # Product 3\n",
    "    test_period_fil3 = list(test_period3.value)\n",
    "    base_price_change_fil3 = base_price_change3.children[10].value/100\n",
    "    disc_change_fil3 = disc_change3.children[10].value/100\n",
    "\n",
    "    if list(test_period.value) == ['All']:\n",
    "        test_period_fil = qtr_edv['Year Quarter'].unique()\n",
    "    else:\n",
    "        test_period_fil = list(test_period.value)\n",
    "    if list(test_period2.value) == ['All']:\n",
    "        test_period_fil2 = qtr_edv['Year Quarter'].unique()\n",
    "    else:\n",
    "        test_period_fil2 = list(test_period2.value)\n",
    "    if list(test_period3.value) == ['All']:            \n",
    "        test_period_fil3 = qtr_edv['Year Quarter'].unique()    \n",
    "    else:\n",
    "        test_period_fil3 = list(test_period3.value)\n",
    "    \n",
    "\n",
    "    if len(qtr_edv_baseline) != 0 :  # If product is present in selected test period \n",
    "\n",
    "        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "            if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "                clear_output()\n",
    "                print('Select 3 products as existing model pack')\n",
    "            else:\n",
    "\n",
    "                for exist_pack in prod_ensemble_list:\n",
    "                    # Changing prices/discounts for each model pack\n",
    "                    qtr_edv_copy = qtr_edv[qtr_edv['Existing Model Pack'] == exist_pack].copy()\n",
    "\n",
    "                    # Price change\n",
    "                    if exist_pack == edv_disc_join['Existing Model Pack'].unique()[0]: # Model Pack 1\n",
    "                        qtr_edv_copy.loc[qtr_edv_copy['Year Quarter'].isin(test_period_fil),'Baseline Pricing'] = (1+base_price_change_fil)*qtr_edv.loc[qtr_edv['Year Quarter'].isin(test_period_fil),'Baseline Pricing']\n",
    "                    elif exist_pack == edv_disc_join['Existing Model Pack'].unique()[1]: # Model Pack 2\n",
    "                        qtr_edv_copy.loc[qtr_edv_copy['Year Quarter'].isin(test_period_fil2),'Baseline Pricing'] = (1+base_price_change_fil2)*qtr_edv.loc[qtr_edv['Year Quarter'].isin(test_period_fil2),'Baseline Pricing']\n",
    "                    elif exist_pack == edv_disc_join['Existing Model Pack'].unique()[2]: # Model Pack 3\n",
    "                        qtr_edv_copy.loc[qtr_edv_copy['Year Quarter'].isin(test_period_fil3),'Baseline Pricing'] = (1+base_price_change_fil3)*qtr_edv.loc[qtr_edv['Year Quarter'].isin(test_period_fil3),'Baseline Pricing']\n",
    "\n",
    "                    # Discount change\n",
    "                    qtr_disc_copy = qtr_disc[qtr_disc['Existing Model Pack'] == exist_pack].copy()\n",
    "\n",
    "                    if exist_pack == edv_disc_join['Existing Model Pack'].unique()[0]: # Model Pack 1\n",
    "                        qtr_disc_copy.loc[qtr_disc_copy['Year Quarter'].isin(test_period_fil),'Discount (%)'] = (1+disc_change_fil)*qtr_disc.loc[qtr_disc['Year Quarter'].isin(test_period_fil),'Discount (%)']\n",
    "                    elif exist_pack == edv_disc_join['Existing Model Pack'].unique()[1]: # Model Pack 2\n",
    "                        qtr_disc_copy.loc[qtr_disc_copy['Year Quarter'].isin(test_period_fil2),'Discount (%)'] = (1+disc_change_fil2)*qtr_disc.loc[qtr_disc['Year Quarter'].isin(test_period_fil2),'Discount (%)']\n",
    "                    elif exist_pack == edv_disc_join['Existing Model Pack'].unique()[2]: # Model Pack 3\n",
    "                        qtr_disc_copy.loc[qtr_disc_copy['Year Quarter'].isin(test_period_fil3),'Discount (%)'] = (1+disc_change_fil3)*qtr_disc.loc[qtr_disc['Year Quarter'].isin(test_period_fil3),'Discount (%)']\n",
    "\n",
    "                    # Updated adcal prices\n",
    "                    qtr_edv_new = qtr_edv_prev[qtr_edv_prev['Existing Model Pack'] == exist_pack].merge(qtr_edv_copy, on = 'Year Quarter', how = 'left')\n",
    "                    qtr_edv_new.drop(columns = 'Existing Model Pack_y', inplace = True)\n",
    "                    qtr_edv_new = qtr_edv_new.rename(columns = {'Existing Model Pack_x':'Existing Model Pack','Baseline Pricing_x':'Previous Baseline Pricing','Baseline Pricing_y':'Current Baseline Pricing'})\n",
    "                    # Updated adcal discounts\n",
    "                    qtr_disc_new = qtr_disc_prev[qtr_disc_prev['Existing Model Pack'] == exist_pack].merge(qtr_disc_copy, on = 'Year Quarter', how = 'left')\n",
    "                    qtr_disc_new.drop(columns = 'Existing Model Pack_y', inplace = True)\n",
    "                    qtr_disc_new = qtr_disc_new.rename(columns = {'Existing Model Pack_x':'Existing Model Pack','Discount (%)_x':'Previous Discount (%)', 'Discount (%)_y':'Current Discount (%)'})\n",
    "\n",
    "\n",
    "                    # ##################################### MERGING DATAFRAMES #####################################\n",
    "                    qtr_price_disc_new = qtr_edv_new.merge(qtr_disc_new, on = ['Year Quarter','Existing Model Pack'], how = 'left')       \n",
    "                    qtr_price_disc_new = qtr_price_disc_new[['Existing Model Pack','Year Quarter','Previous Baseline Pricing', 'Current Baseline Pricing', 'Previous Discount (%)','Current Discount (%)']]       \n",
    "                    qtr_price_disc_new.fillna(0, inplace=True)       \n",
    "\n",
    "                    qtr_price_disc_new_merge = qtr_price_disc_new_merge.append(qtr_price_disc_new, ignore_index = True)\n",
    "                    qtr_price_disc_new_merge = qtr_price_disc_new_merge[['Existing Model Pack','Year Quarter','Previous Baseline Pricing','Current Baseline Pricing','Previous Discount (%)','Current Discount (%)']]\n",
    "\n",
    "                qtr_price_disc_new_merge = qtr_price_disc_new_merge.sort_values('Existing Model Pack')\n",
    "                display(widgets.Label(value=\"Existing Model Packs Price/Discount Changes\"),HTML(qtr_price_disc_new_merge.round(2).to_html(index=False)))\n",
    "\n",
    "            \n",
    "        else: # In case of normal approach\n",
    "            \n",
    "            for i in [1,2]:\n",
    "\n",
    "                qtr_edv_copy = qtr_edv.copy()\n",
    "                qtr_edv_copy['iter'] = i\n",
    "                if i == 1:                \n",
    "                    qtr_edv_copy.loc[qtr_edv_copy['Year Quarter'].isin(test_period_fil),'Baseline Pricing'] = (1+base_price_change_fil)*qtr_edv.loc[qtr_edv['Year Quarter'].isin(test_period_fil),'Baseline Pricing']\n",
    "                elif i == 2:\n",
    "                    qtr_edv_copy.loc[qtr_edv_copy['Year Quarter'].isin(test_period_fil2),'Baseline Pricing'] = (1+base_price_change_fil2)*qtr_edv.loc[qtr_edv['Year Quarter'].isin(test_period_fil2),'Baseline Pricing']\n",
    "                qtr_disc_copy = qtr_disc.copy()            \n",
    "                qtr_disc_copy['iter'] = i\n",
    "                if i == 1:                \n",
    "                    qtr_disc_copy.loc[qtr_disc_copy['Year Quarter'].isin(test_period_fil),'Discount (%)'] = (1+disc_change_fil)*qtr_disc.loc[qtr_disc['Year Quarter'].isin(test_period_fil),'Discount (%)']\n",
    "                elif i == 2:\n",
    "                    qtr_disc_copy.loc[qtr_disc_copy['Year Quarter'].isin(test_period_fil2),'Discount (%)'] = (1+disc_change_fil2)*qtr_disc.loc[qtr_disc['Year Quarter'].isin(test_period_fil2),'Discount (%)']\n",
    "\n",
    "                # Updated adcal prices\n",
    "                if i == 1:\n",
    "                    qtr_edv_new = qtr_edv_prev.merge(qtr_edv_copy, on = 'Year Quarter', how = 'left')\n",
    "                    qtr_edv_new = qtr_edv_new.rename(columns = {'Baseline Pricing_x':'Previous Baseline Pricing','Baseline Pricing_y':'Current Baseline Pricing'})\n",
    "                    # Updated adcal discounts\n",
    "                    qtr_disc_new = qtr_disc_prev.merge(qtr_disc_copy, on = 'Year Quarter', how = 'left')\n",
    "                    qtr_disc_new = qtr_disc_new.rename(columns = {'Discount (%)_x':'Previous Discount (%)', 'Discount (%)_y':'Current Discount (%)'})\n",
    "\n",
    "                elif i == 2:\n",
    "                    qtr_edv_new2 = qtr_edv_prev.merge(qtr_edv_copy, on = 'Year Quarter', how = 'left')\n",
    "                    qtr_edv_new2 = qtr_edv_new2.rename(columns = {'Baseline Pricing_x':'Previous Baseline Pricing','Baseline Pricing_y':'Current Baseline Pricing'})\n",
    "                    # Updated adcal discounts\n",
    "                    qtr_disc_new2 = qtr_disc_prev.merge(qtr_disc_copy, on = 'Year Quarter', how = 'left')\n",
    "                    qtr_disc_new2= qtr_disc_new2.rename(columns = {'Discount (%)_x':'Previous Discount (%)', 'Discount (%)_y':'Current Discount (%)'})\n",
    "\n",
    "            # ##################################### MERGING DATAFRAMES #####################################\n",
    "            qtr_price_disc_new = qtr_edv_new.merge(qtr_disc_new, on = 'Year Quarter', how = 'left')\n",
    "            qtr_price_disc_new2 = qtr_edv_new2.merge(qtr_disc_new2, on = 'Year Quarter', how = 'left')\n",
    "            qtr_price_disc_new = qtr_price_disc_new[['Year Quarter','Previous Baseline Pricing', 'Current Baseline Pricing', 'Previous Discount (%)','Current Discount (%)']]\n",
    "            qtr_price_disc_new2 = qtr_price_disc_new2[['Year Quarter','Previous Baseline Pricing', 'Current Baseline Pricing', 'Previous Discount (%)','Current Discount (%)']]\n",
    "            qtr_price_disc_new.fillna(0, inplace=True)\n",
    "            qtr_price_disc_new2.fillna(0, inplace=True)\n",
    "\n",
    "            qtr_price_disc_new['New Product'] = '1'\n",
    "            qtr_price_disc_new2['New Product'] = '2'\n",
    "\n",
    "            qtr_price_disc_new_merge = qtr_price_disc_new.append(qtr_price_disc_new2, ignore_index = True)\n",
    "            qtr_price_disc_new_merge = qtr_price_disc_new_merge[['New Product','Year Quarter','Previous Baseline Pricing','Current Baseline Pricing','Previous Discount (%)','Current Discount (%)']]            \n",
    "            \n",
    "            if toggle_np2.value == 'Yes':\n",
    "                display(widgets.Label(value=\"New Product 1 vs New Product 2\"),HTML(qtr_price_disc_new_merge.round(2).to_html(index=False)))        \n",
    "            else:\n",
    "                display(qtr_price_disc_new.round(2).astype(str).style.set_table_styles([{'selector' : '','props' : [('border','1px solid black')]}]))\n",
    "\n",
    "    else:        \n",
    "        print('Product is not present in that Region/Channel for selected period')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Existing Product Promotions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "    try:\n",
    "        if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "\n",
    "            Volume_dataset_all_reg_for_promo = Volume_dataset_all_reg.groupby(['Region','Channel','Store.Chain','Banner','Product','Test_period','Week'])['Front.Page','Middle.Page','Back.Page'].mean().reset_index()\n",
    "            Volume_dataset_all_reg_for_promo.rename(columns={'Front.Page':'Front Page','Middle.Page':'Middle Page','Back.Page':'Back Page'}, inplace=True)\n",
    "\n",
    "            qtr_promo = Volume_dataset_all_reg_for_promo[(Volume_dataset_all_reg_for_promo.Test_period >= start_week.children[8].value)\n",
    "                                              &(Volume_dataset_all_reg_for_promo.Test_period <= end_week.children[8].value)\n",
    "                                              &(Volume_dataset_all_reg_for_promo.Region.isin(Region_List))\n",
    "                                              &(Volume_dataset_all_reg_for_promo.Channel.isin(channel_list))\n",
    "                                              &(Volume_dataset_all_reg_for_promo.Product.isin([product_fil]))]\n",
    "\n",
    "            qtr_df = qtr_promo.groupby('Banner').agg({'Front Page':'sum','Middle Page':'sum','Back Page':'sum'}).reset_index() \n",
    "\n",
    "            banner_list = list(qtr_promo['Banner'].unique())\n",
    "            banner_max_df = pd.DataFrame()\n",
    "            for banner_var in banner_list:\n",
    "                max_limit = qtr_promo[qtr_promo['Banner']==banner_var].Week.nunique()\n",
    "                promo_given = (qtr_df.loc[qtr_df.Banner == banner_var,'Front Page'] + qtr_df.loc[qtr_df.Banner == banner_var,'Middle Page']  + qtr_df.loc[qtr_df.Banner == banner_var,'Back Page']).values[0]\n",
    "                avail_promo = max_limit - promo_given\n",
    "\n",
    "                banner_max_df = banner_max_df.append({'Banner':banner_var,'Total Promotions Available':avail_promo}, ignore_index = True)\n",
    "\n",
    "            qtr_df_final = pd.merge(qtr_df, banner_max_df, how='inner',on='Banner')\n",
    "            qtr_df_final['Existing Model Pack'] = product_fil\n",
    "            qtr_df_final = qtr_df_final[['Existing Model Pack','Banner','Front Page','Middle Page','Back Page','Total Promotions Available']]\n",
    "\n",
    "            # pd.options.display.precision = 0\n",
    "            display(HTML(qtr_df_final.to_html(index=False)))\n",
    "            print(colored('NOTE: The Total Promotions Available is the sum of promotions available for Front, Middle & Back Page promotions', 'red',attrs=['bold']))\n",
    "\n",
    "        elif toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "            if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "                clear_output()\n",
    "                print('Select 3 products as existing model pack')\n",
    "            else:\n",
    "                Volume_dataset_all_reg_for_promo = Volume_dataset_all_reg.groupby(['Region','Channel','Store.Chain','Banner','Product','Test_period','Week'])['Front.Page','Middle.Page','Back.Page'].mean().reset_index()\n",
    "                Volume_dataset_all_reg_for_promo.rename(columns={'Front.Page':'Front Page','Middle.Page':'Middle Page','Back.Page':'Back Page'}, inplace=True)\n",
    "\n",
    "                qtr_promo = Volume_dataset_all_reg_for_promo[(Volume_dataset_all_reg_for_promo.Test_period >= start_week.children[8].value)\n",
    "                                                  &(Volume_dataset_all_reg_for_promo.Test_period <= end_week.children[8].value)\n",
    "                                                  &(Volume_dataset_all_reg_for_promo.Region.isin(Region_List))\n",
    "                                                  &(Volume_dataset_all_reg_for_promo.Channel.isin(channel_list))\n",
    "                                                  &(Volume_dataset_all_reg_for_promo.Product.isin(prod_ensemble_list))]\n",
    "\n",
    "                qtr_df = qtr_promo.groupby(['Product','Banner']).agg({'Front Page':'sum','Middle Page':'sum','Back Page':'sum'}).reset_index() \n",
    "\n",
    "                banner_max_df = pd.DataFrame()\n",
    "                for exist_pack in prod_ensemble_list:\n",
    "                    banner_list = list(qtr_promo[qtr_promo.Product == exist_pack]['Banner'].unique())            \n",
    "                    qtr_promo_pack = qtr_promo[qtr_promo.Product == exist_pack]\n",
    "                    for banner_var in banner_list:\n",
    "\n",
    "                        max_limit = qtr_promo_pack[(qtr_promo_pack['Banner']==banner_var)].Week.nunique()\n",
    "                        promo_given = (qtr_df.loc[(qtr_df.Banner == banner_var) & (qtr_df.Product == exist_pack),'Front Page'] + qtr_df.loc[(qtr_df.Banner == banner_var)& (qtr_df.Product == exist_pack),'Middle Page']  + qtr_df.loc[(qtr_df.Banner == banner_var) & (qtr_df.Product == exist_pack),'Back Page']).values[0]\n",
    "                        avail_promo = max_limit - promo_given\n",
    "\n",
    "                        banner_max_df = banner_max_df.append({'Product':exist_pack,'Banner':banner_var,'Total Promotions Available':avail_promo}, ignore_index = True)\n",
    "\n",
    "                    qtr_df_final = pd.merge(qtr_df, banner_max_df, how='inner',on=['Banner','Product'])\n",
    "                    qtr_df_final = qtr_df_final[['Product','Banner','Front Page','Middle Page','Back Page','Total Promotions Available']]\n",
    "                    qtr_df_final.rename(columns = {'Product':'Existing Model Pack',}, inplace = True)\n",
    "                    qtr_df_final = qtr_df_final.sort_values('Existing Model Pack')\n",
    "            \n",
    "                display(HTML(qtr_df_final.to_html(index=False)))\n",
    "                print(colored('NOTE: The Total Promotions Available is the sum of promotions available for Front, Middle & Back Page promotions', 'red',attrs=['bold']))                        \n",
    "            \n",
    "    except:\n",
    "        print('Product is not present in that Region/Channel for selected period')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "\n",
    "    ############# Toggle for Promo change #############\n",
    "    promo_change_toggle = widgets.ToggleButtons(options=['Yes (All Banners)', 'Yes (Specific Banners)','No'],value='No',description='Do you want to change promotions?',\n",
    "        disabled=False,\n",
    "        display='flex',\n",
    "        flex_flow='column',\n",
    "        align_items='stretch',\n",
    "        style= {'description_width': 'initial'})\n",
    "\n",
    "    promo_change_toggle.style.button_width='auto'\n",
    "    promo_change_toggle.style.button_height='auto'\n",
    "\n",
    "    display(promo_change_toggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "\n",
    "    if (promo_change_toggle.value == 'Yes (All Banners)') | (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "        def run_all(ev):\n",
    "            display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index(),IPython.notebook.ncells()-19)'))\n",
    "\n",
    "        layout = widgets.Layout(width='auto', height='40px')\n",
    "        button = widgets.Button(description=\"Proceed further\", layout = layout)\n",
    "        button.on_click(run_all)\n",
    "        display(button)\n",
    "    else:\n",
    "        def run_all(ev):\n",
    "            display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index(),IPython.notebook.ncells()-15)'))\n",
    "\n",
    "        layout = widgets.Layout(width='auto', height='40px')\n",
    "        button = widgets.Button(description=\"Proceed further\", layout = layout)\n",
    "        button.on_click(run_all)\n",
    "        display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "\n",
    "    if (promo_change_toggle.value == 'Yes (All Banners)') | (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "        # In case no product is present in selected region-channel\n",
    "        if len(qtr_edv_baseline) != 0 :\n",
    "\n",
    "            if (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "                # Banner selection\n",
    "                specific_banner = sorted(list(qtr_df_final['Banner'].unique()))\n",
    "                specific_banner_list = specific_banner\n",
    "                banner_select = widgets.SelectMultiple(options=list(np.sort(specific_banner_list)), value = [specific_banner_list[0]],layout = layout3)\n",
    "\n",
    "            # Type-in filters for promotion increase\n",
    "            layout_promo = widgets.Layout(width='auto', height='auto')\n",
    "\n",
    "            # Product 1: Providing front,middle,back promotion edits\n",
    "            front_change = widgets.HBox([widgets.Label(value=\"Front Page Change\"), widgets.BoundedFloatText(value=0, min = -13, max = 13, disabled=False,layout = layout_promo)])\n",
    "            middle_change = widgets.HBox([widgets.Label(value=\"Middle Page Change\"), widgets.BoundedFloatText(value=0, min = -13, max = 13,  disabled=False,layout = layout_promo)])\n",
    "            back_change = widgets.HBox([widgets.Label(value=\"Back Page Change\"),widgets.BoundedFloatText(value=0, min = -13, max = 13, disabled=False,layout = layout_promo)])\n",
    "\n",
    "            # Product 2: Providing front,middle,back promotion edits\n",
    "            front_change2 = widgets.HBox([widgets.Label(value=\"Front Page Change\"), widgets.BoundedFloatText(value=0, min = -13, max = 13, disabled=False,layout = layout_promo)])\n",
    "            middle_change2 = widgets.HBox([widgets.Label(value=\"Middle Page Change\"), widgets.BoundedFloatText(value=0, min = -13, max = 13,  disabled=False,layout = layout_promo)])\n",
    "            back_change2 = widgets.HBox([widgets.Label(value=\"Back Page Change\"),widgets.BoundedFloatText(value=0, min = -13, max = 13, disabled=False,layout = layout_promo)])\n",
    "\n",
    "            # Product 3: Providing front,middle,back promotion edits\n",
    "            front_change3 = widgets.HBox([widgets.Label(value=\"Front Page Change\"), widgets.BoundedFloatText(value=0, min = -13, max = 13, disabled=False,layout = layout_promo)])\n",
    "            middle_change3 = widgets.HBox([widgets.Label(value=\"Middle Page Change\"), widgets.BoundedFloatText(value=0, min = -13, max = 13,  disabled=False,layout = layout_promo)])\n",
    "            back_change3 = widgets.HBox([widgets.Label(value=\"Back Page Change\"),widgets.BoundedFloatText(value=0, min = -13, max = 13, disabled=False,layout = layout_promo)])\n",
    "\n",
    "            if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach                \n",
    "                # Show selections as per specific banner\n",
    "                if (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "                    display(widgets.HBox([widgets.Label(value=\"Select Banners\"),banner_select]))\n",
    "                    # Show New product 1, new product 2 promo change\n",
    "                    if toggle_np2.value == 'Yes':\n",
    "                        display(widgets.Label(value=\"$New Product 1$\"),widgets.HBox([front_change,middle_change,back_change]))\n",
    "                        display(widgets.Label(value=\"$New Product 2$\"),widgets.HBox([front_change2,middle_change2,back_change2]))\n",
    "                    else:\n",
    "                        display(widgets.HBox([front_change,middle_change,back_change]))\n",
    "\n",
    "                if (promo_change_toggle.value == 'Yes (All Banners)'):\n",
    "                    # Show New product 1, new product 2 promo change\n",
    "                    if toggle_np2.value == 'Yes':\n",
    "                        display(widgets.Label(value=\"$New Product 1$\"),widgets.HBox([front_change,middle_change,back_change]))\n",
    "                        display(widgets.Label(value=\"$New Product 2$\"),widgets.HBox([front_change2,middle_change2,back_change2]))\n",
    "                    else:\n",
    "                        display(widgets.HBox([front_change,middle_change,back_change]))\n",
    "                        \n",
    "            elif toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "                    clear_output()\n",
    "                    print('Select 3 products as existing model pack')\n",
    "                else:\n",
    "\n",
    "                    # Show selections as per specific banner\n",
    "                    if (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "                        display(widgets.HBox([widgets.Label(value=\"Select Banners\"),banner_select]))\n",
    "                        # Show model pack 1,modle pack 2, model pack 3 promo change\n",
    "                        display(widgets.Label(value=\"$Model Pack 1$\" + ': ' + edv_disc_join['Existing Model Pack'].unique()[0]),widgets.HBox([front_change,middle_change,back_change]))\n",
    "                        display(widgets.Label(value=\"$Model Pack 2$\" + ': ' + edv_disc_join['Existing Model Pack'].unique()[1]),widgets.HBox([front_change2,middle_change2,back_change2]))\n",
    "                        display(widgets.Label(value=\"$Model Pack 3$\" + ': ' + edv_disc_join['Existing Model Pack'].unique()[2]),widgets.HBox([front_change3,middle_change3,back_change3]))\n",
    "\n",
    "\n",
    "                    if (promo_change_toggle.value == 'Yes (All Banners)'):\n",
    "                        # Show model pack 1,modle pack 2, model pack 3 promo change\n",
    "                        display(widgets.Label(value=\"$Model Pack 1$\" + ': ' + edv_disc_join['Existing Model Pack'].unique()[0]),widgets.HBox([front_change,middle_change,back_change]))\n",
    "                        display(widgets.Label(value=\"$Model Pack 2$\" + ': ' + edv_disc_join['Existing Model Pack'].unique()[1]),widgets.HBox([front_change2,middle_change2,back_change2]))\n",
    "                        display(widgets.Label(value=\"$Model Pack 3$\" + ': ' + edv_disc_join['Existing Model Pack'].unique()[2]),widgets.HBox([front_change3,middle_change3,back_change3]))                \n",
    "\n",
    "            # Banners with 0 promotions\n",
    "            banner_promos = Volume_dataset_all_reg.groupby('Banner')['Front.Page','Middle.Page','Back.Page'].sum().reset_index()\n",
    "            fp_remove_banner = list(banner_promos[banner_promos['Front.Page'] == 0].Banner.unique())\n",
    "            mp_remove_banner = list(banner_promos[banner_promos['Middle.Page'] == 0].Banner.unique())\n",
    "            bp_remove_banner = list(banner_promos[banner_promos['Back.Page'] == 0].Banner.unique())\n",
    "\n",
    "            # In case zero banner exist for existing product then pop up the question\n",
    "            if len(qtr_df_final[(qtr_df_final.Banner.isin(fp_remove_banner)) | (qtr_df_final.Banner.isin(mp_remove_banner)) | (qtr_df_final.Banner.isin(bp_remove_banner))]) != 0:\n",
    "                # Increase for zero banner promos?\n",
    "                zero_banner_promo_inc = widgets.ToggleButtons(options=['Yes','No'],value='No',description='Do you want to increase for banners which have never given promotions?',\n",
    "                disabled=False,\n",
    "                display='flex',\n",
    "                flex_flow='column',\n",
    "                align_items='stretch',\n",
    "                style= {'description_width': 'initial'})\n",
    "\n",
    "                zero_banner_promo_inc.style.button_width='auto'\n",
    "                zero_banner_promo_inc.style.button_height='auto'\n",
    "\n",
    "                display(zero_banner_promo_inc)\n",
    "\n",
    "        else:\n",
    "            print(\"Product is not present in that Region/Channel for selected period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "\n",
    "    if (promo_change_toggle.value == 'Yes (All Banners)') | (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "        def run_all(ev):\n",
    "            display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1,IPython.notebook.ncells()-15)'))\n",
    "\n",
    "        layout = widgets.Layout(width='auto', height='40px')\n",
    "        button = widgets.Button(description=\"Apply changes\", layout = layout)\n",
    "        button.on_click(run_all)\n",
    "        display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "\n",
    "    Test_week_list = {\"2019Q1\" : [\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-10\",\"2019-11\",\"2019-12\",\"2019-13\"],\n",
    "     \"2019Q2\" : [\"2019-14\",\"2019-15\",\"2019-16\",\"2019-17\",\"2019-18\",\"2019-19\",\"2019-20\",\"2019-21\",\"2019-22\",\"2019-23\",\"2019-24\",\"2019-25\",\"2019-26\"],\n",
    "     \"2019Q3\" : [\"2019-27\",\"2019-28\",\"2019-29\",\"2019-30\",\"2019-31\",\"2019-32\",\"2019-33\",\"2019-34\",\"2019-35\",\"2019-36\",\"2019-37\",\"2019-38\",\"2019-39\"],\n",
    "     \"2019Q4\" : [\"2019-40\",\"2019-41\",\"2019-42\",\"2019-43\",\"2019-44\",\"2019-45\",\"2019-46\",\"2019-47\",\"2019-48\",\"2019-49\",\"2019-50\",\"2019-51\",\"2019-52\"],\n",
    "                     \n",
    "     \"2020Q1\" : [\"2020-01\",\"2020-02\",\"2020-03\",\"2020-04\",\"2020-05\",\"2020-06\",\"2020-07\",\"2020-08\",\"2020-09\",\"2020-10\",\"2020-11\",\"2020-12\",\"2020-13\"],\n",
    "     \"2020Q2\" : [\"2020-14\",\"2020-15\",\"2020-16\",\"2020-17\",\"2020-18\",\"2020-19\",\"2020-20\",\"2020-21\",\"2020-22\",\"2020-23\",\"2020-24\",\"2020-25\",\"2020-26\"],\n",
    "     \"2020Q3\" : [\"2020-27\",\"2020-28\",\"2020-29\",\"2020-30\",\"2020-31\",\"2020-32\",\"2020-33\",\"2020-34\",\"2020-35\",\"2020-36\",\"2020-37\",\"2020-38\",\"2020-39\"],\n",
    "     \"2020Q4\" : [\"2020-40\",\"2020-41\",\"2020-42\",\"2020-43\",\"2020-44\",\"2020-45\",\"2020-46\",\"2020-47\",\"2020-48\",\"2020-49\",\"2020-50\",\"2020-51\",\"2020-52\"],\n",
    "     \"fulltrain\" : []}\n",
    "\n",
    "    Test_periods = [\"2019Q4\",\"2019Q1\",\"2019Q2\",\"2019Q3\",\"2020Q4\",\"2020Q1\",\"2020Q2\",\"2020Q3\"]\n",
    "\n",
    "    # Initializing dataframe\n",
    "    Test_results = pd.DataFrame()\n",
    "    \n",
    "    ban_week_fp_copy = pd.DataFrame()    \n",
    "    ban_week_mp_copy = pd.DataFrame()    \n",
    "    ban_week_bp_copy = pd.DataFrame()\n",
    "\n",
    "    ban_week_fp2 = pd.DataFrame({'Banner': pd.Series([], dtype='object'),'Week': pd.Series([], dtype='object'),'ban_week_fp_check': pd.Series([], dtype='object')})\n",
    "    ban_week_mp2 = pd.DataFrame({'Banner': pd.Series([], dtype='object'),'Week': pd.Series([], dtype='object'),'ban_week_mp_check': pd.Series([], dtype='object')})\n",
    "    ban_week_bp2 = pd.DataFrame({'Banner': pd.Series([], dtype='object'),'Week': pd.Series([], dtype='object')})\n",
    "\n",
    "    front_page_promo_weeks_all = pd.DataFrame()\n",
    "    middle_page_promo_weeks_all = pd.DataFrame() \n",
    "    back_page_promo_weeks_all = pd.DataFrame()\n",
    "    Test_results_sim = pd.DataFrame()\n",
    "    \n",
    "    random.seed(0)\n",
    "   \n",
    "    #########################################################################################################\n",
    "    # Normal Simulation at banner-week-product level\n",
    "    #########################################################################################################\n",
    "    \n",
    "    if len(qtr_edv_baseline) != 0 :  # In case there is not product present or product not present in selected region-channel & test period\n",
    "        \n",
    "        if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "            prod_ensemble_list = [product_fil]\n",
    "        \n",
    "        for product_fil in sorted(prod_ensemble_list): # Loop over product_fil\n",
    "            ban_week_fp = pd.DataFrame({'Banner': pd.Series([], dtype='object'),'Week': pd.Series([], dtype='object'),'ban_week_fp_check': pd.Series([], dtype='object')})\n",
    "            ban_week_mp = pd.DataFrame({'Banner': pd.Series([], dtype='object'),'Week': pd.Series([], dtype='object'),'ban_week_mp_check': pd.Series([], dtype='object')})\n",
    "            ban_week_bp = pd.DataFrame({'Banner': pd.Series([], dtype='object'),'Week': pd.Series([], dtype='object')})\n",
    "            \n",
    "            if (promo_change_toggle.value == 'Yes (All Banners)') | (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "                if (front_change.children[1].value != 0) | (middle_change.children[1].value != 0) | (back_change.children[1].value != 0) | (front_change2.children[1].value != 0) | (middle_change2.children[1].value != 0) | (back_change2.children[1].value != 0) | (front_change3.children[1].value != 0) | (middle_change3.children[1].value != 0) | (back_change3.children[1].value != 0) : \n",
    "\n",
    "                    display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Updating Promotions </h2></div>'))\n",
    "\n",
    "                    if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                        if toggle_np2.value == 'Yes':\n",
    "                            iter_change_list = [1,2]\n",
    "                        elif toggle_np2.value == 'No':\n",
    "                            iter_change_list = [1]\n",
    "                    else:\n",
    "                        iter_change_list = [1]\n",
    "\n",
    "                    for i in iter_change_list:\n",
    "                        #Defining empty dataframes for model training\n",
    "                        combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                        Test_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                        Test_Data = pd.DataFrame()\n",
    "\n",
    "                        #Model training and prediction\n",
    "                        for Region_key in Region_List:\n",
    "                            # In case the product is not present at that region\n",
    "                            if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                                continue\n",
    "\n",
    "                            Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                                       &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                            Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                            required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                            combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                            combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                                     (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                            combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                            combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                            ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                            banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                            # Uploaded data banner dummies\n",
    "                            banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                            # If all banners are not present for the training data\n",
    "                            c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                            if len(c) != 0:\n",
    "                                banner_dummies[c] = 0\n",
    "\n",
    "                            # Reordering columns                \n",
    "                            banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                            combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                            ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                            combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                            combined_dataset.sort_values('Week',inplace = True)\n",
    "                            combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                            combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                            combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                            #Adding dummies for category\n",
    "                            category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                            # Adding dummies for product attribute - Category\n",
    "                            pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                            # Uploaded data category dummies\n",
    "                            category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                            # If all pack subtypes are not present for the training data\n",
    "                            c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                            if len(c) != 0:\n",
    "                                category_dummies[c] = 0\n",
    "\n",
    "                            # Reordering columns                \n",
    "                            category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                            combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                            combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                            # Adding dummies for product attribute - pack subtype\n",
    "                            pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                            # Uploaded data pack subtype dummies\n",
    "                            pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                            # If all pack subtypes are not present for the training data\n",
    "                            c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                            if len(c) != 0:\n",
    "                                pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                            # Reordering columns                \n",
    "                            pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                            combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                            combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                            # Adding pack content dummies\n",
    "                            pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                            # Uploaded data pack content dummies\n",
    "                            pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                            # If all pack contents are not present for the training data\n",
    "                            c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                            if len(c) != 0:\n",
    "                                pack_content_dummies[c] = 0\n",
    "\n",
    "                            # Reordering columns                \n",
    "                            pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                            combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                            combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                            combined_dataset_test = combined_dataset.copy()\n",
    "\n",
    "                            # Product stage dummies\n",
    "                            stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                            combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                            combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                            # Changing product stage as per start date provided by the user\n",
    "                            prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                            # Getting updated product stages\n",
    "                            combined_dataset_test_check = combined_dataset_test[combined_dataset_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                            combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                            combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                            combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                            combined_dataset_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                            # Product stage dummies\n",
    "                            stage_dummies_test = pd.get_dummies(combined_dataset_test['Intial_weeks'])\n",
    "\n",
    "                            # If all product stages are not present for the uploaded data\n",
    "                            c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                            if len(c) != 0 :\n",
    "                                stage_dummies_test[c] = 0\n",
    "\n",
    "                            # Reordering columns                \n",
    "                            stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                            combined_dataset_test = pd.concat([combined_dataset_test, stage_dummies_test], axis=1)\n",
    "                            combined_dataset_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                            # CHANGES : For adcal discounts\n",
    "                            if 'All' in list(test_period.value):\n",
    "                                # Discount change\n",
    "                                if i == 1:                \n",
    "                                    combined_dataset_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test['Adcal_DD']\n",
    "                                    combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "                                elif i == 2:\n",
    "                                    combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                                    combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']          \n",
    "                            else:                \n",
    "                                if i == 1:                \n",
    "                                    # Discount change\n",
    "                                    combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil)]['Adcal_DD']\n",
    "                                    # Baseline pricing\n",
    "                                    combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                                elif i == 2:\n",
    "                                    # Discount change\n",
    "                                    combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                                    # Baseline pricing\n",
    "                                    combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "\n",
    "                            #Adding discount depth columns\n",
    "                            combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                  else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                            combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                             else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                            combined_dataset_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                  else 0 for x in combined_dataset_test[\"Adcal_DD\"]]\n",
    "                            combined_dataset_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                             else 0 for x in combined_dataset_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                            # Adcal Price calculation\n",
    "                            # In case the discounts go over 100%, limit to 100%\n",
    "                            combined_dataset_test.loc[combined_dataset_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "\n",
    "                            combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                            # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                            combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                            # Adcal Price calculation\n",
    "                            combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                            # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                            combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                            # Dropping adcal DD as DD1 & DD2 are added\n",
    "                            combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                            combined_dataset_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                            # Dropping null volumes\n",
    "                            complete_dataset = combined_dataset.copy()\n",
    "                            complete_dataset_test = combined_dataset_test.copy()\n",
    "\n",
    "                            complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "                            complete_dataset_test = complete_dataset_test.loc[complete_dataset_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                            complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                            complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                            complete_dataset_test['Adcal_Price'] = np.log(complete_dataset_test['Adcal_Price'])\n",
    "                            complete_dataset_test['Eq.Unit.Sales'] = np.log(complete_dataset_test['Eq.Unit.Sales'])\n",
    "\n",
    "                            # TEST PERIOD LOOP\n",
    "                            Test_periods = ['2019Q1','2019Q2','2019Q3','2019Q4']\n",
    "\n",
    "                            for Test_period in Test_periods:\n",
    "                                if (Test_period>end_week.children[8].value) | (Test_period > Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].max()):\n",
    "                                    continue\n",
    "                                if (Test_period<start_week.children[8].value) | (Test_period < Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].min()):\n",
    "                                    continue\n",
    "\n",
    "                                #Filtering test weeks\n",
    "                                Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                                # Dividing train & test data\n",
    "                                complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                                complete_test_data_set = complete_dataset_test.loc[complete_dataset_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                                # Dropping unneccessary columns from train dataset\n",
    "                                train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                                train_data_brand = train_data_set.copy()\n",
    "                                train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Filtering for existing product\n",
    "                                test_data_set = complete_test_data_set[complete_test_data_set.Product == product_fil].copy()\n",
    "\n",
    "                                # New Category\n",
    "                                if i == 1:\n",
    "                                    for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                        if (cols == category_drop.value):                      \n",
    "                                            test_data_set[cols] = 1\n",
    "                                        else:\n",
    "                                            test_data_set[cols] = 0\n",
    "                                if i == 2:\n",
    "                                    for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                        if (cols == category_drop2.value):                      \n",
    "                                            test_data_set[cols] = 1\n",
    "                                        else:\n",
    "                                            test_data_set[cols] = 0\n",
    "\n",
    "                                # New Pack Subtype\n",
    "                                if i == 1:\n",
    "                                    for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                        if (cols == pack_subtype_drop.value):                      \n",
    "                                            test_data_set[cols] = 1\n",
    "                                        else:\n",
    "                                            test_data_set[cols] = 0\n",
    "                                if i == 2:\n",
    "                                    for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                        if (cols == pack_subtype_drop2.value):                      \n",
    "                                            test_data_set[cols] = 1\n",
    "                                        else:\n",
    "                                            test_data_set[cols] = 0\n",
    "\n",
    "                                # New Pack Content\n",
    "                                if i == 1:\n",
    "                                    for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                        if (cols == pack_content_drop.value):                      \n",
    "                                            test_data_set[cols] = 1\n",
    "                                        else:\n",
    "                                            test_data_set[cols] = 0\n",
    "                                if i == 2:\n",
    "                                    for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                        if (cols == pack_content_drop2.value):                      \n",
    "                                            test_data_set[cols] = 1\n",
    "                                        else:\n",
    "                                            test_data_set[cols] = 0\n",
    "\n",
    "                                test_data_brand = test_data_set.copy()\n",
    "\n",
    "                                if i == 1 :\n",
    "                                    # For New SIZE\n",
    "                                    test_data_brand['SIZE_ML'] = size.value\n",
    "\n",
    "                                    # For New Count\n",
    "                                    test_data_brand['COUNT'] = count.value\n",
    "                                elif i == 2:\n",
    "                                    # For New SIZE\n",
    "                                    test_data_brand['SIZE_ML'] = size2.value\n",
    "\n",
    "                                    # For New Count\n",
    "                                    test_data_brand['COUNT'] = count2.value\n",
    "\n",
    "                                # Dropping unneccessary columns from test dataset\n",
    "                                test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                #Creating test and train data - independent & dependent columns\n",
    "                                X_train = train_data_brand\n",
    "                                y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                                X_test = test_data_brand\n",
    "                                y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                                # Appending test data\n",
    "                                test_data_set[\"Region\"] = Region_key\n",
    "                                test_data_set[\"Test_period\"] = Test_period\n",
    "                                # In case the product is not present in certain region-quarter\n",
    "                                if len(test_data_set) == 0:\n",
    "                                    continue\n",
    "\n",
    "                                Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "\n",
    "                                if((Region_key == 'EAST') and (Test_period == '2019Q1')):\n",
    "                                        rf = model_object_EAST_2019Q1_v2\n",
    "                                elif((Region_key == 'EAST') and (Test_period == '2019Q2')):\n",
    "                                        rf = model_object_EAST_2019Q2_v2\n",
    "                                elif((Region_key == 'EAST') and (Test_period == '2019Q3')):\n",
    "                                        rf = model_object_EAST_2019Q3_v2\n",
    "                                elif((Region_key == 'EAST') and (Test_period == '2019Q4')):\n",
    "                                        rf = model_object_EAST_2019Q4_v2\n",
    "                                elif((Region_key == 'ONTARIO') and (Test_period == '2019Q1')):\n",
    "                                        rf = model_object_ONTARIO_2019Q1_v2\n",
    "                                elif((Region_key == 'ONTARIO') and (Test_period == '2019Q2')):\n",
    "                                        rf = model_object_ONTARIO_2019Q2_v2\n",
    "                                elif((Region_key == 'ONTARIO') and (Test_period == '2019Q3')):\n",
    "                                        rf = model_object_ONTARIO_2019Q3_v2\n",
    "                                elif((Region_key == 'ONTARIO') and (Test_period == '2019Q4')):\n",
    "                                        rf = model_object_ONTARIO_2019Q4_v2\n",
    "                                elif((Region_key == 'QUEBEC') and (Test_period == '2019Q1')):\n",
    "                                        rf = model_object_QUEBEC_2019Q1_v2\n",
    "                                elif((Region_key == 'QUEBEC') and (Test_period == '2019Q2')):\n",
    "                                        rf = model_object_QUEBEC_2019Q2_v2\n",
    "                                elif((Region_key == 'QUEBEC') and (Test_period == '2019Q3')):\n",
    "                                        rf = model_object_QUEBEC_2019Q3_v2\n",
    "                                elif((Region_key == 'QUEBEC') and (Test_period == '2019Q4')):\n",
    "                                        rf = model_object_QUEBEC_2019Q4_v2\n",
    "                                elif((Region_key == 'WEST') and (Test_period == '2019Q1')):\n",
    "                                        rf = model_object_WEST_2019Q1_v2\n",
    "                                elif((Region_key == 'WEST') and (Test_period == '2019Q2')):\n",
    "                                        rf = model_object_WEST_2019Q2_v2\n",
    "                                elif((Region_key == 'WEST') and (Test_period == '2019Q3')):\n",
    "                                        rf = model_object_WEST_2019Q3_v2\n",
    "                                else:\n",
    "                                        rf = model_object_WEST_2019Q4_v2\n",
    "\n",
    "                                #Model predictions\n",
    "                                predictions_rf = rf.predict(X_test)                \n",
    "\n",
    "                                #Storing test results\n",
    "                                result = X_test.copy()\n",
    "                                result[\"Predictions_rf\"] = predictions_rf\n",
    "                                result[\"Actuals\"] = y_test\n",
    "\n",
    "                                result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                                result['Actuals'] = np.exp(result['Actuals'])                                                    \n",
    "\n",
    "                                #Calculating APE for the models\n",
    "                                result[\"ape_rf\"] = (result[\"Predictions_rf\"]-result[\"Actuals\"]).abs()\n",
    "\n",
    "                                result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                                result[\"Region\"] = Region_key\n",
    "                                result[\"Test_period\"] = Test_period\n",
    "                                result[\"Banner\"] = test_data_set[\"Banner\"]\n",
    "                                result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                                result[\"DD_1\"] = test_data_set['DD_1']\n",
    "                                result[\"DD_2\"] = test_data_set['DD_2']\n",
    "                                result[\"Front.Page\"] = test_data_set[\"Front.Page\"]\n",
    "                                result[\"Middle.Page\"] = test_data_set[\"Middle.Page\"]\n",
    "                                result[\"Back.Page\"] = test_data_set[\"Back.Page\"]\n",
    "                                result[\"Channel\"] = test_data_set['Channel']\n",
    "                                result[\"iter\"] = i\n",
    "                                ##Columns for Revenue                            \n",
    "                                result[\"Adcal_Price\"] = test_data_set[\"Adcal_Price\"]\n",
    "\n",
    "                                result = result[[\"Region\",\"Banner\",\"Product\",'Channel',\"Test_period\",\"Week\",'Front.Page','Middle.Page','Back.Page',\"Actuals\",\"ape_rf\",\"Predictions_rf\",\"DD_1\",\"DD_2\",\"Adcal_Price\",'iter']]\n",
    "\n",
    "                                #Appending results to the final results dataframe\n",
    "                                Test_results = Test_results.append(result, ignore_index=True)\n",
    "\n",
    "                            Test_results_sim = Test_results.rename(columns = {'Predictions_rf':'Baseline Prediction'})\n",
    "\n",
    "                ##########################################################################################################   \n",
    "                                                #  FRONT PAGE INCREASE SIMULATION \n",
    "                ##########################################################################################################    \n",
    "                        if i == 1:\n",
    "                            change_fp = front_change.children[1].value\n",
    "                        elif i == 2:\n",
    "                            change_fp = front_change2.children[1].value\n",
    "                       \n",
    "                        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                            if product_fil == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                                change_fp = front_change.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                                change_fp = front_change2.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                                change_fp = front_change3.children[1].value\n",
    "\n",
    "                        if change_fp > 0:    \n",
    "\n",
    "                            ################## FRONT PAGE SIMULATION ##################\n",
    "                            import warnings\n",
    "                            warnings.filterwarnings('ignore')\n",
    "\n",
    "                            # Defining empty dataframes for model training\n",
    "                            All_coefs = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Train_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Test_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            train_data_brand = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            Test_Data = pd.DataFrame()    \n",
    "\n",
    "                            #Model training and prediction\n",
    "                            for Region_key in Region_List:\n",
    "                                # In case the product is not present at that region\n",
    "                                if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                                    continue\n",
    "\n",
    "                                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                                combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                                ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                                banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                                # Uploaded data banner dummies\n",
    "                                banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                                # If all banners are not present for the training data\n",
    "                                c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    banner_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                combined_dataset.sort_values('Week',inplace = True)\n",
    "                                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                                #Adding dummies for category\n",
    "                                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                                # Adding dummies for product attribute - Category\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data category dummies\n",
    "                                category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    category_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding dummies for product attribute - pack subtype\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data pack subtype dummies\n",
    "                                pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding pack content dummies\n",
    "                                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                                # Uploaded data pack content dummies\n",
    "                                pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                                # If all pack contents are not present for the training data\n",
    "                                c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_content_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Changing product stage as per start date provided by the user\n",
    "                                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                                # Getting updated product stages\n",
    "                                combined_dataset_test_check = combined_dataset_test[combined_dataset_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies_test = pd.get_dummies(combined_dataset_test['Intial_weeks'])\n",
    "\n",
    "                                # If all product stages are not present for the uploaded data\n",
    "                                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                                if len(c) != 0 :\n",
    "                                    stage_dummies_test[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                                combined_dataset_test = pd.concat([combined_dataset_test, stage_dummies_test], axis=1)\n",
    "                                combined_dataset_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # CHANGES : For adcal discounts\n",
    "                                if 'All' in list(test_period.value):\n",
    "                                    # Discount change\n",
    "                                    if i == 1:                \n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']          \n",
    "                                else:                \n",
    "                                    if i == 1:                \n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil)]['Adcal_DD'] \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "\n",
    "                                #Adding discount depth columns\n",
    "                                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                                combined_dataset_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset_test[\"Adcal_DD\"]]\n",
    "                                combined_dataset_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                # In case the discounts go over 100%, limit to 100%\n",
    "                                combined_dataset_test.loc[combined_dataset_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "\n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                                combined_dataset_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Dropping null volumes\n",
    "                                complete_dataset = combined_dataset.copy()\n",
    "                                complete_dataset_test = combined_dataset_test.copy()\n",
    "\n",
    "                                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "                                complete_dataset_test = complete_dataset_test.loc[complete_dataset_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                                complete_dataset_test['Adcal_Price'] = np.log(complete_dataset_test['Adcal_Price'])\n",
    "                                complete_dataset_test['Eq.Unit.Sales'] = np.log(complete_dataset_test['Eq.Unit.Sales'])\n",
    "\n",
    "                                # TEST PERIOD LOOP\n",
    "                                Test_periods = ['2019Q1','2019Q2','2019Q3','2019Q4']\n",
    "\n",
    "                                for Test_period in Test_periods:\n",
    "                                    if (Test_period>end_week.children[8].value) | (Test_period > Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].max()):\n",
    "                                        continue\n",
    "                                    if (Test_period<start_week.children[8].value) | (Test_period < Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].min()):\n",
    "                                        continue\n",
    "\n",
    "                                    #Filtering test weeks\n",
    "                                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                                    # Dividing train & test data\n",
    "                                    complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                                    complete_test_data_set = complete_dataset_test.loc[complete_dataset_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                                    # Dropping unneccessary columns from train dataset\n",
    "                                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                                    train_data_brand = train_data_set.copy()\n",
    "                                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    # Filtering for existing product\n",
    "                                    test_data_set = complete_test_data_set[complete_test_data_set.Product == product_fil].copy()\n",
    "\n",
    "                                    #################### INCREASING PROMOTIONS FOR BANNER WEEKS COMBINATIONS ####################\n",
    "                                    # Get banner week promos\n",
    "                                    banner_week_promos_combo = test_data_set[['Banner','Week','Front.Page','Middle.Page','Back.Page']].drop_duplicates()             \n",
    "                                    banner_week_upc = banner_week_promos_combo.copy()\n",
    "                                    # Get rows where there is availability of increase in prmotions\n",
    "                                    avail_promo_weeks = banner_week_upc[(banner_week_upc['Front.Page'] == 0) & (banner_week_upc['Middle.Page'] == 0) & (banner_week_upc['Back.Page'] == 0)]\n",
    "                                    # Increasing 1 front page promotions for every week\n",
    "                                    avail_promo_weeks['Front.Page'] = 1\n",
    "                                    # Merge to get required increase in banner-week combintation\n",
    "                                    test_data_set_promo_check = test_data_set.merge(avail_promo_weeks[['Banner','Week','Front.Page']], how = 'left', on = ['Banner','Week'])\n",
    "                                    # Filling na with existing values in Front.Page_y\n",
    "                                    test_data_set_promo_check['Front.Page_y'].fillna(test_data_set_promo_check['Front.Page_x'], inplace = True)\n",
    "                                    # Replacing values of 'Front.Page_x' with 'Front.Page_y' wherever it was zero previously\n",
    "                                    test_data_set_promo_check['Front.Page_x'] = np.where(test_data_set_promo_check['Front.Page_x'] == 0, test_data_set_promo_check['Front.Page_y'], test_data_set_promo_check['Front.Page_x'])\n",
    "                                    # Drop 'Front.Page_y' \n",
    "                                    test_data_set_promo_check.drop('Front.Page_y', axis = 1, inplace =True)\n",
    "                                    # Renaming 'Front.Page_x'\n",
    "                                    test_data_set_promo_check.rename(columns = {'Front.Page_x':'Front.Page'}, inplace = True)\n",
    "\n",
    "                                    test_data_set = test_data_set_promo_check.copy()   \n",
    "\n",
    "                                    # New Category\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0                    \n",
    "\n",
    "                                    # New Pack Subtype\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    # New Pack Content\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    test_data_brand = test_data_set.copy()\n",
    "\n",
    "                                    if i == 1 :\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count.value\n",
    "                                    elif i == 2:\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size2.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count2.value\n",
    "\n",
    "                                    # Dropping unneccessary columns from test dataset\n",
    "                                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    #Creating test and train data - independent & dependent columns\n",
    "                                    X_train = train_data_brand\n",
    "                                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                                    X_test = test_data_brand\n",
    "                                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                                    # Appending test data\n",
    "                                    test_data_set[\"Region\"] = Region_key\n",
    "                                    test_data_set[\"Test_period\"] = Test_period\n",
    "                                    # In case the product is not present in certain region-quarter\n",
    "                                    if len(test_data_set) == 0:\n",
    "                                        continue\n",
    "\n",
    "                                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "\n",
    "                                    if((Region_key == 'EAST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_EAST_2019Q1_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_EAST_2019Q2_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_EAST_2019Q3_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_EAST_2019Q4_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_ONTARIO_2019Q1_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_ONTARIO_2019Q2_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_ONTARIO_2019Q3_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_ONTARIO_2019Q4_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_QUEBEC_2019Q1_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_QUEBEC_2019Q2_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_QUEBEC_2019Q3_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_QUEBEC_2019Q4_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_WEST_2019Q1_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_WEST_2019Q2_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_WEST_2019Q3_v2\n",
    "                                    else:\n",
    "                                            rf = model_object_WEST_2019Q4_v2\n",
    "\n",
    "                                    #Model predictions\n",
    "                                    predictions_rf = rf.predict(X_test)                \n",
    "\n",
    "                                    #Storing test results\n",
    "                                    result = X_test.copy()\n",
    "                                    result[\"Predictions_rf\"] = predictions_rf\n",
    "                                    result[\"Actuals\"] = y_test\n",
    "\n",
    "                                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                                    result['Actuals'] = np.exp(result['Actuals'])                                                    \n",
    "\n",
    "                                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                                    result[\"Region\"] = Region_key\n",
    "                                    result[\"Test_period\"] = Test_period\n",
    "                                    result[\"Banner\"] = test_data_set[\"Banner\"]\n",
    "                                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                                    result['iter'] = i\n",
    "\n",
    "                                    result = result[[\"Region\",\"Banner\",\"Product\",'Channel',\"Test_period\",\"Week\",\"Actuals\",\"Predictions_rf\",'iter']]\n",
    "\n",
    "                                    #Appending results to the final results dataframe\n",
    "                                    Test_results = Test_results.append(result, ignore_index=True)\n",
    "\n",
    "                            Test_results = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]\n",
    "                            # Removing 0 promotion banners\n",
    "                            if len(qtr_df_final[(qtr_df_final.Banner.isin(fp_remove_banner)) | (qtr_df_final.Banner.isin(mp_remove_banner)) | (qtr_df_final.Banner.isin(bp_remove_banner))]) != 0:\n",
    "                                if zero_banner_promo_inc.value == 'No':\n",
    "                                    Test_results = Test_results[~Test_results.Banner.isin(fp_remove_banner)]\n",
    "\n",
    "                            # Merge & get prediction difference column\n",
    "                            Test_results = Test_results[Test_results.iter == i].merge(Test_results_sim[Test_results_sim.iter == i][['Banner','Week','Baseline Prediction']], on = ['Banner','Week'], how = 'left')        \n",
    "                            Test_results['Diff'] = Test_results[Test_results.iter == i]['Predictions_rf'] - Test_results[Test_results.iter == i]['Baseline Prediction']\n",
    "                            # Consider positive difference only\n",
    "                            Test_results = Test_results[Test_results.iter == i][Test_results[Test_results.iter == i].Diff > 0]\n",
    "                            Test_results = Test_results[Test_results.iter == i].sort_values('Diff', ascending = False)\n",
    "\n",
    "                            # Select required number of banner-weeks where promotion increase would occur\n",
    "                            if (promo_change_toggle.value == 'Yes (Specific Banners)'):         \n",
    "                                Test_results = Test_results[Test_results.Banner.isin(list(banner_select.value))]\n",
    "                                for banner_loop in banner_select.value:    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_fp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_fp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_fp = ban_week_fp.append(ban_week_fp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_fp['ban_week_fp_check'] = ban_week_fp['Banner'] + ban_week_fp['Week']                            \n",
    "\n",
    "                                    elif i == 2:\n",
    "                                        ban_week_fp2 = ban_week_fp2.append(ban_week_fp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_fp2['ban_week_fp_check'] = ban_week_fp2['Banner'] + ban_week_fp2['Week']                             \n",
    "\n",
    "                            elif (promo_change_toggle.value == 'Yes (All Banners)'):\n",
    "                                for banner_loop in qtr_df_final.Banner.unique():    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_fp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_fp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_fp = ban_week_fp.append(ban_week_fp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_fp['ban_week_fp_check'] = ban_week_fp['Banner'] + ban_week_fp['Week']                            \n",
    "                                    elif i == 2:\n",
    "                                        ban_week_fp2 = ban_week_fp2.append(ban_week_fp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_fp2['ban_week_fp_check'] = ban_week_fp2['Banner'] + ban_week_fp2['Week']\n",
    "                           \n",
    "                            if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach                        \n",
    "                                ban_week_fp['Product'] = product_fil\n",
    "                            else:\n",
    "                                if i == 1:\n",
    "                                    ban_week_fp['Product'] = product_fil\n",
    "                                elif i == 2:\n",
    "                                    ban_week_fp2['Product'] = product_fil\n",
    "\n",
    "                        ##########################################################################################################   \n",
    "                                                        #  FRONT PAGE DECREASE SIMULATION \n",
    "                        ##########################################################################################################    \n",
    "\n",
    "                        if i == 1:\n",
    "                            change_fp = front_change.children[1].value\n",
    "                        elif i == 2:\n",
    "                            change_fp = front_change2.children[1].value\n",
    "\n",
    "                        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                            if product_fil == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                                change_fp = front_change.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                                change_fp = front_change2.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                                change_fp = front_change3.children[1].value\n",
    "\n",
    "                        if (change_fp < 0) & ((qtr_df_final['Front Page']).sum() != 0):\n",
    "\n",
    "                            import warnings\n",
    "                            warnings.filterwarnings('ignore')\n",
    "\n",
    "                            # Defining empty dataframes for model training\n",
    "                            All_coefs = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Train_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Test_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            train_data_brand = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            Test_Data = pd.DataFrame()    \n",
    "\n",
    "                            #Model training and prediction\n",
    "                            for Region_key in Region_List:\n",
    "                                # In case the product is not present at that region\n",
    "                                if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                                    continue\n",
    "\n",
    "                                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                                combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                                ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                                banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                                # Uploaded data banner dummies\n",
    "                                banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                                # If all banners are not present for the training data\n",
    "                                c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    banner_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                combined_dataset.sort_values('Week',inplace = True)\n",
    "                                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                                #Adding dummies for category\n",
    "                                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                                # Adding dummies for product attribute - Category\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data category dummies\n",
    "                                category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    category_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding dummies for product attribute - pack subtype\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data pack subtype dummies\n",
    "                                pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding pack content dummies\n",
    "                                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                                # Uploaded data pack content dummies\n",
    "                                pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                                # If all pack contents are not present for the training data\n",
    "                                c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_content_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Changing product stage as per start date provided by the user\n",
    "                                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                                # Getting updated product stages\n",
    "                                combined_dataset_test_check = combined_dataset_test[combined_dataset_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies_test = pd.get_dummies(combined_dataset_test['Intial_weeks'])\n",
    "\n",
    "                                # If all product stages are not present for the uploaded data\n",
    "                                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                                if len(c) != 0 :\n",
    "                                    stage_dummies_test[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                                combined_dataset_test = pd.concat([combined_dataset_test, stage_dummies_test], axis=1)\n",
    "                                combined_dataset_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # CHANGES : For adcal discounts\n",
    "                                if 'All' in list(test_period.value):\n",
    "                                    # Discount change\n",
    "                                    if i == 1:                \n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']          \n",
    "                                else:                \n",
    "                                    if i == 1:                \n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil)]['Adcal_DD'] \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "\n",
    "                                #Adding discount depth columns\n",
    "                                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                                combined_dataset_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset_test[\"Adcal_DD\"]]\n",
    "                                combined_dataset_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                # In case the discounts go over 100%, limit to 100%\n",
    "                                combined_dataset_test.loc[combined_dataset_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "\n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                                combined_dataset_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Dropping null volumes\n",
    "                                complete_dataset = combined_dataset.copy()\n",
    "                                complete_dataset_test = combined_dataset_test.copy()\n",
    "\n",
    "                                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "                                complete_dataset_test = complete_dataset_test.loc[complete_dataset_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                                complete_dataset_test['Adcal_Price'] = np.log(complete_dataset_test['Adcal_Price'])\n",
    "                                complete_dataset_test['Eq.Unit.Sales'] = np.log(complete_dataset_test['Eq.Unit.Sales'])\n",
    "\n",
    "                                # TEST PERIOD LOOP\n",
    "                                Test_periods = ['2019Q1','2019Q2','2019Q3','2019Q4']\n",
    "\n",
    "                                for Test_period in Test_periods:\n",
    "                                    if (Test_period>end_week.children[8].value) | (Test_period > Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].max()):\n",
    "                                        continue\n",
    "                                    if (Test_period<start_week.children[8].value) | (Test_period < Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].min()):\n",
    "                                        continue\n",
    "\n",
    "                                    #Filtering test weeks\n",
    "                                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                                    # Dividing train & test data\n",
    "                                    complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                                    complete_test_data_set = complete_dataset_test.loc[complete_dataset_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                                    # Dropping unneccessary columns from train dataset\n",
    "                                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                                    train_data_brand = train_data_set.copy()\n",
    "                                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    # Filtering for existing product\n",
    "                                    test_data_set = complete_test_data_set[complete_test_data_set.Product == product_fil].copy()\n",
    "\n",
    "                                    #################### DECREASING PROMOTIONS FOR BANNER WEEKS COMBINATIONS ####################\n",
    "                                    # Allotting all front page promos as 0\n",
    "                                    front_page_promo_weeks = pd.DataFrame(test_data_set[test_data_set['Front.Page'] != 0][['Banner','Week']])\n",
    "                                    front_page_promo_weeks_all = front_page_promo_weeks_all.append(front_page_promo_weeks, ignore_index = True)\n",
    "                                    test_data_set['Front.Page'] = 0\n",
    "\n",
    "                                    # New Category\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "\n",
    "                                    # New Pack Subtype\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    # New Pack Content\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    test_data_brand = test_data_set.copy()                        \n",
    "\n",
    "                                    if i == 1 :\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count.value\n",
    "                                    elif i == 2:\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size2.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count2.value\n",
    "\n",
    "                                    # Dropping unneccessary columns from test dataset\n",
    "                                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    #Creating test and train data - independent & dependent columns\n",
    "                                    X_train = train_data_brand\n",
    "                                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                                    X_test = test_data_brand\n",
    "                                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                                    # Appending test data\n",
    "                                    test_data_set[\"Region\"] = Region_key\n",
    "                                    test_data_set[\"Test_period\"] = Test_period\n",
    "\n",
    "                                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "                                    # In case the product is not present in certain region-quarter\n",
    "                                    if len(test_data_set) == 0:\n",
    "                                        continue\n",
    "\n",
    "                                    if((Region_key == 'EAST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_EAST_2019Q1_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_EAST_2019Q2_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_EAST_2019Q3_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_EAST_2019Q4_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_ONTARIO_2019Q1_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_ONTARIO_2019Q2_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_ONTARIO_2019Q3_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_ONTARIO_2019Q4_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_QUEBEC_2019Q1_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_QUEBEC_2019Q2_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_QUEBEC_2019Q3_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_QUEBEC_2019Q4_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_WEST_2019Q1_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_WEST_2019Q2_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_WEST_2019Q3_v2\n",
    "                                    else:\n",
    "                                            rf = model_object_WEST_2019Q4_v2\n",
    "\n",
    "                                    #Model predictions\n",
    "                                    predictions_rf = rf.predict(X_test)                \n",
    "\n",
    "                                    #Storing test results\n",
    "                                    result = X_test.copy()\n",
    "                                    result[\"Predictions_rf\"] = predictions_rf\n",
    "                                    result[\"Actuals\"] = y_test\n",
    "\n",
    "                                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                                    result['Actuals'] = np.exp(result['Actuals'])                                                    \n",
    "\n",
    "                                    result[\"Banner\"] = test_data_set['Banner']\n",
    "                                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                                    result[\"Region\"] = Region_key\n",
    "                                    result[\"Test_period\"] = Test_period\n",
    "                                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                                    result[\"Front.Page\"] = test_data_set[\"Front.Page\"]\n",
    "                                    result[\"Middle.Page\"] = test_data_set[\"Middle.Page\"]\n",
    "                                    result[\"Back.Page\"] = test_data_set[\"Back.Page\"]\n",
    "                                    result['iter'] = i\n",
    "\n",
    "                                    result = result[[\"Region\",\"Banner\",\"Product\",'Channel',\"Test_period\",\"Week\",'Front.Page','Middle.Page','Back.Page',\"Actuals\",\"Predictions_rf\",'iter']]\n",
    "\n",
    "                                    #Appending results to the final results dataframe\n",
    "                                    Test_results = Test_results.append(result, ignore_index=True)    \n",
    "\n",
    "                            Test_results = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]        \n",
    "                            # Merge & get prediction difference column\n",
    "                            Test_results = Test_results[Test_results.iter == i].merge(Test_results_sim[Test_results_sim.iter == i][['Banner','Week','Baseline Prediction']], on = ['Banner','Week'], how = 'left')        \n",
    "                            Test_results['Diff'] = Test_results[Test_results.iter == i]['Predictions_rf'] - Test_results[Test_results.iter == i]['Baseline Prediction']\n",
    "                            # Consider positive difference only\n",
    "                            Test_results = Test_results[Test_results.iter == i][Test_results[Test_results.iter == i].Diff > 0]\n",
    "                            Test_results = Test_results[Test_results.iter == i].sort_values('Diff', ascending = False)\n",
    "\n",
    "                            # Filtering out banner weeks where back & middle page promotion were given \n",
    "                            Test_results = Test_results[(Test_results['Back.Page'] == 0) & (Test_results['Middle.Page'] == 0)]    \n",
    "\n",
    "                            # Filtering banner weeks where front page promotion was given previously\n",
    "                            Test_results['Ban_week_concat'] = Test_results['Banner'] + Test_results['Week']\n",
    "                            front_page_promo_weeks_all['Ban_week_concat'] = front_page_promo_weeks_all['Banner'] + front_page_promo_weeks_all['Week']    \n",
    "                            Test_results = Test_results[Test_results['Ban_week_concat'].isin(front_page_promo_weeks_all['Ban_week_concat'].unique())]\n",
    "\n",
    "                            # Select required number of banner-weeks where promotion decrease would occur\n",
    "                            if (promo_change_toggle.value == 'Yes (Specific Banners)'):         \n",
    "                                Test_results = Test_results[Test_results.Banner.isin(list(banner_select.value))]\n",
    "                                for banner_loop in banner_select.value:    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_fp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_fp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_fp = ban_week_fp.append(ban_week_fp0, ignore_index = True)\n",
    "                                    elif i == 2:\n",
    "                                        ban_week_fp2 = ban_week_fp2.append(ban_week_fp0, ignore_index = True)\n",
    "\n",
    "                            elif (promo_change_toggle.value == 'Yes (All Banners)'):\n",
    "                                for banner_loop in qtr_df_final.Banner.unique():    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_fp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_fp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_fp = ban_week_fp.append(ban_week_fp0, ignore_index = True)\n",
    "                                    elif i == 2:\n",
    "                                        ban_week_fp2 = ban_week_fp2.append(ban_week_fp0, ignore_index = True)\n",
    "                            \n",
    "                            if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach                        \n",
    "                                ban_week_fp['Product'] = product_fil\n",
    "                            else:\n",
    "                                if i == 1:\n",
    "                                    ban_week_fp['Product'] = product_fil\n",
    "                                elif i == 2:\n",
    "                                    ban_week_fp2['Product'] = product_fil\n",
    "\n",
    "                        ##########################################################################################################   \n",
    "                                                        #  MIDDLE PAGE INCREASE SIMULATION \n",
    "                        ##########################################################################################################    \n",
    "\n",
    "\n",
    "                        if i == 1:\n",
    "                            change_mp = middle_change.children[1].value\n",
    "                        elif i == 2:\n",
    "                            change_mp = middle_change2.children[1].value\n",
    "\n",
    "                        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                            if product_fil == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                                change_mp = middle_change.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                                change_mp = middle_change2.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                                change_mp = middle_change3.children[1].value\n",
    "                           \n",
    "                        \n",
    "                        if change_mp > 0:    \n",
    "\n",
    "                            import warnings\n",
    "                            warnings.filterwarnings('ignore')\n",
    "\n",
    "                            # Defining empty dataframes for model training\n",
    "                            All_coefs = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Train_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Test_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            train_data_brand = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            Test_Data = pd.DataFrame()    \n",
    "\n",
    "                            #Model training and prediction\n",
    "                            for Region_key in Region_List:\n",
    "                                # In case the product is not present at that region\n",
    "                                if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                                    continue\n",
    "\n",
    "                                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                                combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                                ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                                banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                                # Uploaded data banner dummies\n",
    "                                banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                                # If all banners are not present for the training data\n",
    "                                c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    banner_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                combined_dataset.sort_values('Week',inplace = True)\n",
    "                                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                                #Adding dummies for category\n",
    "                                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                                # Adding dummies for product attribute - Category\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data category dummies\n",
    "                                category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    category_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding dummies for product attribute - pack subtype\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data pack subtype dummies\n",
    "                                pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding pack content dummies\n",
    "                                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                                # Uploaded data pack content dummies\n",
    "                                pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                                # If all pack contents are not present for the training data\n",
    "                                c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_content_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Changing product stage as per start date provided by the user\n",
    "                                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                                # Getting updated product stages\n",
    "                                combined_dataset_test_check = combined_dataset_test[combined_dataset_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies_test = pd.get_dummies(combined_dataset_test['Intial_weeks'])\n",
    "\n",
    "                                # If all product stages are not present for the uploaded data\n",
    "                                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                                if len(c) != 0 :\n",
    "                                    stage_dummies_test[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                                combined_dataset_test = pd.concat([combined_dataset_test, stage_dummies_test], axis=1)\n",
    "                                combined_dataset_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # CHANGES : For adcal discounts\n",
    "                                if 'All' in list(test_period.value):\n",
    "                                    # Discount change\n",
    "                                    if i == 1:                \n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']          \n",
    "                                else:                \n",
    "                                    if i == 1:                \n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil)]['Adcal_DD'] \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "\n",
    "                                #Adding discount depth columns\n",
    "                                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                                combined_dataset_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset_test[\"Adcal_DD\"]]\n",
    "                                combined_dataset_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                # In case the discounts go over 100%, limit to 100%\n",
    "                                combined_dataset_test.loc[combined_dataset_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "\n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                                combined_dataset_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Dropping null volumes\n",
    "                                complete_dataset = combined_dataset.copy()\n",
    "                                complete_dataset_test = combined_dataset_test.copy()\n",
    "\n",
    "                                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "                                complete_dataset_test = complete_dataset_test.loc[complete_dataset_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                                complete_dataset_test['Adcal_Price'] = np.log(complete_dataset_test['Adcal_Price'])\n",
    "                                complete_dataset_test['Eq.Unit.Sales'] = np.log(complete_dataset_test['Eq.Unit.Sales'])\n",
    "\n",
    "                                # TEST PERIOD LOOP\n",
    "                                Test_periods = ['2019Q1','2019Q2','2019Q3','2019Q4']\n",
    "\n",
    "                                for Test_period in Test_periods:\n",
    "                                    if (Test_period>end_week.children[8].value) | (Test_period > Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].max()):\n",
    "                                        continue\n",
    "                                    if (Test_period<start_week.children[8].value) | (Test_period < Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].min()):\n",
    "                                        continue\n",
    "\n",
    "                                    #Filtering test weeks\n",
    "                                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                                    # Dividing train & test data\n",
    "                                    complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                                    complete_test_data_set = complete_dataset_test.loc[complete_dataset_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                                    # Dropping unneccessary columns from train dataset\n",
    "                                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                                    train_data_brand = train_data_set.copy()\n",
    "                                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    # Filtering for existing product\n",
    "                                    test_data_set = complete_test_data_set[complete_test_data_set.Product == product_fil].copy()\n",
    "\n",
    "                                    #################### INCREASING PROMOTIONS FOR BANNER WEEKS COMBINATIONS ####################\n",
    "\n",
    "                                    # Get banner week promos\n",
    "                                    banner_week_promos_combo = test_data_set[['Banner','Week','Front.Page','Middle.Page','Back.Page']].drop_duplicates()            \n",
    "                                    banner_week_upc = banner_week_promos_combo.copy()\n",
    "                                    # Get rows where there is availability of increase in prmotions\n",
    "                                    avail_promo_weeks = banner_week_upc[(banner_week_upc['Front.Page'] == 0) & (banner_week_upc['Middle.Page'] == 0) & (banner_week_upc['Back.Page'] == 0)]\n",
    "                                    # Increasing 1 Middle page promotions for every week\n",
    "                                    avail_promo_weeks['Middle.Page'] = 1\n",
    "                                    # Merge to get required increase in banner-week combintation\n",
    "                                    test_data_set_promo_check = test_data_set.merge(avail_promo_weeks[['Banner','Week','Middle.Page']], how = 'left', on = ['Banner','Week'])\n",
    "\n",
    "                                    # Filling na with existing values in Middle.Page_y\n",
    "                                    test_data_set_promo_check['Middle.Page_y'].fillna(test_data_set_promo_check['Middle.Page_x'], inplace = True)\n",
    "                                    # Replacing values of 'Middle.Page_x' with 'Middle.Page_y' wherever it was zero previously\n",
    "                                    test_data_set_promo_check['Middle.Page_x'] = np.where(test_data_set_promo_check['Middle.Page_x'] == 0, test_data_set_promo_check['Middle.Page_y'], test_data_set_promo_check['Middle.Page_x'])\n",
    "                                    # Drop 'Middle.Page_y' \n",
    "                                    test_data_set_promo_check.drop('Middle.Page_y', axis = 1, inplace =True)\n",
    "                                    # Renaming 'Middle.Page_x'\n",
    "                                    test_data_set_promo_check.rename(columns = {'Middle.Page_x':'Middle.Page'}, inplace = True)\n",
    "\n",
    "                                    test_data_set = test_data_set_promo_check.copy()\n",
    "\n",
    "                                    # New Category\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    # New Pack Subtype\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    # New Pack Content\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    test_data_brand = test_data_set.copy()\n",
    "\n",
    "                                    if i == 1 :\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count.value\n",
    "                                    elif i == 2:\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size2.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count2.value\n",
    "\n",
    "                                    # Dropping unneccessary columns from test dataset\n",
    "                                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    #Creating test and train data - independent & dependent columns\n",
    "                                    X_train = train_data_brand\n",
    "                                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                                    X_test = test_data_brand\n",
    "                                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                                    # Appending test data\n",
    "                                    test_data_set[\"Region\"] = Region_key\n",
    "                                    test_data_set[\"Test_period\"] = Test_period\n",
    "                                    # In case the product is not present in certain region-quarter\n",
    "                                    if len(test_data_set) == 0:\n",
    "                                        continue\n",
    "\n",
    "                                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "\n",
    "                                    if((Region_key == 'EAST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_EAST_2019Q1_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_EAST_2019Q2_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_EAST_2019Q3_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_EAST_2019Q4_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_ONTARIO_2019Q1_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_ONTARIO_2019Q2_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_ONTARIO_2019Q3_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_ONTARIO_2019Q4_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_QUEBEC_2019Q1_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_QUEBEC_2019Q2_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_QUEBEC_2019Q3_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_QUEBEC_2019Q4_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_WEST_2019Q1_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_WEST_2019Q2_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_WEST_2019Q3_v2\n",
    "                                    else:\n",
    "                                            rf = model_object_WEST_2019Q4_v2\n",
    "\n",
    "                                    #Model predictions\n",
    "                                    predictions_rf = rf.predict(X_test)                \n",
    "\n",
    "                                    #Storing test results\n",
    "                                    result = X_test.copy()\n",
    "                                    result[\"Predictions_rf\"] = predictions_rf\n",
    "                                    result[\"Actuals\"] = y_test\n",
    "\n",
    "                                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                                    result['Actuals'] = np.exp(result['Actuals'])                                                    \n",
    "\n",
    "                                    #Calculating APE for the models\n",
    "                                    result[\"ape_rf\"] = (result[\"Predictions_rf\"]-result[\"Actuals\"]).abs()\n",
    "\n",
    "                                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                                    result[\"Region\"] = Region_key\n",
    "                                    result[\"Test_period\"] = Test_period\n",
    "                                    result[\"Banner\"] = test_data_set[\"Banner\"]\n",
    "                                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                                    result[\"DD_1\"] = test_data_set['DD_1']\n",
    "                                    result[\"DD_2\"] = test_data_set['DD_2']\n",
    "                                    ##Columns for Revenue                                \n",
    "                                    result[\"Adcal_Price\"] = test_data_set[\"Adcal_Price\"]\n",
    "                                    result['iter'] = i\n",
    "\n",
    "                                    result = result[[\"Region\",\"Banner\",\"Product\",'Channel',\"Test_period\",\"Week\",\"Actuals\",\"ape_rf\",\"Predictions_rf\",\"DD_1\",\"DD_2\",\"Adcal_Price\",'iter']]\n",
    "\n",
    "                                    #Appending results to the final results dataframe\n",
    "                                    Test_results = Test_results.append(result, ignore_index=True)                \n",
    "\n",
    "                            Test_results = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]\n",
    "                            # Removing 0 promotion banners\n",
    "                            if len(qtr_df_final[(qtr_df_final.Banner.isin(fp_remove_banner)) | (qtr_df_final.Banner.isin(mp_remove_banner)) | (qtr_df_final.Banner.isin(bp_remove_banner))]) != 0:\n",
    "                                if zero_banner_promo_inc.value == 'No':\n",
    "                                    Test_results = Test_results[~Test_results.Banner.isin(mp_remove_banner)]\n",
    "\n",
    "                            # Merge & get prediction difference column\n",
    "                            Test_results = Test_results[Test_results.iter == i].merge(Test_results_sim[Test_results_sim.iter == i][['Banner','Week','Baseline Prediction']], on = ['Banner','Week'], how = 'left')        \n",
    "                            Test_results['Diff'] = Test_results[Test_results.iter == i]['Predictions_rf'] - Test_results[Test_results.iter == i]['Baseline Prediction']\n",
    "                            # Consider positive difference only\n",
    "                            Test_results = Test_results[Test_results.iter == i][Test_results[Test_results.iter == i].Diff > 0]\n",
    "                            Test_results = Test_results[Test_results.iter == i].sort_values('Diff', ascending = False)\n",
    "\n",
    "                            # Filtering out banner weeks where front page promotions were given for priority\n",
    "                            if change_fp != 0:\n",
    "                                Test_results['ban_week_mp_check'] = Test_results['Banner'] + Test_results['Week']\n",
    "                                # if-else loop for iter == 1, iter == 2\n",
    "                                if i == 1:\n",
    "                                    Test_results = Test_results[~Test_results['ban_week_mp_check'].isin(ban_week_fp['ban_week_fp_check'].unique())]\n",
    "                                elif i == 2:\n",
    "                                    Test_results = Test_results[~Test_results['ban_week_mp_check'].isin(ban_week_fp2['ban_week_fp_check'].unique())]\n",
    "\n",
    "                            # Select required number of banner-weeks where promotion increase would occur\n",
    "                            if (promo_change_toggle.value == 'Yes (Specific Banners)'):         \n",
    "                                Test_results = Test_results[Test_results.Banner.isin(list(banner_select.value))]\n",
    "                                for banner_loop in banner_select.value:    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_mp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_mp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_mp = ban_week_mp.append(ban_week_mp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_mp['ban_week_mp_check'] = ban_week_mp['Banner'] + ban_week_mp['Week']                            \n",
    "\n",
    "                                    elif i == 2:\n",
    "                                        ban_week_mp2 = ban_week_mp2.append(ban_week_mp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_mp2['ban_week_mp_check'] = ban_week_mp2['Banner'] + ban_week_mp2['Week']                             \n",
    "\n",
    "                            elif (promo_change_toggle.value == 'Yes (All Banners)'):\n",
    "                                for banner_loop in qtr_df_final.Banner.unique():    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_mp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_mp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_mp = ban_week_mp.append(ban_week_mp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_mp['ban_week_mp_check'] = ban_week_mp['Banner'] + ban_week_mp['Week']                            \n",
    "                                    elif i == 2:\n",
    "                                        ban_week_mp2 = ban_week_mp2.append(ban_week_mp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_mp2['ban_week_mp_check'] = ban_week_mp2['Banner'] + ban_week_mp2['Week'] \n",
    "\n",
    "                            if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach                        \n",
    "                                ban_week_mp['Product'] = product_fil\n",
    "                            else:\n",
    "                                if i == 1:\n",
    "                                    ban_week_mp['Product'] = product_fil\n",
    "                                elif i == 2:\n",
    "                                    ban_week_mp2['Product'] = product_fil\n",
    "\n",
    "                            \n",
    "                        ##########################################################################################################   \n",
    "                                                        #  MIDDLE PAGE DECREASE SIMULATION \n",
    "                        ##########################################################################################################    \n",
    "\n",
    "                        if i == 1:\n",
    "                            change_mp = middle_change.children[1].value\n",
    "                        elif i == 2:\n",
    "                            change_mp = middle_change2.children[1].value\n",
    "\n",
    "                        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                            if product_fil == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                                change_mp = middle_change.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                                change_mp = middle_change2.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                                change_mp = middle_change3.children[1].value\n",
    "                            \n",
    "                        if (change_mp < 0) & ((qtr_df_final['Middle Page']).sum() != 0):\n",
    "\n",
    "                            ################## MIDDLE PAGE SIMULATION ##################\n",
    "                            import warnings\n",
    "                            warnings.filterwarnings('ignore')\n",
    "\n",
    "                            # Defining empty dataframes for model training\n",
    "                            All_coefs = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Train_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Test_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            train_data_brand = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            Test_Data = pd.DataFrame()    \n",
    "\n",
    "                            #Model training and prediction\n",
    "                            for Region_key in Region_List:\n",
    "                                # In case the product is not present at that region\n",
    "                                if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                                    continue\n",
    "\n",
    "                                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                                combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                                ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                                banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                                # Uploaded data banner dummies\n",
    "                                banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                                # If all banners are not present for the training data\n",
    "                                c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    banner_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                combined_dataset.sort_values('Week',inplace = True)\n",
    "                                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                                #Adding dummies for category\n",
    "                                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                                # Adding dummies for product attribute - Category\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data category dummies\n",
    "                                category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    category_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding dummies for product attribute - pack subtype\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data pack subtype dummies\n",
    "                                pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding pack content dummies\n",
    "                                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                                # Uploaded data pack content dummies\n",
    "                                pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                                # If all pack contents are not present for the training data\n",
    "                                c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_content_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Changing product stage as per start date provided by the user\n",
    "                                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                                # Getting updated product stages\n",
    "                                combined_dataset_test_check = combined_dataset_test[combined_dataset_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies_test = pd.get_dummies(combined_dataset_test['Intial_weeks'])\n",
    "\n",
    "                                # If all product stages are not present for the uploaded data\n",
    "                                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                                if len(c) != 0 :\n",
    "                                    stage_dummies_test[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                                combined_dataset_test = pd.concat([combined_dataset_test, stage_dummies_test], axis=1)\n",
    "                                combined_dataset_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # CHANGES : For adcal discounts\n",
    "                                if 'All' in list(test_period.value):\n",
    "                                    # Discount change\n",
    "                                    if i == 1:                \n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']          \n",
    "                                else:                \n",
    "                                    if i == 1:                \n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil)]['Adcal_DD'] \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "                                #Adding discount depth columns\n",
    "                                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                                combined_dataset_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset_test[\"Adcal_DD\"]]\n",
    "                                combined_dataset_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                # In case the discounts go over 100%, limit to 100%\n",
    "                                combined_dataset_test.loc[combined_dataset_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "\n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                                combined_dataset_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Dropping null volumes\n",
    "                                complete_dataset = combined_dataset.copy()\n",
    "                                complete_dataset_test = combined_dataset_test.copy()\n",
    "\n",
    "                                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "                                complete_dataset_test = complete_dataset_test.loc[complete_dataset_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                                complete_dataset_test['Adcal_Price'] = np.log(complete_dataset_test['Adcal_Price'])\n",
    "                                complete_dataset_test['Eq.Unit.Sales'] = np.log(complete_dataset_test['Eq.Unit.Sales'])\n",
    "\n",
    "                                # TEST PERIOD LOOP\n",
    "                                Test_periods = ['2019Q1','2019Q2','2019Q3','2019Q4']\n",
    "\n",
    "                                for Test_period in Test_periods:\n",
    "                                    if (Test_period>end_week.children[8].value) | (Test_period > Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].max()):\n",
    "                                        continue\n",
    "                                    if (Test_period<start_week.children[8].value) | (Test_period < Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].min()):\n",
    "                                        continue\n",
    "\n",
    "                                    #Filtering test weeks\n",
    "                                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                                    # Dividing train & test data\n",
    "                                    complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                                    complete_test_data_set = complete_dataset_test.loc[complete_dataset_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                                    # Dropping unneccessary columns from train dataset\n",
    "                                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                                    train_data_brand = train_data_set.copy()\n",
    "                                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    # Filtering for existing product\n",
    "                                    test_data_set = complete_test_data_set[complete_test_data_set.Product == product_fil].copy()\n",
    "\n",
    "                                    #################### DECREASING PROMOTIONS FOR BANNER WEEKS COMBINATIONS ####################\n",
    "                                    # Allotting all middle page promos as 0\n",
    "                                    middle_page_promo_weeks = pd.DataFrame(test_data_set[test_data_set['Middle.Page'] != 0][['Banner','Week']])\n",
    "                                    middle_page_promo_weeks_all = middle_page_promo_weeks_all.append(middle_page_promo_weeks, ignore_index = True)\n",
    "                                    test_data_set['Middle.Page'] = 0\n",
    "\n",
    "                                    # New Category\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    # New Pack Subtype\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    # New Pack Content\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    test_data_brand = test_data_set.copy()\n",
    "\n",
    "                                    if i == 1 :\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count.value\n",
    "                                    elif i == 2:\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size2.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count2.value\n",
    "\n",
    "                                    # Dropping unneccessary columns from test dataset\n",
    "                                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    #Creating test and train data - independent & dependent columns\n",
    "                                    X_train = train_data_brand\n",
    "                                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                                    X_test = test_data_brand\n",
    "                                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                                    # Appending test data\n",
    "                                    test_data_set[\"Region\"] = Region_key\n",
    "                                    test_data_set[\"Test_period\"] = Test_period\n",
    "                                    # In case the product is not present in certain region-quarter\n",
    "                                    if len(test_data_set) == 0:\n",
    "                                        continue\n",
    "\n",
    "                                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "\n",
    "                                    if((Region_key == 'EAST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_EAST_2019Q1_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_EAST_2019Q2_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_EAST_2019Q3_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_EAST_2019Q4_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_ONTARIO_2019Q1_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_ONTARIO_2019Q2_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_ONTARIO_2019Q3_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_ONTARIO_2019Q4_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_QUEBEC_2019Q1_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_QUEBEC_2019Q2_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_QUEBEC_2019Q3_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_QUEBEC_2019Q4_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_WEST_2019Q1_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_WEST_2019Q2_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_WEST_2019Q3_v2\n",
    "                                    else:\n",
    "                                            rf = model_object_WEST_2019Q4_v2\n",
    "\n",
    "                                    #Model predictions\n",
    "                                    predictions_rf = rf.predict(X_test)                \n",
    "\n",
    "                                    #Storing test results\n",
    "                                    result = X_test.copy()\n",
    "                                    result[\"Predictions_rf\"] = predictions_rf\n",
    "                                    result[\"Actuals\"] = y_test\n",
    "\n",
    "                                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                                    result['Actuals'] = np.exp(result['Actuals'])                                                    \n",
    "\n",
    "                                    result[\"Banner\"] = test_data_set['Banner']\n",
    "                                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                                    result[\"Region\"] = Region_key\n",
    "                                    result[\"Test_period\"] = Test_period\n",
    "                                    result[\"Week\"] = test_data_set[\"Week\"]  \n",
    "                                    result[\"Front.Page\"] = test_data_set[\"Front.Page\"]\n",
    "                                    result[\"Middle.Page\"] = test_data_set[\"Middle.Page\"]\n",
    "                                    result[\"Back.Page\"] = test_data_set[\"Back.Page\"]\n",
    "                                    result['iter'] = i\n",
    "\n",
    "                                    result = result[[\"Region\",\"Banner\",\"Product\",'Channel',\"Test_period\",\"Week\",'Front.Page','Middle.Page','Back.Page',\"Actuals\",\"Predictions_rf\",'iter']]\n",
    "\n",
    "                                    #Appending results to the final results dataframe\n",
    "                                    Test_results = Test_results.append(result, ignore_index=True)                \n",
    "\n",
    "                            Test_results = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]        \n",
    "                            # Merge & get prediction difference column\n",
    "                            Test_results = Test_results[Test_results.iter == i].merge(Test_results_sim[Test_results_sim.iter == i][['Banner','Week','Baseline Prediction']], on = ['Banner','Week'], how = 'left')        \n",
    "                            Test_results['Diff'] = Test_results[Test_results.iter == i]['Predictions_rf'] - Test_results[Test_results.iter == i]['Baseline Prediction']\n",
    "                            # Consider positive difference only\n",
    "                            Test_results = Test_results[Test_results.iter == i][Test_results[Test_results.iter == i].Diff > 0]\n",
    "                            Test_results = Test_results[Test_results.iter == i].sort_values('Diff', ascending = False)\n",
    "\n",
    "                            # Filtering out banner weeks where back & middle page promotion were given \n",
    "                            Test_results = Test_results[(Test_results['Back.Page'] == 0) & (Test_results['Front.Page'] == 0)]    \n",
    "\n",
    "                            # Filtering banner weeks where middle page promotion was given previously\n",
    "                            Test_results['Ban_week_concat'] = Test_results['Banner'] + Test_results['Week']\n",
    "                            middle_page_promo_weeks_all['Ban_week_concat'] = middle_page_promo_weeks_all['Banner'] + middle_page_promo_weeks_all['Week']    \n",
    "                            Test_results = Test_results[Test_results['Ban_week_concat'].isin(middle_page_promo_weeks_all['Ban_week_concat'].unique())]\n",
    "\n",
    "                            # Select required number of banner-weeks where promotion decrease would occur\n",
    "                            if (promo_change_toggle.value == 'Yes (Specific Banners)'):         \n",
    "                                Test_results = Test_results[Test_results.Banner.isin(list(banner_select.value))]\n",
    "                                for banner_loop in banner_select.value:    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_mp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_mp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_mp = ban_week_mp.append(ban_week_mp0, ignore_index = True)\n",
    "                                    elif i == 2:\n",
    "                                        ban_week_mp2 = ban_week_mp2.append(ban_week_mp0, ignore_index = True)\n",
    "\n",
    "                            elif (promo_change_toggle.value == 'Yes (All Banners)'):\n",
    "                                for banner_loop in qtr_df_final.Banner.unique():    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_mp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_mp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_mp = ban_week_mp.append(ban_week_mp0, ignore_index = True)\n",
    "                                    elif i == 2:\n",
    "                                        ban_week_mp2 = ban_week_mp2.append(ban_week_mp0, ignore_index = True)\n",
    "\n",
    "                            if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach                        \n",
    "                                ban_week_mp['Product'] = product_fil\n",
    "                            else:\n",
    "                                if i == 1:\n",
    "                                    ban_week_mp['Product'] = product_fil\n",
    "                                elif i == 2:\n",
    "                                    ban_week_mp2['Product'] = product_fil\n",
    "\n",
    "                        ##########################################################################################################   \n",
    "                                                        #  BACK PAGE INCREASE SIMULATION \n",
    "                        ##########################################################################################################    \n",
    "\n",
    "                        if i == 1:\n",
    "                            change_bp = back_change.children[1].value\n",
    "                        elif i == 2:\n",
    "                            change_bp = back_change2.children[1].value\n",
    "\n",
    "                        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                            if product_fil == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                                change_bp = back_change.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                                change_bp = back_change2.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                                change_bp = back_change3.children[1].value\n",
    "                            \n",
    "                        if change_bp > 0:    \n",
    "\n",
    "                            import warnings\n",
    "                            warnings.filterwarnings('ignore')\n",
    "\n",
    "                            # Defining empty dataframes for model training\n",
    "                            All_coefs = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Train_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Test_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            train_data_brand = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            Test_Data = pd.DataFrame()    \n",
    "\n",
    "                            #Model training and prediction\n",
    "                            for Region_key in Region_List:\n",
    "                                # In case the product is not present at that region\n",
    "                                if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                                    continue\n",
    "\n",
    "                                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                                combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                                ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                                banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                                # Uploaded data banner dummies\n",
    "                                banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                                # If all banners are not present for the training data\n",
    "                                c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    banner_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                combined_dataset.sort_values('Week',inplace = True)\n",
    "                                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                                #Adding dummies for category\n",
    "                                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                                # Adding dummies for product attribute - Category\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data category dummies\n",
    "                                category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    category_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding dummies for product attribute - pack subtype\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data pack subtype dummies\n",
    "                                pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding pack content dummies\n",
    "                                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                                # Uploaded data pack content dummies\n",
    "                                pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                                # If all pack contents are not present for the training data\n",
    "                                c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_content_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Changing product stage as per start date provided by the user\n",
    "                                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                                # Getting updated product stages\n",
    "                                combined_dataset_test_check = combined_dataset_test[combined_dataset_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies_test = pd.get_dummies(combined_dataset_test['Intial_weeks'])\n",
    "\n",
    "                                # If all product stages are not present for the uploaded data\n",
    "                                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                                if len(c) != 0 :\n",
    "                                    stage_dummies_test[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                                combined_dataset_test = pd.concat([combined_dataset_test, stage_dummies_test], axis=1)\n",
    "                                combined_dataset_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # CHANGES : For adcal discounts\n",
    "                                if 'All' in list(test_period.value):\n",
    "                                    # Discount change\n",
    "                                    if i == 1:                \n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']          \n",
    "                                else:                \n",
    "                                    if i == 1:                \n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil)]['Adcal_DD'] \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "                                #Adding discount depth columns\n",
    "                                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                                combined_dataset_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset_test[\"Adcal_DD\"]]\n",
    "                                combined_dataset_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                # In case the discounts go over 100%, limit to 100%\n",
    "                                combined_dataset_test.loc[combined_dataset_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "\n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                                combined_dataset_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Dropping null volumes\n",
    "                                complete_dataset = combined_dataset.copy()\n",
    "                                complete_dataset_test = combined_dataset_test.copy()\n",
    "\n",
    "                                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "                                complete_dataset_test = complete_dataset_test.loc[complete_dataset_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                                complete_dataset_test['Adcal_Price'] = np.log(complete_dataset_test['Adcal_Price'])\n",
    "                                complete_dataset_test['Eq.Unit.Sales'] = np.log(complete_dataset_test['Eq.Unit.Sales'])\n",
    "\n",
    "                                # TEST PERIOD LOOP\n",
    "                                Test_periods = ['2019Q1','2019Q2','2019Q3','2019Q4']\n",
    "\n",
    "                                for Test_period in Test_periods:\n",
    "                                    if (Test_period>end_week.children[8].value) | (Test_period > Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].max()):\n",
    "                                        continue\n",
    "                                    if (Test_period<start_week.children[8].value) | (Test_period < Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].min()):\n",
    "                                        continue\n",
    "\n",
    "                                    #Filtering test weeks\n",
    "                                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                                    # Dividing train & test data\n",
    "                                    complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                                    complete_test_data_set = complete_dataset_test.loc[complete_dataset_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                                    # Dropping unneccessary columns from train dataset\n",
    "                                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                                    train_data_brand = train_data_set.copy()\n",
    "                                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    # Filtering for existing product\n",
    "                                    test_data_set = complete_test_data_set[complete_test_data_set.Product == product_fil].copy()\n",
    "\n",
    "                                    #################### INCREASING PROMOTIONS FOR BANNER WEEKS COMBINATIONS ####################\n",
    "                                    # Get banner week promos\n",
    "                                    banner_week_promos_combo = test_data_set[['Banner','Week','Front.Page','Middle.Page','Back.Page']].drop_duplicates()\n",
    "                                    banner_week_upc = banner_week_promos_combo.copy()\n",
    "                                    # Get rows where there is availability of increase in prmotions\n",
    "                                    avail_promo_weeks = banner_week_upc[(banner_week_upc['Front.Page'] == 0) & (banner_week_upc['Middle.Page'] == 0) & (banner_week_upc['Back.Page'] == 0)]\n",
    "                                    # Increasing 1 Back page promotions for every week\n",
    "                                    avail_promo_weeks['Back.Page'] = 1\n",
    "\n",
    "                                    # Merge to get required increase in banner-week combintation\n",
    "                                    test_data_set_promo_check = test_data_set.merge(avail_promo_weeks[['Banner','Week','Back.Page']], how = 'left', on = ['Banner','Week'])\n",
    "\n",
    "                                    # Filling na with existing values in Back.Page_y\n",
    "                                    test_data_set_promo_check['Back.Page_y'].fillna(test_data_set_promo_check['Back.Page_x'], inplace = True)\n",
    "                                    # Replacing values of 'Back.Page_x' with 'Back.Page_y' wherever it was zero previously\n",
    "                                    test_data_set_promo_check['Back.Page_x'] = np.where(test_data_set_promo_check['Back.Page_x'] == 0, test_data_set_promo_check['Back.Page_y'], test_data_set_promo_check['Back.Page_x'])\n",
    "                                    # Drop 'Back.Page_y' \n",
    "                                    test_data_set_promo_check.drop('Back.Page_y', axis = 1, inplace =True)\n",
    "                                    # Renaming 'Back.Page_x'\n",
    "                                    test_data_set_promo_check.rename(columns = {'Back.Page_x':'Back.Page'}, inplace = True)\n",
    "\n",
    "                                    test_data_set = test_data_set_promo_check.copy()\n",
    "\n",
    "                                    # New Category\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    # New Pack Subtype\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    # New Pack Content\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    test_data_brand = test_data_set.copy()\n",
    "\n",
    "                                    if i == 1 :\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count.value\n",
    "                                    elif i == 2:\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size2.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count2.value\n",
    "\n",
    "                                    # Dropping unneccessary columns from test dataset\n",
    "                                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    #Creating test and train data - independent & dependent columns\n",
    "                                    X_train = train_data_brand\n",
    "                                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                                    X_test = test_data_brand\n",
    "                                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                                    # Appending test data\n",
    "                                    test_data_set[\"Region\"] = Region_key\n",
    "                                    test_data_set[\"Test_period\"] = Test_period\n",
    "                                    # In case the product is not present in certain region-quarter\n",
    "                                    if len(test_data_set) == 0:\n",
    "                                        continue\n",
    "\n",
    "                                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "\n",
    "                                    if((Region_key == 'EAST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_EAST_2019Q1_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_EAST_2019Q2_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_EAST_2019Q3_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_EAST_2019Q4_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_ONTARIO_2019Q1_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_ONTARIO_2019Q2_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_ONTARIO_2019Q3_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_ONTARIO_2019Q4_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_QUEBEC_2019Q1_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_QUEBEC_2019Q2_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_QUEBEC_2019Q3_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_QUEBEC_2019Q4_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_WEST_2019Q1_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_WEST_2019Q2_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_WEST_2019Q3_v2\n",
    "                                    else:\n",
    "                                            rf = model_object_WEST_2019Q4_v2\n",
    "                                            \n",
    "                                    #Model predictions\n",
    "                                    predictions_rf = rf.predict(X_test)                \n",
    "\n",
    "                                    #Storing test results\n",
    "                                    result = X_test.copy()\n",
    "                                    result[\"Predictions_rf\"] = predictions_rf\n",
    "                                    result[\"Actuals\"] = y_test\n",
    "\n",
    "                                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                                    result['Actuals'] = np.exp(result['Actuals'])                                                    \n",
    "\n",
    "                                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                                    result[\"Region\"] = Region_key\n",
    "                                    result[\"Test_period\"] = Test_period\n",
    "                                    result[\"Banner\"] = test_data_set[\"Banner\"]\n",
    "                                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                                    result['iter'] = i\n",
    "\n",
    "                                    result = result[[\"Region\",\"Banner\",\"Product\",'Channel',\"Test_period\",\"Week\",\"Actuals\",\"Predictions_rf\",'iter']]\n",
    "\n",
    "                                    #Appending results to the final results dataframe\n",
    "                                    Test_results = Test_results.append(result, ignore_index=True)                \n",
    "\n",
    "                            Test_results = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]\n",
    "                            # Removing 0 promotion banners\n",
    "                            if len(qtr_df_final[(qtr_df_final.Banner.isin(fp_remove_banner)) | (qtr_df_final.Banner.isin(mp_remove_banner)) | (qtr_df_final.Banner.isin(bp_remove_banner))]) != 0:\n",
    "                                if zero_banner_promo_inc.value == 'No':\n",
    "                                    Test_results = Test_results[~Test_results.Banner.isin(bp_remove_banner)]\n",
    "\n",
    "                            # Merge & get prediction difference column\n",
    "                            Test_results = Test_results[Test_results.iter == i].merge(Test_results_sim[Test_results_sim.iter == i][['Banner','Week','Baseline Prediction']], on = ['Banner','Week'], how = 'left')        \n",
    "                            Test_results['Diff'] = Test_results[Test_results.iter == i]['Predictions_rf'] - Test_results[Test_results.iter == i]['Baseline Prediction']\n",
    "                            # Consider positive difference only\n",
    "                            Test_results = Test_results[Test_results.iter == i][Test_results[Test_results.iter == i].Diff > 0]\n",
    "                            Test_results = Test_results[Test_results.iter == i].sort_values('Diff', ascending = False)\n",
    "\n",
    "                            # Filtering out front & middle page promotion banner weeks selected for priority\n",
    "                            if change_fp != 0:\n",
    "                                if i == 1:\n",
    "                                    Test_results['ban_week_bp_check'] = Test_results['Banner'] + Test_results['Week']       \n",
    "                                    Test_results = Test_results[~Test_results['ban_week_bp_check'].isin(ban_week_fp['ban_week_fp_check'].unique())]\n",
    "                                elif i == 2:\n",
    "                                    Test_results['ban_week_bp_check'] = Test_results['Banner'] + Test_results['Week']       \n",
    "                                    Test_results = Test_results[~Test_results['ban_week_bp_check'].isin(ban_week_fp2['ban_week_fp_check'].unique())]\n",
    "\n",
    "                            if change_mp != 0:\n",
    "                                if i == 1:\n",
    "                                    Test_results['ban_week_bp_check'] = Test_results['Banner'] + Test_results['Week']       \n",
    "                                    Test_results = Test_results[~Test_results['ban_week_bp_check'].isin(ban_week_mp['ban_week_mp_check'].unique())]\n",
    "                                elif i == 2:\n",
    "                                    Test_results['ban_week_bp_check'] = Test_results['Banner'] + Test_results['Week']       \n",
    "                                    Test_results = Test_results[~Test_results['ban_week_bp_check'].isin(ban_week_mp2['ban_week_mp_check'].unique())]                        \n",
    "\n",
    "                            # Select required number of banner-weeks where promotion increase would occur\n",
    "                            if (promo_change_toggle.value == 'Yes (Specific Banners)'):         \n",
    "                                Test_results = Test_results[Test_results.Banner.isin(list(banner_select.value))]\n",
    "                                for banner_loop in banner_select.value:    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_bp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_bp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_bp = ban_week_bp.append(ban_week_bp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_bp['ban_week_bp_check'] = ban_week_bp['Banner'] + ban_week_bp['Week']                            \n",
    "\n",
    "                                    elif i == 2:\n",
    "                                        ban_week_bp2 = ban_week_bp2.append(ban_week_bp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_bp2['ban_week_bp_check'] = ban_week_bp2['Banner'] + ban_week_bp2['Week']                             \n",
    "\n",
    "                            elif (promo_change_toggle.value == 'Yes (All Banners)'):\n",
    "                                for banner_loop in qtr_df_final.Banner.unique():    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_bp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_bp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_bp = ban_week_bp.append(ban_week_bp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_bp['ban_week_bp_check'] = ban_week_bp['Banner'] + ban_week_bp['Week']                            \n",
    "                                    elif i == 2:\n",
    "                                        ban_week_bp2 = ban_week_bp2.append(ban_week_bp0, ignore_index = True)\n",
    "                                        # Banner-week concat column    \n",
    "                                        ban_week_bp2['ban_week_bp_check'] = ban_week_bp2['Banner'] + ban_week_bp2['Week'] \n",
    "\n",
    "                            if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach                        \n",
    "                                ban_week_bp['Product'] = product_fil\n",
    "                            else:\n",
    "                                if i == 1:\n",
    "                                    ban_week_bp['Product'] = product_fil\n",
    "                                elif i == 2:\n",
    "                                    ban_week_bp2['Product'] = product_fil\n",
    "\n",
    "                        ##########################################################################################################\n",
    "                                                        # BACK PAGE DECREASE SIMULATION \n",
    "                        ##########################################################################################################    \n",
    "\n",
    "                        if i == 1:\n",
    "                            change_bp = back_change.children[1].value\n",
    "                        elif i == 2:\n",
    "                            change_bp = back_change2.children[1].value\n",
    "\n",
    "                        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                            if product_fil == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                                change_bp = back_change.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                                change_bp = back_change2.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                                change_bp = back_change3.children[1].value\n",
    "                            \n",
    "                        if (change_bp < 0) & ((qtr_df_final['Back Page']).sum() != 0):\n",
    "\n",
    "                            ################## BACK PAGE SIMULATION ##################\n",
    "                            import warnings\n",
    "                            warnings.filterwarnings('ignore')\n",
    "\n",
    "                            # Defining empty dataframes for model training\n",
    "                            All_coefs = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Train_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            Test_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "                            train_data_brand = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "                            Test_Data = pd.DataFrame()    \n",
    "\n",
    "                            #Model training and prediction\n",
    "                            for Region_key in Region_List:\n",
    "                                # In case the product is not present at that region\n",
    "                                if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                                    continue\n",
    "\n",
    "                                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                                combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                                ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                                banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                                # Uploaded data banner dummies\n",
    "                                banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                                # If all banners are not present for the training data\n",
    "                                c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    banner_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                combined_dataset.sort_values('Week',inplace = True)\n",
    "                                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                                #Adding dummies for category\n",
    "                                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                                # Adding dummies for product attribute - Category\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data category dummies\n",
    "                                category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    category_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding dummies for product attribute - pack subtype\n",
    "                                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                                # Uploaded data pack subtype dummies\n",
    "                                pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                                # If all pack subtypes are not present for the training data\n",
    "                                c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Adding pack content dummies\n",
    "                                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                                # Uploaded data pack content dummies\n",
    "                                pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                                # If all pack contents are not present for the training data\n",
    "                                c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                                if len(c) != 0:\n",
    "                                    pack_content_dummies[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Changing product stage as per start date provided by the user\n",
    "                                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                                # Getting updated product stages\n",
    "                                combined_dataset_test_check = combined_dataset_test[combined_dataset_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                                combined_dataset_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                                # Product stage dummies\n",
    "                                stage_dummies_test = pd.get_dummies(combined_dataset_test['Intial_weeks'])\n",
    "\n",
    "                                # If all product stages are not present for the uploaded data\n",
    "                                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                                if len(c) != 0 :\n",
    "                                    stage_dummies_test[c] = 0\n",
    "\n",
    "                                # Reordering columns                \n",
    "                                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                                combined_dataset_test = pd.concat([combined_dataset_test, stage_dummies_test], axis=1)\n",
    "                                combined_dataset_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                                # CHANGES : For adcal discounts\n",
    "                                if 'All' in list(test_period.value):\n",
    "                                    # Discount change\n",
    "                                    if i == 1:                \n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                                        combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']          \n",
    "                                else:                \n",
    "                                    if i == 1:                \n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil)]['Adcal_DD'] \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                                    elif i == 2:\n",
    "                                        # Discount change\n",
    "                                        combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                                        # Baseline pricing\n",
    "                                        combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "                                #Adding discount depth columns\n",
    "                                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                                combined_dataset_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                                      else 0 for x in combined_dataset_test[\"Adcal_DD\"]]\n",
    "                                combined_dataset_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                                 else 0 for x in combined_dataset_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                # In case the discounts go over 100%, limit to 100%\n",
    "                                combined_dataset_test.loc[combined_dataset_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "\n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Adcal Price calculation\n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                                combined_dataset_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                                # Dropping null volumes\n",
    "                                complete_dataset = combined_dataset.copy()\n",
    "                                complete_dataset_test = combined_dataset_test.copy()\n",
    "\n",
    "                                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "                                complete_dataset_test = complete_dataset_test.loc[complete_dataset_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                                complete_dataset_test['Adcal_Price'] = np.log(complete_dataset_test['Adcal_Price'])\n",
    "                                complete_dataset_test['Eq.Unit.Sales'] = np.log(complete_dataset_test['Eq.Unit.Sales'])\n",
    "\n",
    "                                # TEST PERIOD LOOP\n",
    "                                Test_periods = ['2019Q1','2019Q2','2019Q3','2019Q4']\n",
    "\n",
    "                                for Test_period in Test_periods:\n",
    "                                    if (Test_period>end_week.children[8].value) | (Test_period > Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].max()):\n",
    "                                        continue\n",
    "                                    if (Test_period<start_week.children[8].value) | (Test_period < Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].min()):\n",
    "                                        continue\n",
    "\n",
    "                                    #Filtering test weeks\n",
    "                                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                                    # Dividing train & test data\n",
    "                                    complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                                    complete_test_data_set = complete_dataset_test.loc[complete_dataset_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                                    # Dropping unneccessary columns from train dataset\n",
    "                                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                                    train_data_brand = train_data_set.copy()\n",
    "                                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    # Filtering for existing product\n",
    "                                    test_data_set = complete_test_data_set[complete_test_data_set.Product == product_fil].copy()\n",
    "\n",
    "                                    #################### DECREASING PROMOTIONS FOR BANNER WEEKS COMBINATIONS ####################\n",
    "                                    # Allotting all back page promos as 0\n",
    "                                    back_page_promo_weeks = pd.DataFrame(test_data_set[test_data_set['Back.Page'] != 0][['Banner','Week']])\n",
    "                                    back_page_promo_weeks_all = back_page_promo_weeks_all.append(back_page_promo_weeks, ignore_index = True)            \n",
    "                                    test_data_set['Back.Page'] = 0\n",
    "\n",
    "                                    # New Category\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                            if (cols == category_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    # New Pack Subtype\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                            if (cols == pack_subtype_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    # New Pack Content\n",
    "                                    if i == 1:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "                                    if i == 2:\n",
    "                                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                            if (cols == pack_content_drop2.value):                      \n",
    "                                                test_data_set[cols] = 1\n",
    "                                            else:\n",
    "                                                test_data_set[cols] = 0\n",
    "\n",
    "                                    test_data_brand = test_data_set.copy()\n",
    "\n",
    "                                    if i == 1 :\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count.value\n",
    "                                    elif i == 2:\n",
    "                                        # For New SIZE\n",
    "                                        test_data_brand['SIZE_ML'] = size2.value\n",
    "\n",
    "                                        # For New Count\n",
    "                                        test_data_brand['COUNT'] = count2.value\n",
    "\n",
    "                                    # Dropping unneccessary columns from test dataset\n",
    "                                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",\"Channel\",\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                                    #Creating test and train data - independent & dependent columns\n",
    "                                    X_train = train_data_brand\n",
    "                                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                                    X_test = test_data_brand\n",
    "                                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                                    # Appending test data\n",
    "                                    test_data_set[\"Region\"] = Region_key\n",
    "                                    test_data_set[\"Test_period\"] = Test_period\n",
    "\n",
    "                                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "                                    # In case the product is not present in certain region-quarter\n",
    "                                    if len(test_data_set) == 0:\n",
    "                                        continue\n",
    "\n",
    "                                    if((Region_key == 'EAST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_EAST_2019Q1_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_EAST_2019Q2_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_EAST_2019Q3_v2\n",
    "                                    elif((Region_key == 'EAST') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_EAST_2019Q4_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_ONTARIO_2019Q1_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_ONTARIO_2019Q2_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_ONTARIO_2019Q3_v2\n",
    "                                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_ONTARIO_2019Q4_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_QUEBEC_2019Q1_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_QUEBEC_2019Q2_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_QUEBEC_2019Q3_v2\n",
    "                                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q4')):\n",
    "                                            rf = model_object_QUEBEC_2019Q4_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q1')):\n",
    "                                            rf = model_object_WEST_2019Q1_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q2')):\n",
    "                                            rf = model_object_WEST_2019Q2_v2\n",
    "                                    elif((Region_key == 'WEST') and (Test_period == '2019Q3')):\n",
    "                                            rf = model_object_WEST_2019Q3_v2\n",
    "                                    else:\n",
    "                                            rf = model_object_WEST_2019Q4_v2\n",
    "\n",
    "                                    # Model predictions\n",
    "                                    predictions_rf = rf.predict(X_test)                \n",
    "\n",
    "                                    # Storing test results\n",
    "                                    result = X_test.copy()\n",
    "                                    result[\"Predictions_rf\"] = predictions_rf\n",
    "                                    result[\"Actuals\"] = y_test\n",
    "\n",
    "                                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                                    result['Actuals'] = np.exp(result['Actuals'])                                                    \n",
    "\n",
    "                                    result[\"Banner\"] = test_data_set['Banner']\n",
    "                                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                                    result[\"Region\"] = Region_key\n",
    "                                    result[\"Test_period\"] = Test_period\n",
    "                                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                                    result[\"Front.Page\"] = test_data_set[\"Front.Page\"]\n",
    "                                    result[\"Middle.Page\"] = test_data_set[\"Middle.Page\"]\n",
    "                                    result[\"Back.Page\"] = test_data_set[\"Back.Page\"]\n",
    "                                    result['iter'] = i\n",
    "\n",
    "                                    result = result[[\"Region\",\"Banner\",\"Product\",'Channel',\"Test_period\",\"Week\",'Front.Page','Middle.Page','Back.Page',\"Actuals\",\"Predictions_rf\",'iter']]\n",
    "\n",
    "                                    # Appending results to the final results dataframe\n",
    "                                    Test_results = Test_results.append(result, ignore_index=True)                \n",
    "\n",
    "                            Test_results = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]                            \n",
    "                            # Merge & get prediction difference column\n",
    "                            Test_results = Test_results[Test_results.iter == i].merge(Test_results_sim[Test_results_sim.iter == i][['Banner','Week','Baseline Prediction']], on = ['Banner','Week'], how = 'left')        \n",
    "                            Test_results['Diff'] = Test_results[Test_results.iter == i]['Predictions_rf'] - Test_results[Test_results.iter == i]['Baseline Prediction']\n",
    "                            # Consider positive difference only\n",
    "                            Test_results = Test_results[Test_results.iter == i][Test_results[Test_results.iter == i].Diff > 0]\n",
    "                            Test_results = Test_results[Test_results.iter == i].sort_values('Diff', ascending = False)\n",
    "\n",
    "                            # Filtering out banner weeks where middle & front page promotion were given \n",
    "                            Test_results = Test_results[(Test_results['Middle.Page'] == 0) & (Test_results['Front.Page'] == 0)]    \n",
    "\n",
    "                            # Filtering banner weeks where back page promotion was given previously\n",
    "                            Test_results['Ban_week_concat'] = Test_results['Banner'] + Test_results['Week']\n",
    "                            back_page_promo_weeks_all['Ban_week_concat'] = back_page_promo_weeks_all['Banner'] + back_page_promo_weeks_all['Week']    \n",
    "                            Test_results = Test_results[Test_results['Ban_week_concat'].isin(back_page_promo_weeks_all['Ban_week_concat'].unique())]\n",
    "\n",
    "                            # Select required number of banner-weeks where promotion decrease would occur\n",
    "                            if (promo_change_toggle.value == 'Yes (Specific Banners)'):         \n",
    "                                Test_results = Test_results[Test_results.Banner.isin(list(banner_select.value))]\n",
    "                                for banner_loop in banner_select.value:    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_bp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_bp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_bp = ban_week_bp.append(ban_week_bp0, ignore_index = True)\n",
    "                                    elif i == 2:\n",
    "                                        ban_week_bp2 = ban_week_bp2.append(ban_week_bp0, ignore_index = True)\n",
    "\n",
    "                            elif (promo_change_toggle.value == 'Yes (All Banners)'):\n",
    "                                for banner_loop in qtr_df_final.Banner.unique():    \n",
    "                                    # Selecting banner-weeks\n",
    "                                    ban_week_bp0 = Test_results[Test_results.Banner == banner_loop][:abs(int(change_bp))][['Banner','Week']]\n",
    "                                    # if iter = 1, iter = 2 loop\n",
    "                                    if i == 1:\n",
    "                                        ban_week_bp = ban_week_bp.append(ban_week_bp0, ignore_index = True)\n",
    "                                    elif i == 2:\n",
    "                                        ban_week_bp2 = ban_week_bp2.append(ban_week_bp0, ignore_index = True)\n",
    "\n",
    "                            if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach                        \n",
    "                                ban_week_bp['Product'] = product_fil\n",
    "                            else:\n",
    "                                if i == 1:\n",
    "                                    ban_week_bp['Product'] = product_fil\n",
    "                                elif i == 2:\n",
    "                                    ban_week_bp2['Product'] = product_fil    \n",
    "\n",
    "                        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach                        \n",
    "                            ban_week_fp_copy = ban_week_fp_copy.append(ban_week_fp, ignore_index = True)\n",
    "                            ban_week_mp_copy = ban_week_mp_copy.append(ban_week_mp, ignore_index = True)\n",
    "                            ban_week_bp_copy = ban_week_bp_copy.append(ban_week_bp, ignore_index = True)\n",
    "\n",
    "                    clear_output()    \n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Product is not present in that Region/Channel for selected period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updated Promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the user doesn't want to upload adcal data\n",
    "if toggle_upload.value != 'Yes':\n",
    "    \n",
    "    if len(qtr_edv_baseline) != 0 : # In case there is not product present or product not present in selected region-channel & test period\n",
    "\n",
    "        if (promo_change_toggle.value == 'Yes (All Banners)') | (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "            if (front_change.children[1].value != 0) | (middle_change.children[1].value != 0) | (back_change.children[1].value != 0) | (front_change2.children[1].value != 0) | (middle_change2.children[1].value != 0) | (back_change2.children[1].value != 0) | (front_change3.children[1].value != 0) | (middle_change3.children[1].value != 0) | (back_change3.children[1].value != 0) : \n",
    "\n",
    "                display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp; Updating promotions </h2></div>'))\n",
    "                qtr_df = qtr_df_final.groupby(['Existing Model Pack','Banner']).agg({'Front Page':'sum','Middle Page':'sum','Back Page':'sum'}).reset_index()\n",
    "                qtr_df2 = qtr_df.copy()\n",
    "\n",
    "                for exist_pack in sorted(qtr_df_final['Existing Model Pack'].unique()): # Looping through each existing product\n",
    "\n",
    "                    # Increase/decrease promotions for required banners\n",
    "                    # Front increase\n",
    "                    if exist_pack == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                        if front_change.children[1].value > 0:\n",
    "                            if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                                ban_week_fp_copy = ban_week_fp.copy()\n",
    "                                \n",
    "                            for banner_change in ban_week_fp_copy[ban_week_fp_copy.Product == exist_pack]['Banner'].unique(): \n",
    "                                qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'].values[0] + ban_week_fp_copy[(ban_week_fp_copy.Product == exist_pack) & (ban_week_fp_copy['Banner'] == banner_change)].Banner.count()\n",
    "                    \n",
    "                    # Increasing promotions of 2nd exist product in ensemble approach\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                            if front_change2.children[1].value > 0:\n",
    "                                for banner_change in ban_week_fp_copy[ban_week_fp_copy.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'].values[0] + ban_week_fp_copy[(ban_week_fp_copy.Product == exist_pack) & (ban_week_fp_copy['Banner'] == banner_change)].Banner.count()\n",
    "                    # Increasing promotions of 3rd exist product in ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                            if front_change3.children[1].value > 0:\n",
    "                                for banner_change in ban_week_fp_copy[ban_week_fp_copy.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'].values[0] + ban_week_fp_copy[(ban_week_fp_copy.Product == exist_pack) & (ban_week_fp_copy['Banner'] == banner_change)].Banner.count()\n",
    "                            \n",
    "                    \n",
    "                    # In case the user selects to simulate New Product 2\n",
    "                    if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                        if toggle_np2.value == 'Yes':\n",
    "                            if front_change2.children[1].value > 0:\n",
    "                                for banner_change in ban_week_fp2[ban_week_fp2.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Front Page'] = qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Front Page'].values[0] + ban_week_fp2[(ban_week_fp2['Banner'] == banner_change) & (ban_week_fp2.Product == exist_pack)].Banner.count()\n",
    "                                    \n",
    "                    # Front decrease    \n",
    "                    if exist_pack == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                        if front_change.children[1].value < 0:\n",
    "                            if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                                ban_week_fp_copy = ban_week_fp.copy()\n",
    "                            \n",
    "                            for banner_change in ban_week_fp_copy[ban_week_fp_copy.Product == exist_pack]['Banner'].unique():\n",
    "                                qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'].values[0] - ban_week_fp_copy[(ban_week_fp_copy.Product == exist_pack) & (ban_week_fp_copy['Banner'] == banner_change)].Banner.count()\n",
    "\n",
    "                    # Decreasing promotions of 2nd exist product in ensemble approach\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                            if front_change2.children[1].value < 0:\n",
    "                                for banner_change in ban_week_fp_copy[ban_week_fp_copy.Product == exist_pack]['Banner'].unique():\n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'].values[0] - ban_week_fp_copy[(ban_week_fp_copy.Product == exist_pack) & (ban_week_fp_copy['Banner'] == banner_change)].Banner.count()\n",
    "\n",
    "                    # Decreasing promotions of 3rd exist product in ensemble approach\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                            if front_change3.children[1].value < 0:\n",
    "                                for banner_change in ban_week_fp_copy[ban_week_fp_copy.Product == exist_pack]['Banner'].unique():\n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Front Page'].values[0] - ban_week_fp_copy[(ban_week_fp_copy.Product == exist_pack) & (ban_week_fp_copy['Banner'] == banner_change)].Banner.count()\n",
    "\n",
    "\n",
    "                    # In case the user selects to simulate New Product 2\n",
    "                    if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                        # In case the user selects to simulate New Product 2\n",
    "                        if toggle_np2.value == 'Yes':                \n",
    "                            if front_change2.children[1].value < 0:\n",
    "                                for banner_change in ban_week_fp2[ban_week_fp2.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Front Page'] = qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Front Page'].values[0] - ban_week_fp2[(ban_week_fp2['Banner'] == banner_change) & (ban_week_fp2.Product == exist_pack)].Banner.count()\n",
    "\n",
    "                    # Middle increase\n",
    "                    if exist_pack == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                        if middle_change.children[1].value > 0:\n",
    "                            if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                                ban_week_mp_copy = ban_week_mp.copy()\n",
    "                            \n",
    "                            for banner_change in ban_week_mp_copy[ban_week_mp_copy.Product == exist_pack]['Banner'].unique(): \n",
    "                                qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'].values[0] + ban_week_mp_copy[(ban_week_mp_copy.Product == exist_pack) & (ban_week_mp_copy['Banner'] == banner_change)].Banner.count()\n",
    "                    \n",
    "                    # Increasing promotions of 2nd exist product in ensemble approach\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                            if middle_change2.children[1].value > 0:\n",
    "                                for banner_change in ban_week_mp_copy[ban_week_mp_copy.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'].values[0] + ban_week_mp_copy[(ban_week_mp_copy.Product == exist_pack) & (ban_week_mp_copy['Banner'] == banner_change)].Banner.count()\n",
    "                    # Increasing promotions of 3rd exist product in ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                            if middle_change3.children[1].value > 0:\n",
    "                                for banner_change in ban_week_mp_copy[ban_week_mp_copy.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'].values[0] + ban_week_mp_copy[(ban_week_mp_copy.Product == exist_pack) & (ban_week_mp_copy['Banner'] == banner_change)].Banner.count()\n",
    "                            \n",
    "                    \n",
    "                    # In case the user selects to simulate New Product 2\n",
    "                    if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                        if toggle_np2.value == 'Yes':\n",
    "                            if middle_change2.children[1].value > 0:\n",
    "                                for banner_change in ban_week_mp2[ban_week_mp2.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Middle Page'] = qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Middle Page'].values[0] + ban_week_mp2[(ban_week_mp2['Banner'] == banner_change) & (ban_week_mp2.Product == exist_pack)].Banner.count()\n",
    "                                    \n",
    "                    # Middle decrease    \n",
    "                    if exist_pack == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                        if middle_change.children[1].value < 0:\n",
    "                            if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                                ban_week_mp_copy = ban_week_mp.copy()\n",
    "                            \n",
    "                            for banner_change in ban_week_mp_copy[ban_week_mp_copy.Product == exist_pack]['Banner'].unique():\n",
    "                                qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'].values[0] - ban_week_mp_copy[(ban_week_mp_copy.Product == exist_pack) & (ban_week_mp_copy['Banner'] == banner_change)].Banner.count()\n",
    "\n",
    "                    # Decreasing promotions of 2nd exist product in ensemble approach\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                            if middle_change2.children[1].value < 0:\n",
    "                                for banner_change in ban_week_mp_copy[ban_week_mp_copy.Product == exist_pack]['Banner'].unique():\n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'].values[0] - ban_week_mp_copy[(ban_week_mp_copy.Product == exist_pack) & (ban_week_mp_copy['Banner'] == banner_change)].Banner.count()\n",
    "\n",
    "                    # Decreasing promotions of 3rd exist product in ensemble approach\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                            if middle_change3.children[1].value < 0:\n",
    "                                for banner_change in ban_week_mp_copy[ban_week_mp_copy.Product == exist_pack]['Banner'].unique():\n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Middle Page'].values[0] - ban_week_mp_copy[(ban_week_mp_copy.Product == exist_pack) & (ban_week_mp_copy['Banner'] == banner_change)].Banner.count()\n",
    "\n",
    "                    # In case the user selects to simulate New Product 2\n",
    "                    if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                        # In case the user selects to simulate New Product 2\n",
    "                        if toggle_np2.value == 'Yes':                \n",
    "                            if middle_change2.children[1].value < 0:\n",
    "                                for banner_change in ban_week_mp2[ban_week_mp2.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Middle Page'] = qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Middle Page'].values[0] - ban_week_mp2[(ban_week_mp2['Banner'] == banner_change) & (ban_week_mp2.Product == exist_pack)].Banner.count()\n",
    "\n",
    "                    # Back increase\n",
    "                    if exist_pack == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                        if back_change.children[1].value > 0:\n",
    "                            if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                                ban_week_bp_copy = ban_week_bp.copy()\n",
    "                            \n",
    "                            for banner_change in ban_week_bp_copy[ban_week_bp_copy.Product == exist_pack]['Banner'].unique(): \n",
    "                                qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'].values[0] + ban_week_bp_copy[(ban_week_bp_copy.Product == exist_pack) & (ban_week_bp_copy['Banner'] == banner_change)].Banner.count()\n",
    "                    \n",
    "                    # Increasing promotions of 2nd exist product in ensemble approach\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                            if back_change2.children[1].value > 0:\n",
    "                                for banner_change in ban_week_bp_copy[ban_week_bp_copy.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'].values[0] + ban_week_bp_copy[(ban_week_bp_copy.Product == exist_pack) & (ban_week_bp_copy['Banner'] == banner_change)].Banner.count()\n",
    "                    # Increasing promotions of 3rd exist product in ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                            if back_change3.children[1].value > 0:\n",
    "                                for banner_change in ban_week_bp_copy[ban_week_bp_copy.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'].values[0] + ban_week_bp_copy[(ban_week_bp_copy.Product == exist_pack) & (ban_week_bp_copy['Banner'] == banner_change)].Banner.count()\n",
    "                            \n",
    "                    \n",
    "                    # In case the user selects to simulate New Product 2\n",
    "                    if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                        if toggle_np2.value == 'Yes':\n",
    "                            if back_change2.children[1].value > 0:\n",
    "                                for banner_change in ban_week_bp2[ban_week_bp2.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Back Page'] = qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Back Page'].values[0] + ban_week_bp2[(ban_week_bp2['Banner'] == banner_change) & (ban_week_bp2.Product == exist_pack)].Banner.count()\n",
    "                                    \n",
    "                    # Back decrease    \n",
    "                    if exist_pack == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                        if back_change.children[1].value < 0:\n",
    "                            if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                                ban_week_bp_copy = ban_week_bp.copy()\n",
    "                            \n",
    "                            for banner_change in ban_week_bp_copy[ban_week_bp_copy.Product == exist_pack]['Banner'].unique():\n",
    "                                qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'].values[0] - ban_week_bp_copy[(ban_week_bp_copy.Product == exist_pack) & (ban_week_bp_copy['Banner'] == banner_change)].Banner.count()\n",
    "\n",
    "                    # Decreasing promotions of 2nd exist product in ensemble approach\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                            if back_change2.children[1].value < 0:\n",
    "                                for banner_change in ban_week_bp_copy[ban_week_bp_copy.Product == exist_pack]['Banner'].unique():\n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'].values[0] - ban_week_bp_copy[(ban_week_bp_copy.Product == exist_pack) & (ban_week_bp_copy['Banner'] == banner_change)].Banner.count()\n",
    "\n",
    "                    # Decreasing promotions of 3rd exist product in ensemble approach\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if exist_pack == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                            if back_change3.children[1].value < 0:\n",
    "                                for banner_change in ban_week_bp_copy[ban_week_bp_copy.Product == exist_pack]['Banner'].unique():\n",
    "                                    qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'] = qtr_df.loc[(qtr_df['Existing Model Pack'] == exist_pack) & (qtr_df['Banner'] == banner_change),'Back Page'].values[0] - ban_week_bp_copy[(ban_week_bp_copy.Product == exist_pack) & (ban_week_bp_copy['Banner'] == banner_change)].Banner.count()\n",
    "\n",
    "                    # In case the user selects to simulate New Product 2\n",
    "                    if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach\n",
    "                        # In case the user selects to simulate New Product 2\n",
    "                        if toggle_np2.value == 'Yes':                \n",
    "                            if back_change2.children[1].value < 0:\n",
    "                                for banner_change in ban_week_bp2[ban_week_bp2.Product == exist_pack]['Banner'].unique(): \n",
    "                                    qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Back Page'] = qtr_df2.loc[(qtr_df2['Existing Model Pack'] == exist_pack) & (qtr_df2['Banner'] == banner_change),'Back Page'].values[0] - ban_week_bp2[(ban_week_bp2['Banner'] == banner_change) & (ban_week_bp2.Product == exist_pack)].Banner.count()\n",
    "\n",
    "                clear_output()  \n",
    "\n",
    "                # In case the user selects to simulate New Product 2\n",
    "                if toggle_ensemble_normal.value != 'Multiple Product': # In case of normal approach                                                                                                                                                      \n",
    "                    if toggle_np2.value == 'Yes':\n",
    "                        # Rename columns\n",
    "                        qtr_df.rename(columns = {'Front Page':'Front Page (New Product 1)','Middle Page':'Middle Page (New Product 1)','Back Page':'Back Page (New Product 1)'}, inplace = True)\n",
    "                        qtr_df2.rename(columns = {'Front Page':'Front Page (New Product 2)','Middle Page':'Middle Page (New Product 2)','Back Page':'Back Page (New Product 2)'}, inplace = True)\n",
    "                        qtr_df2.drop(columns = {'Existing Model Pack'}, inplace = True)\n",
    "                        qtr_df_all = qtr_df.merge(qtr_df2, on = 'Banner', how = 'left')\n",
    "                        display(widgets.Label(value=\"Updated Promotions of $New Product 1$ vs $New Product 2$\"),HTML(qtr_df_all.to_html(index=False)))        \n",
    "                    else:\n",
    "                        display(HTML(qtr_df.to_html(index=False)))                                \n",
    "                else:\n",
    "                    display(HTML(qtr_df.to_html(index=False)))        \n",
    "    else:\n",
    "        print(\"Product is not present in that Region/Channel for selected period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simulate new product cell run\n",
    "def run_all(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index(), IPython.notebook.ncells())'))\n",
    "\n",
    "layout = widgets.Layout(width='auto', height='40px')\n",
    "button = widgets.Button(description=\"Simulate New Product Results\",layout = layout, button_style='danger')\n",
    "button.on_click(run_all)\n",
    "\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Defining empty dataframes for model training\n",
    "combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "Test_results = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "Test_Data = pd.DataFrame()\n",
    "Test_results_exist2 = pd.DataFrame()\n",
    "\n",
    "# Checking category, pack subtype, pack content of existing product matches with new product\n",
    "cat_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['Category'].unique()[0]\n",
    "packsubtype_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['Pack.Subtype'].unique()[0]\n",
    "packcontent_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['PACK_CONTENT'].unique()[0]\n",
    "\n",
    "# Select same quarter for existing data\n",
    "\n",
    "Test_week_list_qtr = {\"Q1\" : [\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-10\",\"2019-11\",\"2019-12\",\"2019-13\"],\n",
    "     \"Q2\" : [\"2019-14\",\"2019-15\",\"2019-16\",\"2019-17\",\"2019-18\",\"2019-19\",\"2019-20\",\"2019-21\",\"2019-22\",\"2019-23\",\"2019-24\",\"2019-25\",\"2019-26\"],\n",
    "     \"Q3\" : [\"2019-27\",\"2019-28\",\"2019-29\",\"2019-30\",\"2019-31\",\"2019-32\",\"2019-33\",\"2019-34\",\"2019-35\",\"2019-36\",\"2019-37\",\"2019-38\",\"2019-39\"],\n",
    "     \"Q4\" : [\"2019-40\",\"2019-41\",\"2019-42\",\"2019-43\",\"2019-44\",\"2019-45\",\"2019-46\",\"2019-47\",\"2019-48\",\"2019-49\",\"2019-50\",\"2019-51\",\"2019-52\"],\n",
    "      }\n",
    "\n",
    "# Defining test week list\n",
    "Test_week_list = {\"2019Q1\" : [\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-10\",\"2019-11\",\"2019-12\",\"2019-13\"],\n",
    "     \"2019Q2\" : [\"2019-14\",\"2019-15\",\"2019-16\",\"2019-17\",\"2019-18\",\"2019-19\",\"2019-20\",\"2019-21\",\"2019-22\",\"2019-23\",\"2019-24\",\"2019-25\",\"2019-26\"],\n",
    "     \"2019Q3\" : [\"2019-27\",\"2019-28\",\"2019-29\",\"2019-30\",\"2019-31\",\"2019-32\",\"2019-33\",\"2019-34\",\"2019-35\",\"2019-36\",\"2019-37\",\"2019-38\",\"2019-39\"],\n",
    "     \"2019Q4\" : [\"2019-40\",\"2019-41\",\"2019-42\",\"2019-43\",\"2019-44\",\"2019-45\",\"2019-46\",\"2019-47\",\"2019-48\",\"2019-49\",\"2019-50\",\"2019-51\",\"2019-52\"],\n",
    "                  \n",
    "# CHECK : ADDING 2020 weeks                  \n",
    "     \"2020Q1\" : [\"2020-01\",\"2020-02\",\"2020-03\",\"2020-04\",\"2020-05\",\"2020-06\",\"2020-07\",\"2020-08\",\"2020-09\",\"2020-10\",\"2020-11\",\"2020-12\",\"2020-13\"],\n",
    "     \"2020Q2\" : [\"2020-14\",\"2020-15\",\"2020-16\",\"2020-17\",\"2020-18\",\"2020-19\",\"2020-20\",\"2020-21\",\"2020-22\",\"2020-23\",\"2020-24\",\"2020-25\",\"2020-26\"],\n",
    "     \"2020Q3\" : [\"2020-27\",\"2020-28\",\"2020-29\",\"2020-30\",\"2020-31\",\"2020-32\",\"2020-33\",\"2020-34\",\"2020-35\",\"2020-36\",\"2020-37\",\"2020-38\",\"2020-39\"],\n",
    "     \"2020Q4\" : [\"2020-40\",\"2020-41\",\"2020-42\",\"2020-43\",\"2020-44\",\"2020-45\",\"2020-46\",\"2020-47\",\"2020-48\",\"2020-49\",\"2020-50\",\"2020-51\",\"2020-52\"],\n",
    "     \"fulltrain\" : []}\n",
    "\n",
    "#Model training and prediction\n",
    "category_check_list=[]\n",
    "\n",
    "########################################################################################################################\n",
    "                                    # SIMULATION (WITH UPLOADED DATA)\n",
    "########################################################################################################################\n",
    "\n",
    "# In case the user uploads the data\n",
    "if (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "\n",
    "    # In case no product is present in selected region-channel\n",
    "    if len(qtr_edv_baseline) != 0 :\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Simulating New Product Results</h2></div>'))\n",
    "\n",
    "        # Run for 1 iteration\n",
    "        iter_change_list = [1]\n",
    "\n",
    "        for i in iter_change_list:\n",
    "            channel_list = upload_data.Channel.unique()\n",
    "            #Model training and prediction\n",
    "            for Region_key in upload_data.Region.unique(): \n",
    "                # In case the product is not present at that region\n",
    "                if len(upload_data[(upload_data.Region == Region_key)]) == 0:\n",
    "                    continue\n",
    "\n",
    "                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "    \n",
    "                Volume_dataset_upload = upload_data.loc[(upload_data[\"Region\"] == Region_key)\n",
    "                                           &(upload_data[\"Channel\"].isin(channel_list))]\n",
    "\n",
    "    \n",
    "                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                Volume_dataset_upload['Pantry2'] = Volume_dataset_upload['Pantry2'].fillna(0)\n",
    "            \n",
    "                required_columns_upload = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week'] #CHANGE 101: Integration of uplaoded data with tool\n",
    "                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week'] #CHANGE 101: Integration of uplaoded data with tool\n",
    "                combined_dataset = Volume_dataset[required_columns] \n",
    "                combined_dataset_upload = Volume_dataset_upload[required_columns_upload] \n",
    "\n",
    "                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "                \n",
    "                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                \n",
    "                combined_dataset_upload[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset_upload[\"Week.Name\"]]\n",
    "                combined_dataset_upload[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset_upload[\"Week.Name\"]]                \n",
    "\n",
    "                banner_dummies = pd.get_dummies(combined_dataset.Banner) \n",
    "                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1) #sorting banner columns\n",
    "                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "                \n",
    "                # Uploaded data banner dummies\n",
    "                banner_dummies_upload = pd.get_dummies(combined_dataset_upload.Banner)                                 \n",
    "                # If all banners are not present for the uploaded data\n",
    "                c = [i for i in list(banner_dummies.columns) if i not in list(banner_dummies_upload.columns)]\n",
    "                if len(c) != 0:\n",
    "                    banner_dummies_upload[c] = 0\n",
    "                # Reordering columns                \n",
    "                banner_dummies_upload = banner_dummies_upload.reindex(sorted(banner_dummies_upload.columns), axis=1)                \n",
    "                combined_dataset_upload = pd.concat([combined_dataset_upload, banner_dummies_upload], axis=1)\n",
    "                \n",
    "                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "\n",
    "                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                combined_dataset.sort_values('Week',inplace = True)\n",
    "                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                combined_dataset_upload.sort_values('Week',inplace = True)\n",
    "                combined_dataset_upload = pd.merge(combined_dataset_upload,prod_stage_check_upload(),on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset_upload['Intial_weeks'] = combined_dataset_upload['Intial_weeks'].fillna('Stabilization')                \n",
    "                \n",
    "                #Adding dummies for category\n",
    "                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1) \n",
    "                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                cat_col = category_dummies.columns #need all category dummy in the test data\n",
    "                \n",
    "                #Adding dummies for category UPLOADED DDATA\n",
    "                combined_dataset_upload[cat_col] = 0\n",
    "                \n",
    "                #Adding dummies for product attribute - pack subtype\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                pack_subtypes_col = pack_subtypes_dummies.columns\n",
    "                \n",
    "                #Adding dummies for packsubtype UPLOADED DDATA\n",
    "                combined_dataset_upload[pack_subtypes_col] = 0\n",
    "                \n",
    "                # Adding pack content dummies\n",
    "                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                pack_content_col = pack_content_dummies.columns\n",
    "                \n",
    "                #Adding dummies for packsubtype UPLOADED DDATA\n",
    "                combined_dataset_upload[pack_content_col] = 0\n",
    "                \n",
    "                combined_dataset_upload.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "                combined_dataset_upload.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "                combined_dataset_upload.drop([\"Category\"], inplace=True, axis=1)\n",
    "                \n",
    "                # Product stage dummies\n",
    "                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies_upload = pd.get_dummies(combined_dataset_upload['Intial_weeks'])\n",
    "                \n",
    "                # If all product stages are not present for the uploaded data\n",
    "                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_upload.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    stage_dummies_upload[c] = 0\n",
    "                # Reordering columns                \n",
    "                stage_dummies_upload = stage_dummies_upload.reindex(sorted(stage_dummies_upload.columns), axis=1)\n",
    "                combined_dataset_upload = pd.concat([combined_dataset_upload, stage_dummies_upload], axis=1)\n",
    "                combined_dataset_upload.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                #Adding discount depth columns\n",
    "                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                combined_dataset_upload[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset_upload[\"Adcal_DD\"]]\n",
    "                combined_dataset_upload[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset_upload[\"Adcal_DD\"]]            \n",
    "\n",
    "                # Adcal Price calculation\n",
    "                combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Adcal Price calculation\n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                combined_dataset_upload.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                # Dropping null volumes\n",
    "                complete_dataset = combined_dataset.copy()\n",
    "                complete_dataset_upload = combined_dataset_upload.copy()\n",
    "\n",
    "                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "                complete_dataset_upload['Adcal_Price'] = np.log(complete_dataset_upload['Adcal_Price'])\n",
    "\n",
    "                # TEST PERIOD LOOP (futuristic test period)                \n",
    "                Test_week_list = {\"Post_Launch\" : upload_data[\"Week\"].unique()}\n",
    "                Test_periods = [\"Post_Launch\"]\n",
    "                \n",
    "                for Test_period in Test_periods:                    \n",
    "                    #Filtering test weeks\n",
    "                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                    Test_week_list_qtr_weeks = []\n",
    "                    \n",
    "                    # Selecting required quarter-weeks for existing product\n",
    "                    for quarters in upload_data['Test_period'].str[4:].unique():    \n",
    "                        Test_week_list_qtr_weeks.extend(Test_week_list_qtr[quarters])\n",
    "\n",
    "                    # Dividing train & test data\n",
    "                    complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                    # Data for existing product\n",
    "                    complete_test_data_set_exist = complete_dataset.loc[(complete_dataset.Product == product_fil) & (complete_dataset[\"Week\"].isin(Test_week_list_qtr_weeks))].reset_index(drop=True) # CHECK upload\n",
    "                    # Data for new product\n",
    "                    complete_test_data_set = complete_dataset_upload.loc[complete_dataset_upload[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                    # Dropping unneccessary columns from train dataset\n",
    "                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                    train_data_brand = train_data_set.copy()\n",
    "                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                    # Filtering for existing product\n",
    "                    test_data_set = complete_test_data_set.copy()\n",
    "                    test_data_set_exist = complete_test_data_set_exist.copy()\n",
    "\n",
    "                    # New Category\n",
    "                    if i == 1:\n",
    "                        for cols in upload_data.Category.unique():                            \n",
    "                            test_data_set[cols] = 1\n",
    "                            \n",
    "                    # New Pack Subtype\n",
    "                    if i == 1:\n",
    "                        for cols in upload_data['Pack.Subtype'].unique():                            \n",
    "                            test_data_set[cols] = 1\n",
    "                    \n",
    "                    # New Pack Content\n",
    "                    if i == 1:\n",
    "                        for cols in upload_data['PACK_CONTENT'].unique():                            \n",
    "                            test_data_set[cols] = 1\n",
    "                    \n",
    "                    if product_fil == 'TCCC CORE POWER 414 ML BTTL':\n",
    "                        test_data_set = test_data_set.drop_duplicates()\n",
    "                        test_data_set_exist = test_data_set_exist.drop_duplicates()\n",
    "                        \n",
    "                    test_data_brand = test_data_set.copy()\n",
    "                    test_data_brand_exist = test_data_set_exist.copy()\n",
    "\n",
    "                    if i == 1 :\n",
    "                        # For New SIZE\n",
    "                        test_data_brand['SIZE_ML'] = upload_data.SIZE_ML.unique()[0]\n",
    "\n",
    "                        # For New Count\n",
    "                        test_data_brand['COUNT'] = upload_data.COUNT.unique()[0]\n",
    "\n",
    "                    # Dropping unneccessary columns from test dataset\n",
    "                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\"], inplace=True, axis=1)  #CHANGE 101: Integration of uplaoded data with tool\n",
    "                    test_data_brand_exist.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                    #Creating test and train data - independent & dependent columns\n",
    "                    X_train = train_data_brand\n",
    "                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                    X_test = test_data_brand\n",
    "                    X_test_exist = test_data_brand_exist\n",
    "\n",
    "                    # Appending test data\n",
    "                    test_data_set[\"Region\"] = Region_key\n",
    "                    test_data_set[\"Test_period\"] = Test_period\n",
    "                    test_data_set['iter'] = i\n",
    "\n",
    "                    # In case the product is not present in certain region-quarter\n",
    "                    if len(test_data_set) == 0:\n",
    "                        continue\n",
    "\n",
    "                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "\n",
    "                    #Model training\n",
    "                    rf = RandomForestRegressor(n_jobs=4,random_state=0)\n",
    "                    rf.fit(X_train, y_train)\n",
    "\n",
    "                    #Model predictions\n",
    "                    predictions_rf = rf.predict(X_test)                \n",
    "                    predictions_rf_exist = rf.predict(X_test_exist)                \n",
    "\n",
    "                    #Storing test results\n",
    "                    result = X_test.copy()\n",
    "\n",
    "                    result[\"Predictions_rf\"] = predictions_rf\n",
    "                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])    \n",
    "                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                    result[\"Region\"] = Region_key\n",
    "                    result[\"Test_period\"] = Test_period\n",
    "                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                    result[\"DD_1\"] = test_data_set['DD_1']\n",
    "                    result[\"DD_2\"] = test_data_set['DD_2']\n",
    "                    result[\"Banner\"] = test_data_set['Banner']\n",
    "                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                    result[\"Adcal_Price\"] = test_data_set[\"Adcal_Price\"]\n",
    "                    result['Adcal_Price'] = np.exp(result['Adcal_Price'])\n",
    "\n",
    "                    result_exist = X_test_exist.copy()\n",
    "            \n",
    "                    result_exist[\"Adcal_Price_exist\"] = test_data_set_exist[\"Adcal_Price\"].values\n",
    "                    result_exist['Adcal_Price_exist'] = np.exp(result_exist[\"Adcal_Price_exist\"])\n",
    "                    result_exist['Predictions_rf_exist'] = predictions_rf_exist\n",
    "                    result_exist['Predictions_rf_exist'] = np.exp(result_exist['Predictions_rf_exist'])   \n",
    "                    result_exist['Week'] = test_data_set_exist['Week']\n",
    "                    result_exist['Banner'] = test_data_set_exist['Banner']\n",
    "                    result_exist['Product'] = test_data_set_exist['Product']\n",
    "                    result_exist['Channel'] = test_data_set_exist['Channel']\n",
    "                    result_exist['Region'] = Region_key\n",
    "                    result_exist['Test_period'] = Test_period\n",
    "\n",
    "                    result['iter'] = i            \n",
    " \n",
    "                    result = result[[\"Region\",\"Product\",'Channel',\"Banner\",\"Test_period\",\"Week\",\"Predictions_rf\",\"DD_1\",\"DD_2\",\"Adcal_Price\",'iter']]\n",
    "                    result_exist = result_exist[[\"Region\",\"Product\",'Channel',\"Banner\",\"Test_period\",\"Week\",\"Predictions_rf_exist\",\"Adcal_Price_exist\"]]\n",
    "                    if i == 1:\n",
    "                        result['Volume_RU'] = result['Predictions_rf']\n",
    "                        result_exist['Volume_RU_exist'] = result_exist['Predictions_rf_exist']\n",
    "\n",
    "                    result['Sales_derived'] = result['Predictions_rf'] * result['Adcal_Price']\n",
    "                    result_exist['Sales_derived_exist'] = result_exist['Predictions_rf_exist'] * result_exist['Adcal_Price_exist']\n",
    "\n",
    "                    # Appending results to the final results dataframe            \n",
    "                    Test_results = Test_results.append(result, ignore_index=True)\n",
    "                    Test_results_exist2 = Test_results_exist2.append(result_exist, ignore_index = True)\n",
    "\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "    else:\n",
    "        print(\"Product is not present in that Region/Channel for selected period\")\n",
    "                \n",
    "            \n",
    "########################################################################################################################\n",
    "                                    # SIMULATION (WITHOUT UPLOADED DATA)\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "elif toggle_upload.value == 'No':        \n",
    "\n",
    "    #Model training and prediction\n",
    "    category_check_list=[]\n",
    "\n",
    "    # In case no product is present in selected region-channel\n",
    "    if len(qtr_edv_baseline) != 0 :\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Simulating New Product Results</h2></div>'))\n",
    "        size_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['SIZE_ML'].unique()[0]\n",
    "        count_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['COUNT'].unique()[0]\n",
    "        cat_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['Category'].unique()[0]\n",
    "        packsubtype_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['Pack.Subtype'].unique()[0]\n",
    "        packcontent_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['PACK_CONTENT'].unique()[0]\n",
    "\n",
    "        if toggle_np2.value == 'Yes':\n",
    "            iter_change_list = [1,2]\n",
    "        else:\n",
    "            iter_change_list = [1]\n",
    "\n",
    "        for i in iter_change_list:\n",
    "\n",
    "            for product_fil in sorted(qtr_df_final['Existing Model Pack'].unique()): # Looping through each existing product\n",
    "\n",
    "                ################# PROMOTION CHANGE VARIABLES #################\n",
    "                if (promo_change_toggle.value == 'Yes (All Banners)') | (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "\n",
    "                    if i == 1:\n",
    "                        change_mp = middle_change.children[1].value\n",
    "                    elif i == 2:\n",
    "                        change_mp = middle_change2.children[1].value\n",
    "\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "                            clear_output()\n",
    "                            print('Select 3 products as existing model pack')\n",
    "                        else:\n",
    "                        \n",
    "                            if product_fil == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                                change_mp = middle_change.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                                change_mp = middle_change2.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                                change_mp = middle_change3.children[1].value\n",
    "\n",
    "                    if i == 1:\n",
    "                        change_fp = front_change.children[1].value\n",
    "                    elif i == 2:\n",
    "                        change_fp = front_change2.children[1].value\n",
    "\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "                            clear_output()\n",
    "                            print('Select 3 products as existing model pack')\n",
    "                        else:\n",
    "\n",
    "                            if product_fil == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                                change_fp = front_change.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                                change_fp = front_change2.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                                change_fp = front_change3.children[1].value\n",
    "\n",
    "                    if i == 1:\n",
    "                        change_bp = back_change.children[1].value\n",
    "                    elif i == 2:\n",
    "                        change_bp = back_change2.children[1].value\n",
    "\n",
    "                    if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                        if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "                            clear_output()\n",
    "                            print('Select 3 products as existing model pack')\n",
    "                        else:\n",
    "                            if product_fil == edv_disc_join['Existing Model Pack'].unique()[0]:\n",
    "                                change_bp = back_change.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[1]:\n",
    "                                change_bp = back_change2.children[1].value\n",
    "                            elif product_fil == edv_disc_join['Existing Model Pack'].unique()[2]:\n",
    "                                change_bp = back_change3.children[1].value\n",
    "\n",
    "                ################# #Model training and prediction #################\n",
    "\n",
    "                #Model training and prediction\n",
    "                for Region_key in Region_List:                 \n",
    "                    # In case the product is not present at that region\n",
    "                    if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                        continue\n",
    "\n",
    "                    category_check_list.extend(list(Volume_dataset_all_reg[(Volume_dataset_all_reg[\"Region\"] == Region_key) & (Volume_dataset_all_reg[\"Product\"] == product_fil)].Category.unique()))\n",
    "\n",
    "                    Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                               &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                    Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)   \n",
    "                    required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                    combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                    combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                             (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                    combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                    combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                    ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                    banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                    # Uploaded data banner dummies\n",
    "                    banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                    # If all banners are not present for the training data\n",
    "                    c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                    if len(c) != 0:\n",
    "                        banner_dummies[c] = 0\n",
    "\n",
    "                    # Reordering columns                \n",
    "                    banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                    combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                    ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                    combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                    combined_dataset.sort_values('Week',inplace = True)\n",
    "                    combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                    combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                    combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                    #Adding dummies for category\n",
    "                    category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                    # Adding dummies for product attribute - Category\n",
    "                    pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                    # Uploaded data category dummies\n",
    "                    category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                    # If all pack subtypes are not present for the training data\n",
    "                    c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                    if len(c) != 0:\n",
    "                        category_dummies[c] = 0\n",
    "\n",
    "                    # Reordering columns                \n",
    "                    category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                    combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                    combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                    # Adding dummies for product attribute - pack subtype\n",
    "                    pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                    # Uploaded data pack subtype dummies\n",
    "                    pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                    # If all pack subtypes are not present for the training data\n",
    "                    c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                    if len(c) != 0:\n",
    "                        pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                    # Reordering columns                \n",
    "                    pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                    combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                    combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                    # Adding pack content dummies\n",
    "                    pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                    # Uploaded data pack content dummies\n",
    "                    pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                    # If all pack contents are not present for the training data\n",
    "                    c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                    if len(c) != 0:\n",
    "                        pack_content_dummies[c] = 0\n",
    "\n",
    "                    # Reordering columns                \n",
    "                    pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                    combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                    combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "                    \n",
    "                    combined_dataset_test = combined_dataset.copy()\n",
    "\n",
    "                    # Product stage dummies\n",
    "                    stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                    combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                    combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                    # Changing product stage as per start date provided by the user\n",
    "                    prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                    # Getting updated product stages\n",
    "                    combined_dataset_test_check = combined_dataset_test[combined_dataset_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                    combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                    combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                    combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                    combined_dataset_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                    # Product stage dummies\n",
    "                    stage_dummies_test = pd.get_dummies(combined_dataset_test['Intial_weeks'])\n",
    "\n",
    "                    # If all product stages are not present for the uploaded data\n",
    "                    c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                    if len(c) != 0 :\n",
    "                        stage_dummies_test[c] = 0\n",
    "\n",
    "                    # Reordering columns                \n",
    "                    stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                    combined_dataset_test = pd.concat([combined_dataset_test, stage_dummies_test], axis=1)\n",
    "                    combined_dataset_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                    # In case the user doesn't want to upload adcal data\n",
    "                    if toggle_upload.value != 'Yes':                        \n",
    "                        if 'All' in list(test_period.value):\n",
    "                            # Discount change\n",
    "                            if i == 1:\n",
    "                                if product_fil == sorted(qtr_df_final['Existing Model Pack'].unique())[0]:\n",
    "                                    combined_dataset_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test['Adcal_DD']\n",
    "                                    combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "\n",
    "                                if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                                    if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "                                        clear_output()\n",
    "                                        print('Select 3 products as existing model pack')\n",
    "                                    else:\n",
    "\n",
    "                                        if product_fil == sorted(qtr_df_final['Existing Model Pack'].unique())[1]:\n",
    "                                            combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                                            combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "                                        if product_fil == sorted(qtr_df_final['Existing Model Pack'].unique())[2]:\n",
    "                                            combined_dataset_test['Adcal_DD'] = (1+disc_change_fil3)*combined_dataset_test['Adcal_DD']\n",
    "                                            combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil3)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "\n",
    "\n",
    "                            elif i == 2:\n",
    "                                combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                                combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']          \n",
    "                        else:                \n",
    "                            if i == 1:\n",
    "                                if product_fil == sorted(qtr_df_final['Existing Model Pack'].unique())[0]:                                \n",
    "                                    combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil)]['Adcal_DD']                                 \n",
    "                                    combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "\n",
    "                                if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "                                    if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "                                        clear_output()\n",
    "                                        print('Select 3 products as existing model pack')\n",
    "                                    else:\n",
    "                                    \n",
    "                                        if product_fil == sorted(qtr_df_final['Existing Model Pack'].unique())[1]:                                \n",
    "                                            combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fi2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']                                 \n",
    "                                            combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "                                        if product_fil == sorted(qtr_df_final['Existing Model Pack'].unique())[2]:                                \n",
    "                                            combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil3),'Adcal_DD'] = (1+disc_change_fil3)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil3)]['Adcal_DD']                                 \n",
    "                                            combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil3)),'EDV.Price'] = (1+base_price_change_fil3)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil3))]['EDV.Price']\n",
    "\n",
    "                            elif i == 2:\n",
    "                                # Discount change\n",
    "                                combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                                # Baseline pricing\n",
    "                                combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "                    #Adding discount depth columns\n",
    "                    combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                          else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                    combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                     else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                    combined_dataset_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                          else 0 for x in combined_dataset_test[\"Adcal_DD\"]]\n",
    "                    combined_dataset_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                     else 0 for x in combined_dataset_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                    # Adcal Price calculation\n",
    "                    # In case the discounts go over 100%, limit to 100%\n",
    "                    combined_dataset_test.loc[combined_dataset_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "                    combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                    # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                    combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                    # Adcal Price calculation\n",
    "                    combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                    # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                    combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                    # Dropping adcal DD as DD1 & DD2 are added\n",
    "                    combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                    combined_dataset_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                    # Dropping null volumes\n",
    "                    complete_dataset = combined_dataset.copy()\n",
    "                    complete_dataset_test = combined_dataset_test.copy()\n",
    "\n",
    "                    complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "                    complete_dataset_test = complete_dataset_test.loc[complete_dataset_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                    complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                    complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                    complete_dataset_test['Adcal_Price'] = np.log(complete_dataset_test['Adcal_Price'])\n",
    "                    complete_dataset_test['Eq.Unit.Sales'] = np.log(complete_dataset_test['Eq.Unit.Sales'])\n",
    "\n",
    "                    # TEST PERIOD LOOP\n",
    "                    Test_periods = ['2019Q1','2019Q2','2019Q3','2019Q4']\n",
    "\n",
    "                    for Test_period in Test_periods:\n",
    "                        if (Test_period>end_week.children[8].value) | (Test_period > Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].max()):\n",
    "                            continue\n",
    "                        if (Test_period<start_week.children[8].value) | (Test_period < Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].min()):\n",
    "                            continue                \n",
    "\n",
    "                        #Filtering test weeks\n",
    "                        Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                        # Dividing train & test data\n",
    "                        complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                        # Data for existing product\n",
    "                        complete_test_data_set_exist = complete_dataset.loc[complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                        # Data for new product\n",
    "                        complete_test_data_set = complete_dataset_test.loc[complete_dataset_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                        # Dropping unneccessary columns from train dataset\n",
    "                        train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                        train_data_brand = train_data_set.copy()\n",
    "                        train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                        # Filtering for existing product\n",
    "                        test_data_set = complete_test_data_set[complete_test_data_set.Product == product_fil].copy()\n",
    "                        test_data_set_exist = complete_test_data_set_exist[complete_test_data_set_exist.Product == product_fil].copy()\n",
    "\n",
    "                        # In case the user doesn't want to upload adcal data\n",
    "                        if toggle_upload.value != 'Yes':\n",
    "\n",
    "                            #################### INCREASING/DECREASING PROMOTIONS FOR BANNER WEEKS COMBINATIONS ####################\n",
    "\n",
    "                            if (promo_change_toggle.value == 'Yes (All Banners)') | (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "                                if (front_change.children[1].value != 0) | (middle_change.children[1].value != 0) | (back_change.children[1].value != 0) | (front_change2.children[1].value != 0) | (middle_change2.children[1].value != 0) | (back_change2.children[1].value != 0) | (front_change3.children[1].value != 0) | (middle_change3.children[1].value != 0) | (back_change3.children[1].value != 0) : \n",
    "\n",
    "                                    ## FRONT PAGE PROMO CHANGES\n",
    "                                    # New Product 1\n",
    "                                    if i == 1:\n",
    "                                        if change_fp > 0:\n",
    "                                            # Adding additional promotions in the required banner-weeks\n",
    "                                            for week,banner in zip(ban_week_fp_copy[ban_week_fp_copy.Product == product_fil]['Week'], ban_week_fp_copy[ban_week_fp_copy.Product == product_fil]['Banner']):\n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] + 1\n",
    "                                        elif change_fp < 0:\n",
    "                                            for week,banner in zip(ban_week_fp_copy[ban_week_fp_copy.Product == product_fil]['Week'], ban_week_fp_copy[ban_week_fp_copy.Product == product_fil]['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] - 1\n",
    "\n",
    "                                    elif i == 2:\n",
    "                                        if change_fp > 0:\n",
    "                                            # Adding additional promotions in the required banner-weeks\n",
    "                                            for week,banner in zip(ban_week_fp2['Week'], ban_week_fp2['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] + 1\n",
    "                                        elif front_change2.children[1].value < 0:\n",
    "                                            for week,banner in zip(ban_week_fp2['Week'], ban_week_fp2['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] - 1\n",
    "\n",
    "                                    ## MIDDLE PAGE PROMO CHANGES\n",
    "                                    if i == 1:\n",
    "                                        if change_mp > 0:\n",
    "                                            # Adding additional promotions in the required banner-weeks\n",
    "                                            for week,banner in zip(ban_week_mp_copy[ban_week_mp_copy.Product == product_fil]['Week'], ban_week_mp_copy[ban_week_mp_copy.Product == product_fil]['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] + 1\n",
    "                                        elif change_mp < 0:\n",
    "                                            for week,banner in zip(ban_week_mp_copy[ban_week_mp_copy.Product == product_fil]['Week'], ban_week_mp_copy[ban_week_mp_copy.Product == product_fil]['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] - 1\n",
    "                                    elif i == 2:\n",
    "                                        if change_mp > 0:\n",
    "                                            # Adding additional promotions in the required banner-weeks\n",
    "                                            for week,banner in zip(ban_week_mp2['Week'], ban_week_mp2['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] + 1\n",
    "                                        elif change_mp < 0:\n",
    "                                            for week,banner in zip(ban_week_mp2['Week'], ban_week_mp2['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] - 1\n",
    "\n",
    "                                    ## BACK PAGE PROMO CHANGES\n",
    "                                    if i == 1:\n",
    "                                        if change_bp > 0:\n",
    "                                            # Adding additional promotions in the required banner-weeks\n",
    "                                            for week,banner in zip(ban_week_bp_copy[ban_week_bp_copy.Product == product_fil]['Week'], ban_week_bp_copy[ban_week_bp_copy.Product == product_fil]['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] + 1\n",
    "                                        elif change_bp < 0:\n",
    "                                            for week,banner in zip(ban_week_bp_copy[ban_week_bp_copy.Product == product_fil]['Week'], ban_week_bp_copy[ban_week_bp_copy.Product == product_fil]['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] - 1\n",
    "                                    elif i == 2:\n",
    "                                        if change_bp > 0:\n",
    "                                            # Adding additional promotions in the required banner-weeks\n",
    "                                            for week,banner in zip(ban_week_bp2['Week'], ban_week_bp2['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] + 1\n",
    "                                        elif change_bp < 0:\n",
    "                                            for week,banner in zip(ban_week_bp2['Week'], ban_week_bp2['Banner']):                            \n",
    "                                                test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] - 1\n",
    "\n",
    "                        # New Category\n",
    "                        if i == 1:\n",
    "                            for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                if (cols == category_drop.value):                      \n",
    "                                    test_data_set[cols] = 1\n",
    "                                else:\n",
    "                                    test_data_set[cols] = 0\n",
    "                        if i == 2:\n",
    "                            for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                                if (cols == category_drop2.value):                      \n",
    "                                    test_data_set[cols] = 1\n",
    "                                else:\n",
    "                                    test_data_set[cols] = 0\n",
    "\n",
    "                        # New Pack Subtype\n",
    "                        if i == 1:\n",
    "                            for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                if (cols == pack_subtype_drop.value):                      \n",
    "                                    test_data_set[cols] = 1\n",
    "                                else:\n",
    "                                    test_data_set[cols] = 0\n",
    "                        if i == 2:\n",
    "                            for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                                if (cols == pack_subtype_drop2.value):                      \n",
    "                                    test_data_set[cols] = 1\n",
    "                                else:\n",
    "                                    test_data_set[cols] = 0\n",
    "                        # New Pack Content\n",
    "                        if i == 1:\n",
    "                            for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                if (cols == pack_content_drop.value):                      \n",
    "                                    test_data_set[cols] = 1\n",
    "                                else:\n",
    "                                    test_data_set[cols] = 0\n",
    "                        if i == 2:\n",
    "                            for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                                if (cols == pack_content_drop2.value):                      \n",
    "                                    test_data_set[cols] = 1\n",
    "                                else:\n",
    "                                    test_data_set[cols] = 0\n",
    "\n",
    "                        if product_fil == 'TCCC CORE POWER 414 ML BTTL':\n",
    "                            test_data_set = test_data_set.drop_duplicates()\n",
    "                            test_data_set_exist = test_data_set_exist.drop_duplicates()\n",
    "\n",
    "                        test_data_brand = test_data_set.copy()\n",
    "                        test_data_brand_exist = test_data_set_exist.copy()\n",
    "\n",
    "                        if i == 1 :\n",
    "                            # For New SIZE\n",
    "                            test_data_brand['SIZE_ML'] = size.value\n",
    "\n",
    "                            # For New Count\n",
    "                            test_data_brand['COUNT'] = count.value\n",
    "                        elif i == 2:\n",
    "                            # For New SIZE\n",
    "                            test_data_brand['SIZE_ML'] = size2.value\n",
    "\n",
    "                            # For New Count\n",
    "                            test_data_brand['COUNT'] = count2.value\n",
    "\n",
    "                        # Dropping unneccessary columns from test dataset\n",
    "                        test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "                        test_data_brand_exist.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                        #Creating test and train data - independent & dependent columns\n",
    "                        X_train = train_data_brand\n",
    "                        y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                        X_test = test_data_brand\n",
    "                        X_test_exist = test_data_brand_exist\n",
    "                        y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                        # Appending test data\n",
    "                        test_data_set[\"Region\"] = Region_key\n",
    "                        test_data_set[\"Test_period\"] = Test_period\n",
    "                        test_data_set['iter'] = i\n",
    "\n",
    "                        # In case the product is not present in certain region-quarter\n",
    "                        if len(test_data_set) == 0:\n",
    "                            continue\n",
    "\n",
    "                        Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "                    \n",
    "                        if((Region_key == 'EAST') and (Test_period == '2019Q1')):\n",
    "                                rf = model_object_EAST_2019Q1_v2\n",
    "                        elif((Region_key == 'EAST') and (Test_period == '2019Q2')):\n",
    "                                rf = model_object_EAST_2019Q2_v2\n",
    "                        elif((Region_key == 'EAST') and (Test_period == '2019Q3')):\n",
    "                                rf = model_object_EAST_2019Q3_v2\n",
    "                        elif((Region_key == 'EAST') and (Test_period == '2019Q4')):\n",
    "                                rf = model_object_EAST_2019Q4_v2\n",
    "                        elif((Region_key == 'ONTARIO') and (Test_period == '2019Q1')):\n",
    "                                rf = model_object_ONTARIO_2019Q1_v2\n",
    "                        elif((Region_key == 'ONTARIO') and (Test_period == '2019Q2')):\n",
    "                                rf = model_object_ONTARIO_2019Q2_v2\n",
    "                        elif((Region_key == 'ONTARIO') and (Test_period == '2019Q3')):\n",
    "                                rf = model_object_ONTARIO_2019Q3_v2\n",
    "                        elif((Region_key == 'ONTARIO') and (Test_period == '2019Q4')):\n",
    "                                rf = model_object_ONTARIO_2019Q4_v2\n",
    "                        elif((Region_key == 'QUEBEC') and (Test_period == '2019Q1')):\n",
    "                                rf = model_object_QUEBEC_2019Q1_v2\n",
    "                        elif((Region_key == 'QUEBEC') and (Test_period == '2019Q2')):\n",
    "                                rf = model_object_QUEBEC_2019Q2_v2\n",
    "                        elif((Region_key == 'QUEBEC') and (Test_period == '2019Q3')):\n",
    "                                rf = model_object_QUEBEC_2019Q3_v2\n",
    "                        elif((Region_key == 'QUEBEC') and (Test_period == '2019Q4')):\n",
    "                                rf = model_object_QUEBEC_2019Q4_v2\n",
    "                        elif((Region_key == 'WEST') and (Test_period == '2019Q1')):\n",
    "                                rf = model_object_WEST_2019Q1_v2\n",
    "                        elif((Region_key == 'WEST') and (Test_period == '2019Q2')):\n",
    "                                rf = model_object_WEST_2019Q2_v2\n",
    "                        elif((Region_key == 'WEST') and (Test_period == '2019Q3')):\n",
    "                                rf = model_object_WEST_2019Q3_v2\n",
    "                        else:\n",
    "                                rf = model_object_WEST_2019Q4_v2\n",
    "\n",
    "                        #Model predictions\n",
    "                        predictions_rf = rf.predict(X_test)                \n",
    "                        predictions_rf_exist = rf.predict(X_test_exist)                \n",
    "\n",
    "                        #Storing test results\n",
    "                        result = X_test.copy()\n",
    "                        result_exist = X_test_exist.copy()\n",
    "\n",
    "                        result[\"Predictions_rf\"] = predictions_rf\n",
    "                        result_exist[\"Predictions_rf\"] = predictions_rf_exist\n",
    "\n",
    "                        result['Predictions_rf'] = np.exp(result['Predictions_rf'])\n",
    "\n",
    "                        result[\"Actuals\"] = y_test\n",
    "                        result['Actuals'] = np.exp(result['Actuals'])                    \n",
    "\n",
    "                        #Calculating APE for the models\n",
    "                        result[\"ape_rf\"] = (result[\"Predictions_rf\"]-result[\"Actuals\"]).abs()\n",
    "\n",
    "                        result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                        result[\"Region\"] = Region_key\n",
    "                        result[\"Test_period\"] = Test_period\n",
    "                        result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                        result[\"DD_1\"] = test_data_set['DD_1']\n",
    "                        result[\"DD_2\"] = test_data_set['DD_2']\n",
    "                        result[\"Banner\"] = test_data_set['Banner']\n",
    "                        result[\"Channel\"] = test_data_set['Channel']\n",
    "                        ##Columns for Revenue                    \n",
    "                        result[\"Adcal_Price\"] = test_data_set[\"Adcal_Price\"]\n",
    "                        result['Adcal_Price'] = np.exp(result['Adcal_Price'])                    \n",
    "\n",
    "                        result[\"Adcal_Price_exist\"] = test_data_set_exist[\"Adcal_Price\"].values\n",
    "                        result['Adcal_Price_exist'] = np.exp(result['Adcal_Price_exist'])                    \n",
    "\n",
    "                        result['Predictions_rf_exist'] = predictions_rf_exist\n",
    "                        result['Predictions_rf_exist'] = np.exp(result['Predictions_rf_exist'])\n",
    "                        result['iter'] = i                                                                        \n",
    "\n",
    "                        result = result[[\"Region\",\"Product\",'Channel',\"Banner\",\"Test_period\",\"Week\",\"Actuals\",\"ape_rf\",\"Predictions_rf\",\"Predictions_rf_exist\",\"Adcal_Price_exist\",\"DD_1\",\"DD_2\",\"Adcal_Price\",'iter']]\n",
    "\n",
    "                        if i == 1:\n",
    "                            result['Volume_RU'] = result['Predictions_rf']\n",
    "                            result['Volume_RU_exist'] = result['Predictions_rf_exist']\n",
    "                        elif i == 2:\n",
    "                            result['Volume_RU'] = result['Predictions_rf']\n",
    "                            result['Volume_RU_exist'] = result['Predictions_rf_exist']\n",
    "\n",
    "                        result['Sales_derived'] = result['Predictions_rf'] * result['Adcal_Price']\n",
    "                        result['Sales_derived_exist'] = result['Predictions_rf_exist'] * result['Adcal_Price_exist']\n",
    "\n",
    "                        # Appending results to the final results dataframe            \n",
    "                        Test_results = Test_results.append(result, ignore_index=True)\n",
    "\n",
    "\n",
    "        if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach\n",
    "            if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "                clear_output()\n",
    "                print('Select 3 products as existing model pack')\n",
    "            else:\n",
    "\n",
    "                # Similarity score of each existing product\n",
    "                sim_score_p1 = score_data[score_data.Product == sorted(qtr_df_final['Existing Model Pack'].unique())[0]]['Similarity_Score'].values[0]\n",
    "                sim_score_p2 = score_data[score_data.Product == sorted(qtr_df_final['Existing Model Pack'].unique())[1]]['Similarity_Score'].values[0]\n",
    "                sim_score_p3 = score_data[score_data.Product == sorted(qtr_df_final['Existing Model Pack'].unique())[2]]['Similarity_Score'].values[0]\n",
    "\n",
    "                # Filtering test results for different products\n",
    "                Test_results1 = Test_results[Test_results.Product == sorted(qtr_df_final['Existing Model Pack'].unique())[0]]\n",
    "                Test_results2 = Test_results[Test_results.Product == sorted(qtr_df_final['Existing Model Pack'].unique())[1]]\n",
    "                Test_results3 = Test_results[Test_results.Product == sorted(qtr_df_final['Existing Model Pack'].unique())[2]]\n",
    "\n",
    "                # Renaming test results columns\n",
    "                Test_results2.rename(columns = {'Predictions_rf':'Predictions_rf2','Predictions_rf_exist':'Predictions_rf_exist2','Sales_derived':'Sales_derived2','Sales_derived_exist':'Sales_derived_exist2'}, inplace = True)\n",
    "                Test_results3.rename(columns = {'Predictions_rf':'Predictions_rf3','Predictions_rf_exist':'Predictions_rf_exist3','Sales_derived':'Sales_derived3','Sales_derived_exist':'Sales_derived_exist3'}, inplace = True)\n",
    "\n",
    "                # Merging for all 3 existing products\n",
    "                Test_results_check = Test_results1.merge(Test_results2[['Banner','Region','Channel','Week','Test_period','Predictions_rf2','Predictions_rf_exist2','Sales_derived2','Sales_derived_exist2']], how = 'outer', on = ['Region','Channel','Banner','Week','Test_period']).merge(Test_results3[['Banner','Region','Channel','Week','Test_period','Predictions_rf3','Predictions_rf_exist3','Sales_derived3','Sales_derived_exist3']], how = 'outer', on = ['Region','Channel','Banner','Week','Test_period'])\n",
    "                # New Product\n",
    "                Test_results_check['Product'] = 'New'\n",
    "\n",
    "                # Fill na as 0\n",
    "                Test_results_check = Test_results_check.fillna(0)\n",
    "\n",
    "                if toggle_manual_simscore.value == 'Manual':\n",
    "                    # Getting average predictions\n",
    "                    Test_results_check['Predictions_rf'] = (Test_results_check['Predictions_rf'] + Test_results_check['Predictions_rf2'] + Test_results_check['Predictions_rf3'])/3\n",
    "                    Test_results_check['Predictions_rf_exist'] = (Test_results_check['Predictions_rf_exist']+ Test_results_check['Predictions_rf_exist2'] + Test_results_check['Predictions_rf_exist3'])/3\n",
    "                    Test_results_check['Sales_derived'] = (Test_results_check['Sales_derived'] + Test_results_check['Sales_derived2'] + Test_results_check['Sales_derived3'])/3\n",
    "                    Test_results_check['Sales_derived_exist'] = (Test_results_check['Sales_derived_exist']  + Test_results_check['Sales_derived_exist2'] + Test_results_check['Sales_derived_exist3'] )/3\n",
    "                else:\n",
    "                    # Getting weighted average predictions\n",
    "                    Test_results_check['Predictions_rf'] = (Test_results_check['Predictions_rf'] * sim_score_p1 + Test_results_check['Predictions_rf2'] * sim_score_p2 + Test_results_check['Predictions_rf3'] * sim_score_p3)/(sim_score_p1 + sim_score_p2 + sim_score_p3)\n",
    "                    Test_results_check['Predictions_rf_exist'] = (Test_results_check['Predictions_rf_exist'] * sim_score_p1 + Test_results_check['Predictions_rf_exist2'] * sim_score_p2 + Test_results_check['Predictions_rf_exist3'] * sim_score_p3)/(sim_score_p1 + sim_score_p2 + sim_score_p3)\n",
    "                    Test_results_check['Sales_derived'] = (Test_results_check['Sales_derived'] * sim_score_p1 + Test_results_check['Sales_derived2'] * sim_score_p2 + Test_results_check['Sales_derived3'] * sim_score_p3)/(sim_score_p1 + sim_score_p2 + sim_score_p3)\n",
    "                    Test_results_check['Sales_derived_exist'] = (Test_results_check['Sales_derived_exist'] * sim_score_p1 + Test_results_check['Sales_derived_exist2'] * sim_score_p2 + Test_results_check['Sales_derived_exist3'] * sim_score_p3)/(sim_score_p1 + sim_score_p2 + sim_score_p3)\n",
    "\n",
    "                Test_results = Test_results_check[['Region', 'Product', 'Channel', 'Banner', 'Test_period', 'Week',\n",
    "                               'Actuals', 'ape_rf', 'Predictions_rf', 'Predictions_rf_exist',\n",
    "                               'Adcal_Price_exist', 'DD_1', 'DD_2', 'Adcal_Price', 'iter', 'Volume_RU',\n",
    "                               'Volume_RU_exist', 'Sales_derived', 'Sales_derived_exist']].copy()                    \n",
    "\n",
    "        clear_output()\n",
    "\n",
    "    else:\n",
    "        print(\"Product is not present in that Region/Channel for selected period\")   \n",
    "        \n",
    "        \n",
    "else:\n",
    "    print(colored('Please upload the file to proceed', 'red',attrs=['bold']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking category, pack subtype, pack content of existing product matches with new product\n",
    "cat_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['Category'].unique()[0]\n",
    "packsubtype_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['Pack.Subtype'].unique()[0]\n",
    "packcontent_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['PACK_CONTENT'].unique()[0]\n",
    "test_data_set_cr_2 = pd.DataFrame()\n",
    "test_data_set_cr_ontario = pd.DataFrame()\n",
    "test_data_set_cr_east = pd.DataFrame()\n",
    "test_data_set_cr_west = pd.DataFrame()\n",
    "test_data_set_cr_quebec = pd.DataFrame()\n",
    "\n",
    "#################################################################################################################### \n",
    "# Cannibalisation Code: Step 1 - New Product Predictions WITH UPLOADED DATA\n",
    "#################################################################################################################### \n",
    "\n",
    "# Product_fil assigned as most similar product in ensemble approach\n",
    "if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach    \n",
    "    score_data_cann = similarity_score_calculation()\n",
    "    score_data_cann = score_data_cann[score_data_cann.Product.isin(prod_ensemble_list)].sort_values('Similarity_Score', ascending = False)\n",
    "    product_fil = score_data_cann.loc[:,'Product'][:1].values[0]\n",
    "    \n",
    "# In case the user uploads the data\n",
    "if (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "\n",
    "    # Defining model train and test periods    \n",
    "    launch_date = upload_data['Week'].min()\n",
    "    Test_week_list = {\"Post_launch\" : upload_data.loc[(upload_data.Week <= upload_data.Week.max()) & (upload_data[\"Week\"]>= launch_date),\"Week\"].unique() }    \n",
    "    # For step2 : to predict during 2019\n",
    "    Test_week_list_2019 = {\"Post_launch\" : Volume_dataset_all_reg.loc[(Volume_dataset_all_reg.Week <= '2019-52') & (Volume_dataset_all_reg[\"Week\"]>= '2019-01'),\"Week\"].unique() }    \n",
    "    Test_periods = [\"Post_launch\"]\n",
    "\n",
    "    #Defining empty dataframes for model training\n",
    "    combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "    Test_results_np_cr = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "    Test_Data = pd.DataFrame()\n",
    "    complete_train_data_set_aa = pd.DataFrame()\n",
    "\n",
    "    # Model training and prediction\n",
    "    category_check_list=[]\n",
    "\n",
    "    # In case no product is present in selected region-channel\n",
    "    if len(qtr_edv_baseline) != 0 :\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Measuring Cannibalization</h2></div>'))\n",
    "        size_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['SIZE_ML'].unique()[0]\n",
    "        count_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['COUNT'].unique()[0]\n",
    "        \n",
    "        # Run for 1 iteration\n",
    "        iter_change_list = [1]\n",
    "\n",
    "        for i in iter_change_list:\n",
    "            channel_list = upload_data.Channel.unique()\n",
    "            #Model training and prediction\n",
    "            for Region_key in upload_data.Region.unique(): \n",
    "                # In case the product is not present at that region\n",
    "                if len(upload_data[(upload_data.Region == Region_key)]) == 0:\n",
    "                    continue\n",
    "\n",
    "                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "    \n",
    "                Volume_dataset_upload = upload_data.loc[(upload_data[\"Region\"] == Region_key)\n",
    "                                           &(upload_data[\"Channel\"].isin(channel_list))]\n",
    "                \n",
    "                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                Volume_dataset_upload['Pantry2'] = Volume_dataset_upload['Pantry2'].fillna(0)\n",
    "            \n",
    "                required_columns_upload = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week'] \n",
    "                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week'] \n",
    "                combined_dataset = Volume_dataset[required_columns] \n",
    "                combined_dataset_upload = Volume_dataset_upload[required_columns_upload] \n",
    "\n",
    "                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "                \n",
    "                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                \n",
    "                combined_dataset_upload[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset_upload[\"Week.Name\"]]\n",
    "                combined_dataset_upload[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset_upload[\"Week.Name\"]]                \n",
    "\n",
    "                banner_dummies = pd.get_dummies(combined_dataset.Banner) \n",
    "                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1) #sorting banner columns\n",
    "                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "                \n",
    "                # Uploaded data banner dummies\n",
    "                banner_dummies_upload = pd.get_dummies(combined_dataset_upload.Banner)                                 \n",
    "                # If all banners are not present for the uploaded data\n",
    "                c = [i for i in list(banner_dummies.columns) if i not in list(banner_dummies_upload.columns)]\n",
    "                if len(c) != 0:\n",
    "                    banner_dummies_upload[c] = 0\n",
    "                # Reordering columns                \n",
    "                banner_dummies_upload = banner_dummies_upload.reindex(sorted(banner_dummies_upload.columns), axis=1)                \n",
    "                combined_dataset_upload = pd.concat([combined_dataset_upload, banner_dummies_upload], axis=1)\n",
    "                \n",
    "                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "\n",
    "                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]            \n",
    "\n",
    "                combined_dataset.sort_values('Week',inplace = True)\n",
    "                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                combined_dataset_upload.sort_values('Week',inplace = True)\n",
    "                combined_dataset_upload = pd.merge(combined_dataset_upload,prod_stage_check_upload(),on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset_upload['Intial_weeks'] = combined_dataset_upload['Intial_weeks'].fillna('Stabilization')                \n",
    "                \n",
    "                #Adding dummies for category\n",
    "                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1) \n",
    "                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                cat_col = category_dummies.columns #need all category dummy in the test data\n",
    "                \n",
    "                #Adding dummies for category UPLOADED DDATA\n",
    "                combined_dataset_upload[cat_col] = 0\n",
    "                \n",
    "                #Adding dummies for product attribute - pack subtype\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                pack_subtypes_col = pack_subtypes_dummies.columns\n",
    "                \n",
    "                #Adding dummies for packsubtype UPLOADED DDATA\n",
    "                combined_dataset_upload[pack_subtypes_col] = 0\n",
    "                \n",
    "                # Adding pack content dummies\n",
    "                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                pack_content_col = pack_content_dummies.columns\n",
    "                \n",
    "                #Adding dummies for packsubtype UPLOADED DDATA\n",
    "                combined_dataset_upload[pack_content_col] = 0\n",
    "                \n",
    "                combined_dataset_upload.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "                combined_dataset_upload.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "                combined_dataset_upload.drop([\"Category\"], inplace=True, axis=1)\n",
    "                \n",
    "                # Product stage dummies\n",
    "                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies_upload = pd.get_dummies(combined_dataset_upload['Intial_weeks'])\n",
    "                \n",
    "                # If all product stages are not present for the uploaded data\n",
    "                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_upload.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    stage_dummies_upload[c] = 0\n",
    "                # Reordering columns                \n",
    "                stage_dummies_upload = stage_dummies_upload.reindex(sorted(stage_dummies_upload.columns), axis=1)\n",
    "                combined_dataset_upload = pd.concat([combined_dataset_upload, stage_dummies_upload], axis=1)\n",
    "                combined_dataset_upload.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                #Adding discount depth columns\n",
    "                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                combined_dataset_upload[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset_upload[\"Adcal_DD\"]]\n",
    "                combined_dataset_upload[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset_upload[\"Adcal_DD\"]]            \n",
    "\n",
    "                # Adcal Price calculation                \n",
    "                combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Adcal Price calculation\n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                combined_dataset_upload.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                # Dropping null volumes\n",
    "                complete_dataset = combined_dataset.copy()\n",
    "                complete_dataset_upload = combined_dataset_upload.copy()\n",
    "\n",
    "                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                complete_dataset_upload['Adcal_Price'] = np.log(complete_dataset_upload['Adcal_Price'])\n",
    "                \n",
    "                for Test_period in Test_periods: \n",
    "\n",
    "                    #Filtering test weeks\n",
    "                    Test_weeks = Test_week_list[Test_period]                                \n",
    "                    \n",
    "                    # Dividing train & test data\n",
    "                    complete_train_data_set = complete_dataset.loc[~complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                   # Data for new product\n",
    "                    complete_test_data_set = complete_dataset_upload.loc[complete_dataset_upload[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                    # Dropping unneccessary columns from train dataset\n",
    "                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                    train_data_brand = train_data_set.copy()\n",
    "                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                    # Filtering for existing product\n",
    "                    test_data_set = complete_test_data_set.copy()\n",
    "\n",
    "                    # New Category\n",
    "                    if i == 1:\n",
    "                        for cols in upload_data.Category.unique():                            \n",
    "                            test_data_set[cols] = 1\n",
    "                            \n",
    "                    # New Pack Subtype\n",
    "                    if i == 1:\n",
    "                        for cols in upload_data['Pack.Subtype'].unique():                            \n",
    "                            test_data_set[cols] = 1\n",
    "                    \n",
    "                    # New Pack Content\n",
    "                    if i == 1:\n",
    "                        for cols in upload_data['PACK_CONTENT'].unique():                            \n",
    "                            test_data_set[cols] = 1\n",
    "                    \n",
    "                    if product_fil == 'TCCC CORE POWER 414 ML BTTL':\n",
    "                        test_data_set = test_data_set.drop_duplicates()\n",
    "                    \n",
    "                    test_data_brand = test_data_set.copy()\n",
    "\n",
    "                    if i == 1 :\n",
    "                        # For New SIZE\n",
    "                        test_data_brand['SIZE_ML'] = upload_data.SIZE_ML.unique()[0]\n",
    "\n",
    "                        # For New Count\n",
    "                        test_data_brand['COUNT'] = upload_data.COUNT.unique()[0]\n",
    "\n",
    "                    # Dropping unneccessary columns from test dataset\n",
    "                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\"], inplace=True, axis=1)\n",
    "\n",
    "                    #Creating test and train data - independent & dependent columns\n",
    "                    X_train = train_data_brand\n",
    "                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                    X_test = test_data_brand\n",
    "\n",
    "                    # Need this dataframe to copy new product features to step 3 of cannibalization\n",
    "                    test_data_set_cr_1 = test_data_set.copy() \n",
    "                    if Region_key == 'ONTARIO':                        \n",
    "                        test_data_set_cr_1['Region'] = Region_key                                         \n",
    "                        test_data_set_cr_ontario = test_data_set_cr_ontario.append(test_data_set_cr_1, ignore_index = True)\n",
    "\n",
    "                    if Region_key == 'EAST':                        \n",
    "                        test_data_set_cr_1['Region'] = Region_key                                         \n",
    "                        test_data_set_cr_east = test_data_set_cr_east.append(test_data_set_cr_1, ignore_index = True)\n",
    "\n",
    "                    if Region_key == 'WEST':                        \n",
    "                        test_data_set_cr_1['Region'] = Region_key                                       \n",
    "                        test_data_set_cr_west = test_data_set_cr_west.append(test_data_set_cr_1, ignore_index = True)\n",
    "\n",
    "                    if Region_key == 'QUEBEC':                        \n",
    "                        test_data_set_cr_1['Region'] = Region_key                                \n",
    "                        test_data_set_cr_quebec = test_data_set_cr_quebec.append(test_data_set_cr_1, ignore_index = True)\n",
    "                    \n",
    "                    # Appending test data\n",
    "                    test_data_set[\"Region\"] = Region_key\n",
    "                    test_data_set[\"Test_period\"] = Test_period\n",
    "                    test_data_set['iter'] = i\n",
    "\n",
    "                    # In case the product is not present in certain region-quarter\n",
    "                    if len(test_data_set) == 0:\n",
    "                        continue\n",
    "\n",
    "                    #Model training\n",
    "                    rf = RandomForestRegressor(n_jobs=4,random_state=0)\n",
    "                    rf.fit(X_train, y_train)\n",
    "\n",
    "                    #Model predictions\n",
    "                    predictions_rf = rf.predict(X_test)                \n",
    "\n",
    "                    #Storing test results\n",
    "                    result = X_test.copy()\n",
    "\n",
    "                    result[\"Predictions_rf\"] = predictions_rf\n",
    "\n",
    "                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "\n",
    "                    #Calculating APE for the models\n",
    "\n",
    "                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                    result[\"Region\"] = Region_key\n",
    "                    result[\"Test_period\"] = Test_period\n",
    "                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                    result[\"DD_1\"] = test_data_set['DD_1']\n",
    "                    result[\"DD_2\"] = test_data_set['DD_2']\n",
    "                    result[\"Banner\"] = test_data_set['Banner']\n",
    "                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                    ##Columns for Revenue                    \n",
    "                    result[\"Adcal_Price\"] = test_data_set[\"Adcal_Price\"]\n",
    "                    result['Adcal_Price'] = np.exp(result['Adcal_Price'])\n",
    "\n",
    "                    result['iter'] = i            \n",
    "\n",
    "                    result = result[[\"Region\",\"Product\",'Channel',\"Banner\",\"Test_period\",\"Week\",\"Predictions_rf\",\"DD_1\",\"DD_2\",\"Adcal_Price\",'iter']]\n",
    "\n",
    "                    # Appending results to the final results dataframe            \n",
    "                    Test_results_np_cr = Test_results_np_cr.append(result, ignore_index=True)\n",
    "\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "    else:\n",
    "        print(\"Product is not present in that Region/Channel for selected period\")\n",
    "   \n",
    "\n",
    " #################################################################################################################### \n",
    "# Cannibalisation Code: Step 1 - New Product Predictions WITHOUT UPLOADED DATA\n",
    "#################################################################################################################### \n",
    "\n",
    "# In case the user doesn't upload the data\n",
    "elif (toggle_upload.value == 'No'):\n",
    "\n",
    "    # Defining model train and test periods    \n",
    "    launch_date = Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value)]['Week'].min()\n",
    "\n",
    "    if launch_date < '2019-01':\n",
    "        launch_date = '2019-01'\n",
    "\n",
    "    Test_week_list = {\n",
    "         \"Post_launch\" : Volume_dataset_all_reg.loc[(Volume_dataset_all_reg.Test_period <= end_week.children[8].value) & ((Volume_dataset_all_reg[\"Week\"]>= launch_date)),\"Week\"].unique() }\n",
    "\n",
    "    Test_periods = [\"Post_launch\"]\n",
    "\n",
    "    #Defining empty dataframes for model training\n",
    "    combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "    Test_results_np_cr = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "    Test_Data = pd.DataFrame()\n",
    "    complete_train_data_set_aa = pd.DataFrame()\n",
    "\n",
    "    # Model training and prediction\n",
    "    category_check_list=[]\n",
    "\n",
    "    # In case no product is present in selected region-channel\n",
    "    if len(qtr_edv_baseline) != 0 :\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Measuring Cannibalization</h2></div>'))\n",
    "        size_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['SIZE_ML'].unique()[0]\n",
    "        count_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['COUNT'].unique()[0]\n",
    "\n",
    "        if toggle_np2.value == 'Yes':\n",
    "            iter_change_list = [1,2]\n",
    "        else:\n",
    "            iter_change_list = [1]\n",
    "\n",
    "        for i in iter_change_list:        \n",
    "\n",
    "            # Model training and prediction\n",
    "            for Region_key in Region_List:        \n",
    "\n",
    "                # In case the product is not present at that region\n",
    "                if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                    continue\n",
    "\n",
    "                category_check_list.extend(list(Volume_dataset_all_reg[(Volume_dataset_all_reg[\"Region\"] == Region_key) & (Volume_dataset_all_reg[\"Product\"] == product_fil)].Category.unique()))\n",
    "\n",
    "                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                # Uploaded data banner dummies\n",
    "                banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                # If all banners are not present for the training data\n",
    "                c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                if len(c) != 0:\n",
    "                    banner_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                combined_dataset.sort_values('Week',inplace = True)\n",
    "                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                #Adding dummies for category\n",
    "                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                # Adding dummies for product attribute - Category\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                # Uploaded data category dummies\n",
    "                category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                # If all pack subtypes are not present for the training data\n",
    "                c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                if len(c) != 0:\n",
    "                    category_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                # Adding dummies for product attribute - pack subtype\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                # Uploaded data pack subtype dummies\n",
    "                pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                # If all pack subtypes are not present for the training data\n",
    "                c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                if len(c) != 0:\n",
    "                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                # Adding pack content dummies\n",
    "                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                # Uploaded data pack content dummies\n",
    "                pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                # If all pack contents are not present for the training data\n",
    "                c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                if len(c) != 0:\n",
    "                    pack_content_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                combined_dataset_test = combined_dataset.copy()\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # Changing product stage as per start date provided by the user\n",
    "                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                # Getting updated product stages\n",
    "                combined_dataset_test_check = combined_dataset_test[combined_dataset_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                combined_dataset_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies_test = pd.get_dummies(combined_dataset_test['Intial_weeks'])\n",
    "                \n",
    "                # If all product stages are not present for the uploaded data\n",
    "                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    stage_dummies_test[c] = 0\n",
    "                    \n",
    "                # Reordering columns                \n",
    "                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                combined_dataset_test = pd.concat([combined_dataset_test, stage_dummies_test], axis=1)\n",
    "                combined_dataset_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # In case the user doesn't want to upload adcal data\n",
    "                if toggle_upload.value != 'Yes':\n",
    "\n",
    "                    # CHANGES : For adcal discounts\n",
    "                    if 'All' in list(test_period.value):                \n",
    "                        # Discount change\n",
    "                        if i == 1:                \n",
    "                            combined_dataset_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test['Adcal_DD']\n",
    "                            combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test.loc[:,'EDV.Price']\n",
    "                        elif i == 2:\n",
    "                            combined_dataset_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test['Adcal_DD']\n",
    "                            combined_dataset_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test.loc[:,'EDV.Price']          \n",
    "                    else:                \n",
    "                        if i == 1:                \n",
    "                            # Discount change\n",
    "                            combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil)]['Adcal_DD'] \n",
    "                            # Baseline pricing\n",
    "                            combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                        elif i == 2:\n",
    "                            # Discount change\n",
    "                            combined_dataset_test.loc[combined_dataset_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_test[combined_dataset_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                            # Baseline pricing\n",
    "                            combined_dataset_test.loc[(combined_dataset_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_test[(combined_dataset_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "                #Adding discount depth columns\n",
    "                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                combined_dataset_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset_test[\"Adcal_DD\"]]\n",
    "                combined_dataset_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                # Adcal Price calculation\n",
    "                # In case the discounts go over 100%, limit to 100%\n",
    "                combined_dataset_test.loc[combined_dataset_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "                \n",
    "                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_test.loc[combined_dataset_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_test.loc[combined_dataset_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Adcal Price calculation\n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                combined_dataset_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                # Dropping null volumes\n",
    "                complete_dataset = combined_dataset.copy()\n",
    "                complete_dataset_test = combined_dataset_test.copy()\n",
    "\n",
    "                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "                complete_dataset_test = complete_dataset_test.loc[complete_dataset_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "\n",
    "                complete_dataset_test['Adcal_Price'] = np.log(complete_dataset_test['Adcal_Price'])\n",
    "                complete_dataset_test['Eq.Unit.Sales'] = np.log(complete_dataset_test['Eq.Unit.Sales'])\n",
    "\n",
    "                # TEST PERIOD LOOP            \n",
    "                for Test_period in Test_periods: \n",
    "\n",
    "                    #Filtering test weeks\n",
    "                    Test_weeks = Test_week_list[Test_period]                                \n",
    "\n",
    "                    # Dividing train & test data                    \n",
    "                    complete_train_data_set = complete_dataset.loc[~(complete_dataset[\"Week\"].isin(Test_weeks))].reset_index(drop=True)\n",
    "\n",
    "                    # Data for new product\n",
    "                    complete_test_data_set = complete_dataset_test.loc[complete_dataset_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                    # Dropping unneccessary columns from train dataset\n",
    "                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                    train_data_brand = train_data_set.copy()\n",
    "                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                    # Filtering for existing product\n",
    "                    test_data_set = complete_test_data_set[complete_test_data_set.Product == product_fil].copy()\n",
    "                    # In case the user doesn't want to upload adcal data\n",
    "                    if toggle_upload.value != 'Yes':\n",
    "\n",
    "                        #################### INCREASING/DECREASING PROMOTIONS FOR BANNER WEEKS COMBINATIONS ####################\n",
    "\n",
    "                        if (promo_change_toggle.value == 'Yes (All Banners)') | (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "                            if (front_change.children[1].value != 0) | (middle_change.children[1].value != 0) | (back_change.children[1].value != 0) | (front_change2.children[1].value != 0) | (middle_change2.children[1].value != 0) | (back_change2.children[1].value != 0) | (front_change3.children[1].value != 0) | (middle_change3.children[1].value != 0) | (back_change3.children[1].value != 0) : \n",
    "\n",
    "                                ## FRONT PAGE PROMO CHANGES\n",
    "                                # New Product 1\n",
    "                                if i == 1:\n",
    "                                    if front_change.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_fp['Week'], ban_week_fp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] + 1\n",
    "                                    elif front_change.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_fp['Week'], ban_week_fp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] - 1\n",
    "\n",
    "                                elif i == 2:\n",
    "                                    if front_change2.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_fp2['Week'], ban_week_fp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] + 1\n",
    "                                    elif front_change2.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_fp2['Week'], ban_week_fp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] - 1\n",
    "\n",
    "                                ## MIDDLE PAGE PROMO CHANGES\n",
    "                                if i == 1:\n",
    "                                    if middle_change.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_mp['Week'], ban_week_mp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] + 1\n",
    "                                    elif middle_change.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_mp['Week'], ban_week_mp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] - 1\n",
    "                                elif i == 2:\n",
    "                                    if middle_change2.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_mp2['Week'], ban_week_mp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] + 1\n",
    "                                    elif middle_change2.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_mp2['Week'], ban_week_mp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] - 1\n",
    "\n",
    "                                ## BACK PAGE PROMO CHANGES\n",
    "                                if i == 1:\n",
    "                                    if back_change.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_bp['Week'], ban_week_bp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] + 1\n",
    "                                    elif back_change.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_bp['Week'], ban_week_bp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] - 1\n",
    "                                elif i == 2:\n",
    "                                    if back_change2.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_bp2['Week'], ban_week_bp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] + 1\n",
    "                                    elif back_change2.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_bp2['Week'], ban_week_bp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] - 1\n",
    "\n",
    "                    # New Category\n",
    "                    if i == 1:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                            if (cols == category_drop.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                    if i == 2:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                            if (cols == category_drop2.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "\n",
    "                    # New Pack Subtype\n",
    "                    if i == 1:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                            if (cols == pack_subtype_drop.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                    if i == 2:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                            if (cols == pack_subtype_drop2.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                    # New Pack Content\n",
    "                    if i == 1:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                            if (cols == pack_content_drop.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                    if i == 2:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                            if (cols == pack_content_drop2.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "\n",
    "                    if product_fil == 'TCCC CORE POWER 414 ML BTTL':\n",
    "                        test_data_set = test_data_set.drop_duplicates()\n",
    "                                \n",
    "                    test_data_brand = test_data_set.copy()                \n",
    "\n",
    "                    if i == 1 :\n",
    "                        # For New SIZE\n",
    "                        test_data_brand['SIZE_ML'] = size.value\n",
    "\n",
    "                        # For New Count\n",
    "                        test_data_brand['COUNT'] = count.value\n",
    "                    elif i == 2:\n",
    "                        # For New SIZE\n",
    "                        test_data_brand['SIZE_ML'] = size2.value\n",
    "\n",
    "                        # For New Count\n",
    "                        test_data_brand['COUNT'] = count2.value\n",
    "\n",
    "                    # Dropping unneccessary columns from test dataset\n",
    "                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                    #Creating test and train data - independent & dependent columns\n",
    "                    X_train = train_data_brand\n",
    "                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                    X_test = test_data_brand\n",
    "                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "                                        \n",
    "                    # Need this dataframe to copy new product features to step 3 of cannibalization\n",
    "                    test_data_set_cr_1 = test_data_set.copy() \n",
    "                    if Region_key == 'ONTARIO':                        \n",
    "                        test_data_set_cr_1['Region'] = Region_key\n",
    "                        test_data_set_cr_1['iter'] = i                   \n",
    "                        test_data_set_cr_ontario = test_data_set_cr_ontario.append(test_data_set_cr_1, ignore_index = True)\n",
    "\n",
    "                    if Region_key == 'EAST':                        \n",
    "                        test_data_set_cr_1['Region'] = Region_key\n",
    "                        test_data_set_cr_1['iter'] = i                   \n",
    "                        test_data_set_cr_east = test_data_set_cr_east.append(test_data_set_cr_1, ignore_index = True)\n",
    "\n",
    "                    if Region_key == 'WEST':                        \n",
    "                        test_data_set_cr_1['Region'] = Region_key\n",
    "                        test_data_set_cr_1['iter'] = i                   \n",
    "                        test_data_set_cr_west = test_data_set_cr_west.append(test_data_set_cr_1, ignore_index = True)\n",
    "\n",
    "                    if Region_key == 'QUEBEC':                        \n",
    "                        test_data_set_cr_1['Region'] = Region_key\n",
    "                        test_data_set_cr_1['iter'] = i                   \n",
    "                        test_data_set_cr_quebec = test_data_set_cr_quebec.append(test_data_set_cr_1, ignore_index = True)\n",
    "                        \n",
    "                    # Appending test data\n",
    "                    test_data_set[\"Region\"] = Region_key\n",
    "                    test_data_set[\"Test_period\"] = Test_period\n",
    "                    test_data_set['iter'] = i\n",
    "\n",
    "                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "                    # In case the product is not present in certain region-quarter\n",
    "                    if len(test_data_set) == 0:\n",
    "                        continue\n",
    "\n",
    "                    #Model training\n",
    "                    rf = RandomForestRegressor(n_jobs=4,random_state=0)\n",
    "                    rf.fit(X_train, y_train)\n",
    "\n",
    "                    #Model predictions\n",
    "                    predictions_rf = rf.predict(X_test)                \n",
    "\n",
    "                    #Storing test results\n",
    "                    result = X_test.copy()\n",
    "\n",
    "                    result[\"Predictions_rf\"] = predictions_rf\n",
    "                    result[\"Actuals\"] = y_test\n",
    "                    \n",
    "                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                    result['Actuals'] = np.exp(result['Actuals'])                    \n",
    "\n",
    "                    #Calculating APE for the models\n",
    "                    result[\"ape_rf\"] = (result[\"Predictions_rf\"]-result[\"Actuals\"]).abs()\n",
    "\n",
    "                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                    result[\"Region\"] = Region_key\n",
    "                    result[\"Test_period\"] = Test_period\n",
    "                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                    result[\"DD_1\"] = test_data_set['DD_1']\n",
    "                    result[\"DD_2\"] = test_data_set['DD_2']\n",
    "                    result[\"Banner\"] = test_data_set['Banner']\n",
    "                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                    ##Columns for Revenue                    \n",
    "                    result[\"Adcal_Price\"] = test_data_set[\"Adcal_Price\"]\n",
    "                    result[\"Adcal_Price\"] = np.exp(result[\"Adcal_Price\"])\n",
    "                    \n",
    "                    result['iter'] = i            \n",
    "\n",
    "                    result = result[[\"Region\",\"Product\",'Channel',\"Banner\",\"Test_period\",\"Week\",\"Actuals\",\"ape_rf\",\"Predictions_rf\",\"DD_1\",\"DD_2\",\"Adcal_Price\",'iter']]\n",
    "\n",
    "                    # Appending results to the final results dataframe            \n",
    "                    Test_results_np_cr = Test_results_np_cr.append(result, ignore_index=True)\n",
    "\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "    else:\n",
    "        print(\"Product is not present in that Region/Channel for selected period\")\n",
    "\n",
    "#################################################################################################################### \n",
    "# Cannibalisation Code: Step 2 - # Existing Product predictions without NP\n",
    "#################################################################################################################### \n",
    "\n",
    "#Defining empty dataframes for model training\n",
    "combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "Test_results_exist_wo_cr = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "Test_Data = pd.DataFrame()\n",
    "\n",
    "#Model training and prediction\n",
    "category_check_list=[]\n",
    "\n",
    "# In case no product is present in selected region-channel\n",
    "if len(qtr_edv_baseline) != 0 :\n",
    "    display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Measuring Cannibalization</h2></div>'))\n",
    "    size_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['SIZE_ML'].unique()[0]\n",
    "    count_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['COUNT'].unique()[0]\n",
    "\n",
    "    if (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "        Region_List = upload_data.Region.unique()\n",
    "        channel_list = upload_data.Channel.unique()\n",
    "    \n",
    "    # Model training and prediction\n",
    "    for Region_key in Region_List:        \n",
    "        # In case the product is not present at that region\n",
    "        if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "            continue\n",
    "\n",
    "        category_check_list.extend(list(Volume_dataset_all_reg[(Volume_dataset_all_reg[\"Region\"] == Region_key) & (Volume_dataset_all_reg[\"Product\"] == product_fil)].Category.unique()))\n",
    "\n",
    "        Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                   &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "        Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "        if (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week'] \n",
    "        elif (toggle_upload.value == 'No'): # Including number of brand, flavor, sweetener, type columns\n",
    "            required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "        combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "        combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                 (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "        combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "        combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "        ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "        banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "        # Uploaded data banner dummies\n",
    "        banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "        # If all banners are not present for the training data\n",
    "        c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "        if len(c) != 0:\n",
    "            banner_dummies[c] = 0\n",
    "\n",
    "        # Reordering columns                \n",
    "        banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "        combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "        ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "        combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "        combined_dataset.sort_values('Week',inplace = True)\n",
    "        combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "        combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "        combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "        #Adding dummies for category\n",
    "        category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "        # Adding dummies for product attribute - Category\n",
    "        pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "        # Uploaded data category dummies\n",
    "        category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "        # If all pack subtypes are not present for the training data\n",
    "        c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "        if len(c) != 0:\n",
    "            category_dummies[c] = 0\n",
    "\n",
    "        # Reordering columns                \n",
    "        category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "        combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "        combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "        # Adding dummies for product attribute - pack subtype\n",
    "        pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "        # Uploaded data pack subtype dummies\n",
    "        pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "        # If all pack subtypes are not present for the training data\n",
    "        c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "        if len(c) != 0:\n",
    "            pack_subtypes_dummies[c] = 0\n",
    "\n",
    "        # Reordering columns                \n",
    "        pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "        combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "        combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "        # Adding pack content dummies\n",
    "        pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "        # Uploaded data pack content dummies\n",
    "        pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "        # If all pack contents are not present for the training data\n",
    "        c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "        if len(c) != 0:\n",
    "            pack_content_dummies[c] = 0\n",
    "\n",
    "        # Reordering columns                \n",
    "        pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "        combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "        combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "        # Product stage dummies\n",
    "        stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "        combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "        combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "        #Adding discount depth columns\n",
    "        combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                              else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "        combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                         else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "        # Adcal Price calculation\n",
    "        combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "        # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "        combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "        # Dropping adcal DD as DD1 & DD2 are added\n",
    "        combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "        # Dropping null volumes\n",
    "        complete_dataset = combined_dataset.copy()        \n",
    "\n",
    "        complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]        \n",
    "\n",
    "        complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "        complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "        \n",
    "        # TEST PERIOD LOOP\n",
    "        for Test_period in Test_periods:\n",
    "\n",
    "            if launch_date < Max_POS:\n",
    "                Test_weeks = Test_week_list[Test_period]\n",
    "            else:\n",
    "                Test_weeks = Test_week_list_2019[Test_period] # 2019 weeks as test data\n",
    "                \n",
    "            # Dividing train & test data\n",
    "            complete_train_data_set = complete_dataset.loc[(complete_dataset.Week <= max(Test_weeks)) & ~(complete_dataset[\"Week\"].isin(Test_weeks))].reset_index(drop=True)            \n",
    "            complete_test_data_set = complete_dataset.loc[complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "            # Dropping unneccessary columns from train dataset\n",
    "            train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "            train_data_brand = train_data_set.copy()\n",
    "            train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "          \n",
    "            test_data_set = complete_test_data_set.copy()\n",
    "            test_data_brand = test_data_set.copy()            \n",
    "\n",
    "            # Dropping unneccessary columns from test dataset\n",
    "            test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)            \n",
    "\n",
    "            #Creating test and train data - independent & dependent columns\n",
    "            X_train = train_data_brand\n",
    "            y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "            X_test = test_data_brand     \n",
    "            y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "            # Appending test data\n",
    "            test_data_set[\"Region\"] = Region_key\n",
    "            test_data_set[\"Test_period\"] = Test_period\n",
    "            test_data_set['iter'] = i\n",
    "\n",
    "            Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "            # In case the product is not present in certain region-quarter\n",
    "            if len(test_data_set) == 0:\n",
    "                continue\n",
    "\n",
    "            #Model training\n",
    "            rf = RandomForestRegressor(n_jobs=4,random_state=0)\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            #Model predictions\n",
    "            predictions_rf = rf.predict(X_test)                       \n",
    "\n",
    "            #Storing test results\n",
    "            result = X_test.copy()            \n",
    "\n",
    "            result[\"Predictions_rf\"] = predictions_rf            \n",
    "            result[\"Actuals\"] = y_test\n",
    "\n",
    "            result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "            result['Actuals'] = np.exp(result['Actuals'])                    \n",
    "            \n",
    "            #Calculating APE for the models\n",
    "            result[\"ape_rf\"] = (result[\"Predictions_rf\"]-result[\"Actuals\"]).abs()\n",
    "\n",
    "            result[\"Product\"] = test_data_set[\"Product\"]\n",
    "            result[\"Region\"] = Region_key\n",
    "            result[\"Test_period\"] = Test_period\n",
    "            result[\"Week\"] = test_data_set[\"Week\"]\n",
    "            result[\"DD_1\"] = test_data_set['DD_1']\n",
    "            result[\"DD_2\"] = test_data_set['DD_2']\n",
    "            result[\"Banner\"] = test_data_set['Banner']\n",
    "            result[\"Channel\"] = test_data_set['Channel']\n",
    "            ##Columns for Revenue            \n",
    "            result[\"Adcal_Price\"] = test_data_set[\"Adcal_Price\"]                               \n",
    "            result[\"Adcal_Price\"] = np.exp(result[\"Adcal_Price\"])\n",
    "            \n",
    "            result = result[[\"Region\",\"Product\",'Channel',\"Banner\",\"Test_period\",\"Week\",\"Actuals\",\"ape_rf\",\"Predictions_rf\",\"DD_1\",\"DD_2\",\"Adcal_Price\"]]\n",
    "\n",
    "            # Appending results to the final results dataframe            \n",
    "            Test_results_exist_wo_cr = Test_results_exist_wo_cr.append(result, ignore_index=True)            \n",
    "\n",
    "    clear_output()\n",
    "\n",
    "else:\n",
    "    print(\"Product is not present in that Region/Channel for selected period\")\n",
    "\n",
    "    \n",
    "#################################################################################################################### \n",
    "# Cannibalisation Code: Step 3 - # Exist product with NP (WITH UPLOADED DATA)\n",
    "#################################################################################################################### \n",
    "\n",
    "# In case the user uploads the data\n",
    "if (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "\n",
    "    #Defining empty dataframes for model training\n",
    "    combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "    Test_results_exist_with_cr = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "    Test_Data = pd.DataFrame()\n",
    "\n",
    "    #Model training and prediction\n",
    "    category_check_list=[]\n",
    "\n",
    "    # In case no product is present in selected region-channel\n",
    "    if len(qtr_edv_baseline) != 0 :\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Measuring Cannibalization</h2></div>'))\n",
    "        size_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['SIZE_ML'].unique()[0]\n",
    "        count_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['COUNT'].unique()[0]\n",
    "\n",
    "        iter_change_list = [1]\n",
    "\n",
    "        for i in iter_change_list:\n",
    "            channel_list = upload_data.Channel.unique()\n",
    "\n",
    "            # Model training and prediction\n",
    "            for Region_key in upload_data.Region.unique():                        \n",
    "                # In case the product is not present at that region\n",
    "                if len(upload_data[(upload_data.Region == Region_key)]) == 0:\n",
    "                    continue\n",
    "\n",
    "                category_check_list.extend(list(Volume_dataset_all_reg[(Volume_dataset_all_reg[\"Region\"] == Region_key) & (Volume_dataset_all_reg[\"Product\"] == product_fil)].Category.unique()))\n",
    "\n",
    "                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "\n",
    "                Volume_dataset_upload = upload_data.loc[(upload_data[\"Region\"] == Region_key)\n",
    "                                           &(upload_data[\"Channel\"].isin(channel_list))]\n",
    "\n",
    "                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                Volume_dataset_upload['Pantry2'] = Volume_dataset_upload['Pantry2'].fillna(0)\n",
    "\n",
    "                required_columns_upload = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week'] \n",
    "                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week'] \n",
    "                combined_dataset = Volume_dataset[required_columns] \n",
    "                combined_dataset_upload = Volume_dataset_upload[required_columns_upload] \n",
    "\n",
    "                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                combined_dataset_upload[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset_upload[\"Week.Name\"]]\n",
    "                combined_dataset_upload[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset_upload[\"Week.Name\"]]                \n",
    "\n",
    "                banner_dummies = pd.get_dummies(combined_dataset.Banner) \n",
    "                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1) #sorting banner columns\n",
    "                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                # Uploaded data banner dummies\n",
    "                banner_dummies_upload = pd.get_dummies(combined_dataset_upload.Banner)                                 \n",
    "                # If all banners are not present for the uploaded data\n",
    "                c = [i for i in list(banner_dummies.columns) if i not in list(banner_dummies_upload.columns)]\n",
    "                if len(c) != 0:\n",
    "                    banner_dummies_upload[c] = 0\n",
    "                # Reordering columns                \n",
    "                banner_dummies_upload = banner_dummies_upload.reindex(sorted(banner_dummies_upload.columns), axis=1)                \n",
    "                combined_dataset_upload = pd.concat([combined_dataset_upload, banner_dummies_upload], axis=1)\n",
    "\n",
    "                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "\n",
    "                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                combined_dataset.sort_values('Week',inplace = True)\n",
    "                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                combined_dataset_upload.sort_values('Week',inplace = True)\n",
    "                combined_dataset_upload = pd.merge(combined_dataset_upload,prod_stage_check_upload(),on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset_upload['Intial_weeks'] = combined_dataset_upload['Intial_weeks'].fillna('Stabilization')                \n",
    "\n",
    "                #Adding dummies for category\n",
    "                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1) \n",
    "                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                cat_col = category_dummies.columns #need all category dummy in the test data\n",
    "\n",
    "                #Adding dummies for category UPLOADED DDATA\n",
    "                combined_dataset_upload[cat_col] = 0\n",
    "\n",
    "                #Adding dummies for product attribute - pack subtype\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                pack_subtypes_col = pack_subtypes_dummies.columns\n",
    "\n",
    "                #Adding dummies for packsubtype UPLOADED DDATA\n",
    "                combined_dataset_upload[pack_subtypes_col] = 0\n",
    "\n",
    "                # Adding pack content dummies\n",
    "                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                pack_content_col = pack_content_dummies.columns\n",
    "\n",
    "                #Adding dummies for packsubtype UPLOADED DDATA\n",
    "                combined_dataset_upload[pack_content_col] = 0\n",
    "\n",
    "                combined_dataset_upload.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "                combined_dataset_upload.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "                combined_dataset_upload.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies_upload = pd.get_dummies(combined_dataset_upload['Intial_weeks'])\n",
    "\n",
    "                # If all product stages are not present for the uploaded data\n",
    "                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_upload.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    stage_dummies_upload[c] = 0\n",
    "                # Reordering columns                \n",
    "                stage_dummies_upload = stage_dummies_upload.reindex(sorted(stage_dummies_upload.columns), axis=1)\n",
    "                combined_dataset_upload = pd.concat([combined_dataset_upload, stage_dummies_upload], axis=1)\n",
    "                combined_dataset_upload.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                #Adding discount depth columns\n",
    "                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "\n",
    "                combined_dataset_upload[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset_upload[\"Adcal_DD\"]]\n",
    "                combined_dataset_upload[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset_upload[\"Adcal_DD\"]]            \n",
    "\n",
    "                # Adcal Price calculation\n",
    "                combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_upload.loc[combined_dataset_upload['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Adcal Price calculation\n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                combined_dataset_upload.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                # Dropping null volumes\n",
    "                complete_dataset = combined_dataset.copy()\n",
    "                complete_dataset_upload = combined_dataset_upload.copy()\n",
    "\n",
    "                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                # Get new product prediction for selected region\n",
    "                if Region_key == 'ONTARIO':                    \n",
    "                    complete_dataset_prod = test_data_set_cr_ontario.copy() \n",
    "                    complete_dataset_prod['Adcal_Price'] = np.exp(complete_dataset_prod['Adcal_Price'])                    \n",
    "\n",
    "                if Region_key == 'WEST':                    \n",
    "                    complete_dataset_prod = test_data_set_cr_west.copy() \n",
    "                    complete_dataset_prod['Adcal_Price'] = np.exp(complete_dataset_prod['Adcal_Price'])                   \n",
    "\n",
    "                if Region_key == 'QUEBEC':                    \n",
    "                    complete_dataset_prod = test_data_set_cr_quebec.copy() \n",
    "                    complete_dataset_prod['Adcal_Price'] = np.exp(complete_dataset_prod['Adcal_Price'])                    \n",
    "\n",
    "                if Region_key == 'EAST':                    \n",
    "                    complete_dataset_prod = test_data_set_cr_east.copy() \n",
    "                    complete_dataset_prod['Adcal_Price'] = np.exp(complete_dataset_prod['Adcal_Price'])                   \n",
    "                    \n",
    "                # Dropping columns as they are not required in complete dataset\n",
    "                complete_dataset_prod = complete_dataset_prod.drop(['Region'],axis = 1)\n",
    "                    \n",
    "                # Adding predicted data of new product in training            \n",
    "                Test_results_np_cr_v2 = Test_results_np_cr[(Test_results_np_cr[\"Region\"]==Region_key) & (Test_results_np_cr[\"iter\"]==i)]            \n",
    "                # Merge data to replace eq.unit,sales with predictions for new product\n",
    "                complete_dataset2= pd.merge(Test_results_np_cr_v2[['Week','Product','Channel','Banner','Predictions_rf']], complete_dataset_prod[complete_dataset_prod.Week <= max(Test_weeks)], \n",
    "                                   left_on=['Week','Product','Channel','Banner'], \n",
    "                                   right_on=['Week','Product','Channel','Banner'],how='outer') \n",
    "\n",
    "                complete_dataset2['Product'] = 'New'\n",
    "\n",
    "                # Replacing eq unit sales with new product prediction # Week filter given for 2017-2018 products\n",
    "                complete_dataset_prod['Eq.Unit.Sales'] = complete_dataset2.loc[(complete_dataset2.Product == 'New') & (complete_dataset2.Week >= launch_date), 'Predictions_rf'].values\n",
    "                complete_dataset_prod['Product'] = 'New'\n",
    "\n",
    "                complete_dataset = complete_dataset.append(complete_dataset_prod, ignore_index = True)\n",
    "\n",
    "                complete_dataset['Eq.Unit.Sales'] = complete_dataset['Eq.Unit.Sales'].round(4)\n",
    "\n",
    "                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "                \n",
    "                # TEST PERIOD LOOP\n",
    "                for Test_period in Test_periods:\n",
    "\n",
    "                    #Filtering test weeks\n",
    "                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                    if launch_date < Max_POS:\n",
    "                        Test_weeks = Test_week_list[Test_period]\n",
    "                    else:\n",
    "                        Test_weeks = Test_week_list_2019[Test_period] # 2019 weeks as test data\n",
    "                    \n",
    "                    # Dividing train & test data\n",
    "                    complete_train_data_set = complete_dataset.loc[(complete_dataset.Week <= max(Test_weeks)) & ~(complete_dataset[\"Week\"].isin(Test_weeks))].reset_index(drop=True)\n",
    "                    # Including new product in training\n",
    "                    complete_train_data_set = complete_train_data_set.append(complete_dataset[complete_dataset.Product == 'New'], ignore_index = True)\n",
    "\n",
    "                    # Test data\n",
    "                    complete_test_data_set = complete_dataset.loc[complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                    complete_test_data_set = complete_test_data_set.append(complete_dataset[complete_dataset.Product == 'New'], ignore_index = True)\n",
    "\n",
    "                    # Dropping unneccessary columns from train dataset\n",
    "                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                    train_data_brand = train_data_set.copy()\n",
    "                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                    test_data_set = complete_test_data_set.copy()\n",
    "                    test_data_brand = test_data_set.copy()\n",
    "\n",
    "                    # Dropping unneccessary columns from test dataset\n",
    "                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)            \n",
    "                    \n",
    "                    #Creating test and train data - independent & dependent columns\n",
    "                    X_train = train_data_brand\n",
    "                    X_train['Adcal_Price'].fillna(X_train['Adcal_Price'].max(), inplace = True)\n",
    "                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                    X_test = test_data_brand  \n",
    "                    X_test['Adcal_Price'].fillna(X_test['Adcal_Price'].max(), inplace = True)\n",
    "                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                    # Appending test data\n",
    "                    test_data_set[\"Region\"] = Region_key\n",
    "                    test_data_set[\"Test_period\"] = Test_period\n",
    "                    test_data_set['iter'] = i\n",
    "                    # In case the product is not present in certain region-quarter\n",
    "                    if len(test_data_set) == 0:\n",
    "                        continue\n",
    "\n",
    "                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)                \n",
    "                    #Model training\n",
    "                    rf = RandomForestRegressor(n_jobs=4,random_state=0)\n",
    "                    rf.fit(X_train, y_train)\n",
    "\n",
    "                    #Model predictions\n",
    "                    predictions_rf = rf.predict(X_test)                       \n",
    "\n",
    "                    #Storing test results\n",
    "                    result = X_test.copy()            \n",
    "\n",
    "                    result[\"Predictions_rf\"] = predictions_rf            \n",
    "                    result[\"Actuals\"] = y_test\n",
    "                    \n",
    "                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                    result['Actuals'] = np.exp(result['Actuals'])                                        \n",
    "\n",
    "                    #Calculating APE for the models\n",
    "                    result[\"ape_rf\"] = (result[\"Predictions_rf\"]-result[\"Actuals\"]).abs()\n",
    "\n",
    "                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                    result[\"Region\"] = Region_key\n",
    "                    result[\"Test_period\"] = Test_period\n",
    "                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                    result[\"DD_1\"] = test_data_set['DD_1']\n",
    "                    result[\"DD_2\"] = test_data_set['DD_2']\n",
    "                    result[\"Banner\"] = test_data_set['Banner']\n",
    "                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                    ##Columns for Revenue                    \n",
    "                    result[\"Adcal_Price\"] = test_data_set[\"Adcal_Price\"] \n",
    "                    result[\"Adcal_Price\"] = np.exp(result[\"Adcal_Price\"])\n",
    "                    \n",
    "                    result['iter'] = i\n",
    "\n",
    "                    result = result[[\"Region\",\"Product\",'Channel','iter',\"Banner\",\"Test_period\",\"Week\",\"Actuals\",\"ape_rf\",\"Predictions_rf\",\"DD_1\",\"DD_2\",\"Adcal_Price\"]]\n",
    "\n",
    "                    # Appending results to the final results dataframe            \n",
    "                    Test_results_exist_with_cr = Test_results_exist_with_cr.append(result, ignore_index=True)\n",
    "\n",
    "# In case the user doesn't uplaod data\n",
    "elif (toggle_upload.value == 'No') :\n",
    "    \n",
    "    #################################################################################################################### \n",
    "                    # Cannibalisation Code: Step 3 - # Exist product with NP (WITHOUT UPLOADED DATA)\n",
    "    #################################################################################################################### \n",
    "\n",
    "    #Defining empty dataframes for model training\n",
    "    combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "    Test_results_exist_with_cr = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "    Test_Data = pd.DataFrame()\n",
    "\n",
    "    #Model training and prediction\n",
    "    category_check_list=[]\n",
    "\n",
    "    # In case no product is present in selected region-channel\n",
    "    if len(qtr_edv_baseline) != 0 :\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Measuring Cannibalization</h2></div>'))\n",
    "    #     display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Simulating New Product Results</h2></div>'))\n",
    "        size_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['SIZE_ML'].unique()[0]\n",
    "        count_exist = Volume_dataset_all_reg[Volume_dataset_all_reg.Product == product_fil]['COUNT'].unique()[0]\n",
    "\n",
    "        if toggle_np2.value == 'Yes':\n",
    "            iter_change_list = [1,2]\n",
    "        else:\n",
    "            iter_change_list = [1]\n",
    "\n",
    "        for i in iter_change_list:\n",
    "\n",
    "\n",
    "            # Model training and prediction\n",
    "            for Region_key in Region_List:                 \n",
    "                # In case the product is not present at that region\n",
    "                if len(Volume_dataset_all_reg[(Volume_dataset_all_reg.Product == product_fil) & (Volume_dataset_all_reg.Region == Region_key)]) == 0:\n",
    "                    continue\n",
    "\n",
    "                category_check_list.extend(list(Volume_dataset_all_reg[(Volume_dataset_all_reg[\"Region\"] == Region_key) & (Volume_dataset_all_reg[\"Product\"] == product_fil)].Category.unique()))\n",
    "\n",
    "                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                required_columns = ['Week','Week.Name','Banner','Product','Channel','Pack.Subtype','PACK_CONTENT','Adcal_Price','EDV.Price','Adcal_DD','Eq.Unit.Sales','Front.Page','Middle.Page','Back.Page','seasonality_index','SIZE_ML','COUNT','No_of_brands','No_of_flavors','No_of_sweetners','No_of_types','Category','Pantry1','Pantry2','Holiday.Week','Pre.Holiday.Week','Post.Holiday.Week']\n",
    "                combined_dataset = Volume_dataset[required_columns]\n",
    "\n",
    "                combined_dataset = combined_dataset.loc[((combined_dataset[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (combined_dataset[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                combined_dataset[\"Christmas_flag\"] = [1 if x==\"Christmas\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "                combined_dataset[\"Easter_flag\"] = [1 if x==\"Easter\" else 0 for x in combined_dataset[\"Week.Name\"]]\n",
    "\n",
    "                ## CHECKING if banner dummies are not present in required region, channel then introduce banner dummies\n",
    "                banner_dummies = pd.get_dummies(combined_dataset.Banner)              \n",
    "\n",
    "                # Uploaded data banner dummies\n",
    "                banner_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key].Banner)                                 \n",
    "                # If all banners are not present for the training data\n",
    "                c = [i for i in list(banner_dummies_check.columns) if i not in list(banner_dummies.columns)]\n",
    "                if len(c) != 0:\n",
    "                    banner_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)                \n",
    "                combined_dataset = pd.concat([combined_dataset, banner_dummies], axis=1)\n",
    "\n",
    "                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "                combined_dataset = combined_dataset.loc[combined_dataset[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                combined_dataset.sort_values('Week',inplace = True)\n",
    "                combined_dataset = pd.merge(combined_dataset,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset['Intial_weeks'] = combined_dataset['Intial_weeks'].fillna('Stabilization')\n",
    "                combined_dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                #Adding dummies for category\n",
    "                category_dummies = pd.get_dummies(combined_dataset['Category'])\n",
    "\n",
    "                # Adding dummies for product attribute - Category\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                # Uploaded data category dummies\n",
    "                category_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Category'])                                 \n",
    "                # If all pack subtypes are not present for the training data\n",
    "                c = [i for i in list(category_dummies_check.columns) if i not in list(category_dummies.columns)]\n",
    "                if len(c) != 0:\n",
    "                    category_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)                \n",
    "                combined_dataset = pd.concat([combined_dataset, category_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                # Adding dummies for product attribute - pack subtype\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset['Pack.Subtype'])\n",
    "                # Uploaded data pack subtype dummies\n",
    "                pack_subtypes_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['Pack.Subtype'])                                 \n",
    "                # If all pack subtypes are not present for the training data\n",
    "                c = [i for i in list(pack_subtypes_dummies_check.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                if len(c) != 0:\n",
    "                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)                \n",
    "                combined_dataset = pd.concat([combined_dataset, pack_subtypes_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                # Adding pack content dummies\n",
    "                pack_content_dummies = pd.get_dummies(combined_dataset['PACK_CONTENT'])\n",
    "                # Uploaded data pack content dummies\n",
    "                pack_content_dummies_check = pd.get_dummies(Volume_dataset_all_reg[Volume_dataset_all_reg.Region == Region_key]['PACK_CONTENT'])                                 \n",
    "                # If all pack contents are not present for the training data\n",
    "                c = [i for i in list(pack_content_dummies_check.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                if len(c) != 0:\n",
    "                    pack_content_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)                \n",
    "                combined_dataset = pd.concat([combined_dataset, pack_content_dummies], axis=1)\n",
    "                combined_dataset.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "                \n",
    "                # Product stage dummies\n",
    "                stage_dummies = pd.get_dummies(combined_dataset['Intial_weeks'])\n",
    "                combined_dataset = pd.concat([combined_dataset, stage_dummies], axis=1)\n",
    "                combined_dataset.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                #Adding discount depth columns\n",
    "                combined_dataset[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                combined_dataset[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset[\"Adcal_DD\"]]\n",
    "                # Adcal Price calculation\n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset.loc[combined_dataset['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset.loc[combined_dataset['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                combined_dataset.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                # Dropping null volumes\n",
    "                complete_dataset = combined_dataset.copy()        \n",
    "\n",
    "                complete_dataset = complete_dataset.loc[complete_dataset[\"Eq.Unit.Sales\"].notnull()]        \n",
    "                \n",
    "                # Get new product prediction for selected region\n",
    "                if Region_key == 'ONTARIO':                    \n",
    "                    complete_dataset_prod = test_data_set_cr_ontario[test_data_set_cr_ontario.iter == i].copy() \n",
    "                    complete_dataset_prod['Adcal_Price'] = np.exp(complete_dataset_prod['Adcal_Price'])\n",
    "                    complete_dataset_prod['Eq.Unit.Sales'] = np.exp(complete_dataset_prod['Eq.Unit.Sales'])\n",
    "\n",
    "                if Region_key == 'WEST':                    \n",
    "                    complete_dataset_prod = test_data_set_cr_west[test_data_set_cr_west.iter == i].copy() \n",
    "                    complete_dataset_prod['Adcal_Price'] = np.exp(complete_dataset_prod['Adcal_Price'])\n",
    "                    complete_dataset_prod['Eq.Unit.Sales'] = np.exp(complete_dataset_prod['Eq.Unit.Sales'])                                \n",
    "\n",
    "                if Region_key == 'QUEBEC':                    \n",
    "                    complete_dataset_prod = test_data_set_cr_quebec[test_data_set_cr_quebec.iter == i].copy() \n",
    "                    complete_dataset_prod['Adcal_Price'] = np.exp(complete_dataset_prod['Adcal_Price'])\n",
    "                    complete_dataset_prod['Eq.Unit.Sales'] = np.exp(complete_dataset_prod['Eq.Unit.Sales'])\n",
    "\n",
    "                if Region_key == 'EAST':                    \n",
    "                    complete_dataset_prod = test_data_set_cr_east[test_data_set_cr_east.iter == i].copy() \n",
    "                    complete_dataset_prod['Adcal_Price'] = np.exp(complete_dataset_prod['Adcal_Price'])\n",
    "                    complete_dataset_prod['Eq.Unit.Sales'] = np.exp(complete_dataset_prod['Eq.Unit.Sales'])\n",
    "\n",
    "                complete_dataset_prod['SIZE_ML'] = size.value\n",
    "                complete_dataset_prod['COUNT'] = count.value\n",
    "                # Dropping columns as they are not required in complete dataset\n",
    "                complete_dataset_prod = complete_dataset_prod.drop(['Region','iter'],axis = 1)\n",
    "\n",
    "                # Adding predicted data of new product in training            \n",
    "                Test_results_np_cr_v2 = Test_results_np_cr[(Test_results_np_cr[\"Region\"]==Region_key) & (Test_results_np_cr[\"iter\"]==i)]            \n",
    "                # Merge data to replace eq.unit,sales with predictions for new product\n",
    "                complete_dataset2= pd.merge(Test_results_np_cr_v2[['Week','Product','Channel','Banner','Predictions_rf']], complete_dataset_prod[complete_dataset_prod.Week <= max(Test_weeks)], \n",
    "                                   left_on=['Week','Product','Channel','Banner'], \n",
    "                                   right_on=['Week','Product','Channel','Banner'],how='outer') \n",
    "\n",
    "                complete_dataset2['Product'] = 'New'\n",
    "\n",
    "                # Replacing eq unit sales with new product prediction # Week filter given for 2017-2018 products\n",
    "                complete_dataset_prod['Eq.Unit.Sales'] = complete_dataset2.loc[(complete_dataset2.Product == 'New') & (complete_dataset2.Week >= launch_date), 'Predictions_rf'].values\n",
    "                complete_dataset_prod['Product'] = 'New'\n",
    "\n",
    "                complete_dataset = complete_dataset.append(complete_dataset_prod, ignore_index = True)\n",
    "\n",
    "                complete_dataset['Eq.Unit.Sales'] = complete_dataset['Eq.Unit.Sales'].round(4)\n",
    "\n",
    "                complete_dataset['Adcal_Price'] = np.log(complete_dataset['Adcal_Price'])\n",
    "                complete_dataset['Eq.Unit.Sales'] = np.log(complete_dataset['Eq.Unit.Sales'])\n",
    "                \n",
    "                # TEST PERIOD LOOP\n",
    "                for Test_period in Test_periods:\n",
    "\n",
    "                    #Filtering test weeks\n",
    "                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                    # Dividing train & test data\n",
    "                    complete_train_data_set = complete_dataset.loc[(complete_dataset.Week <= max(Test_weeks)) & ~(complete_dataset[\"Week\"].isin(Test_weeks))].reset_index(drop=True)\n",
    "                    # Including new product in training\n",
    "                    complete_train_data_set = complete_train_data_set.append(complete_dataset[complete_dataset.Product == 'New'], ignore_index = True)\n",
    "\n",
    "                    # Test data\n",
    "                    complete_test_data_set = complete_dataset.loc[complete_dataset[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                    # Dropping unneccessary columns from train dataset\n",
    "                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                    train_data_brand = train_data_set.copy()\n",
    "                    train_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)\n",
    "\n",
    "                    test_data_set = complete_test_data_set.copy()\n",
    "                    test_data_brand = test_data_set.copy()            \n",
    "\n",
    "                    # Dropping unneccessary columns from test dataset\n",
    "                    test_data_brand.drop([\"Week\",\"Week.Name\",\"Banner\",\"Product\",'Channel',\"EDV.Price\",\"Eq.Unit.Sales\"], inplace=True, axis=1)            \n",
    "\n",
    "                    #Creating test and train data - independent & dependent columns\n",
    "                    X_train = train_data_brand\n",
    "                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                    X_test = test_data_brand     \n",
    "                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "\n",
    "                    # Appending test data\n",
    "                    test_data_set[\"Region\"] = Region_key\n",
    "                    test_data_set[\"Test_period\"] = Test_period\n",
    "                    test_data_set['iter'] = i\n",
    "                    # In case the product is not present in certain region-quarter\n",
    "                    if len(test_data_set) == 0:\n",
    "                        continue\n",
    "\n",
    "                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)                \n",
    "                    #Model training\n",
    "                    rf = RandomForestRegressor(n_jobs=4,random_state=0)\n",
    "                    rf.fit(X_train, y_train)\n",
    "\n",
    "                    #Model predictions\n",
    "                    predictions_rf = rf.predict(X_test)                       \n",
    "\n",
    "                    #Storing test results\n",
    "                    result = X_test.copy()            \n",
    "\n",
    "                    result[\"Predictions_rf\"] = predictions_rf            \n",
    "                    result[\"Actuals\"] = y_test\n",
    "\n",
    "                    result['Predictions_rf'] = np.exp(result['Predictions_rf'])                    \n",
    "                    result['Actuals'] = np.exp(result['Actuals'])                                        \n",
    "\n",
    "                    #Calculating APE for the models\n",
    "                    result[\"ape_rf\"] = (result[\"Predictions_rf\"]-result[\"Actuals\"]).abs()\n",
    "\n",
    "                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                    result[\"Region\"] = Region_key\n",
    "                    result[\"Test_period\"] = Test_period\n",
    "                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                    result[\"DD_1\"] = test_data_set['DD_1']\n",
    "                    result[\"DD_2\"] = test_data_set['DD_2']\n",
    "                    result[\"Banner\"] = test_data_set['Banner']\n",
    "                    result[\"Channel\"] = test_data_set['Channel']\n",
    "                    ##Columns for Revenue                    \n",
    "                    result[\"Adcal_Price\"] = test_data_set[\"Adcal_Price\"] \n",
    "                    result[\"Adcal_Price\"] = np.exp(result[\"Adcal_Price\"])\n",
    "                    result['iter'] = i\n",
    "\n",
    "                    result = result[[\"Region\",\"Product\",'Channel','iter',\"Banner\",\"Test_period\",\"Week\",\"Actuals\",\"ape_rf\",\"Predictions_rf\",\"DD_1\",\"DD_2\",\"Adcal_Price\"]]\n",
    "\n",
    "                    # Appending results to the final results dataframe            \n",
    "                    Test_results_exist_with_cr = Test_results_exist_with_cr.append(result, ignore_index=True)\n",
    "\n",
    "\n",
    "################################################################################################################## \n",
    "                    # Collate all 3 step results\n",
    "################################################################################################################## \n",
    "\n",
    "if (toggle_np2.value == 'Yes') & (toggle_upload.value != 'Yes'):\n",
    "    iter_change_list = [1,2]\n",
    "else:\n",
    "    iter_change_list = [1]\n",
    "\n",
    "iteration_dataset_all_v2 = pd.DataFrame()    \n",
    "iteration_dataset_all_v3 = pd.DataFrame()\n",
    "\n",
    "if launch_date > Max_POS: \n",
    "    launch_date = '2019-01' # To include new product in step 2 otherwise it gets filtered out\n",
    "    \n",
    "for i in iter_change_list:    \n",
    "\n",
    "    # Input data for CR calculation\n",
    "    Model_1_data = Test_results_np_cr[Test_results_np_cr.iter == i]\n",
    "    Model_2_data = Test_results_exist_wo_cr\n",
    "    Model_3_data = Test_results_exist_with_cr[Test_results_exist_with_cr.iter == i]    \n",
    "\n",
    "    # Getting new product volume at region channel level\n",
    "    Model_1_data = Model_1_data[(Model_1_data[\"Week\"]>= launch_date)]\n",
    "    Model_1_summary = Model_1_data.groupby(['Region','Channel','Product'],as_index=False)['Predictions_rf'].sum()\n",
    "    Model_1_summary.columns = ['Region', 'Channel', 'Product', 'New Product Volume']\n",
    "    Model_1_summary['Product'] = 'New'\n",
    "    \n",
    "    # Same category products as affected products\n",
    "    Affected_products_list = Volume_dataset_all_reg[Volume_dataset_all_reg.Category == category_drop.value].Product.unique()\n",
    "    if (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "        Affected_products_list = np.append(Affected_products_list,'New')\n",
    "        \n",
    "    # Getting existing product volumes from model 2 at region channel level\n",
    "    Model_2_data=Model_2_data[(Model_2_data[\"Week\"]>= launch_date)]\n",
    "    Model_2_data=Model_2_data[Model_2_data[\"Product\"].isin(Affected_products_list)]\n",
    "    Model_2_summary= Model_2_data.groupby(['Region','Channel','Product'],as_index=False)['Predictions_rf'].sum()\n",
    "    Model_2_summary.columns= ['Region', 'Channel', 'Product', 'Existing Product Vol Without']\n",
    "\n",
    "    # Getting existing product volumes from model 3 at region channel level\n",
    "    Model_3_data=Model_3_data[(Model_3_data[\"Week\"]>= launch_date)]\n",
    "    \n",
    "    Model_3_data=Model_3_data[Model_3_data[\"Product\"].isin(Affected_products_list)]\n",
    "    Model_3_summary= Model_3_data.groupby(['Region','Channel','Product'],as_index=False)['Predictions_rf'].sum()\n",
    "    Model_3_summary.columns= ['Region', 'Channel', 'Product', 'Existing Product Vol With']\n",
    "\n",
    "    # Merging data\n",
    "    all_data=  pd.merge(Model_1_summary, Model_2_summary, on=[\"Region\",\"Channel\",\"Product\"],how=\"outer\")\n",
    "    all_data=pd.merge(all_data, Model_3_summary, on=[\"Region\",\"Channel\",\"Product\"],how=\"outer\")\n",
    "\n",
    "    ################################################################################################################## \n",
    "                        # Capping, scaling for new product cannibalization\n",
    "    ################################################################################################################## \n",
    "\n",
    "    iteration_dataset_all = pd.DataFrame()\n",
    "\n",
    "    All_result = pd.DataFrame({'Region': pd.Series([], dtype='object'),\n",
    "                             'Channel': pd.Series([], dtype='object'),\n",
    "                              'New_Product': pd.Series([], dtype='object'),\n",
    "                              'New Product Volume': pd.Series([], dtype='object'),\n",
    "                              'Can_sum_neg': pd.Series([], dtype='object'),\n",
    "                               'CR_negatives': pd.Series([], dtype='object')\n",
    "                              })\n",
    "\n",
    "    lower_thresh = -0.20\n",
    "    upper_thresh = 0.26\n",
    "\n",
    "    new_product_list = [product_fil]\n",
    "    for p in new_product_list:\n",
    "        input_data = all_data\n",
    "        new_product_data=input_data[input_data[\"Product\"]=='New']\n",
    "        existing_product_data=input_data    \n",
    "\n",
    "        existing_product_data=existing_product_data[[\"Region\",\"Channel\",\"Product\",\"Existing Product Vol Without\",\"Existing Product Vol With\"]]\n",
    "        existing_product_data.columns=['Region', 'Channel', 'Existing_Product', 'Existing Product Vol Without','Existing Product Vol With']\n",
    "\n",
    "        new_product_data=new_product_data[[\"Region\",\"Channel\",\"Product\",\"New Product Volume\"]]\n",
    "        new_product_data.columns=[\"Region\",\"Channel\",\"New_Product\",\"New Product Volume\"]\n",
    "\n",
    "        working_dataset= pd.merge(existing_product_data,new_product_data,left_on=['Region','Channel'],right_on=['Region','Channel'])\n",
    "        working_dataset[\"Cann Vol\"]= working_dataset[\"Existing Product Vol With\"]-working_dataset[\"Existing Product Vol Without\"]\n",
    "        working_dataset[\"Cann Vol%\"]=working_dataset[\"Cann Vol\"]/working_dataset[\"New Product Volume\"]\n",
    "        working_dataset[\"lower_thresh\"]=lower_thresh\n",
    "        working_dataset[\"upper_thresh\"]=upper_thresh\n",
    "        working_dataset = working_dataset[working_dataset['Existing_Product'] != 'New']\n",
    "        region_list= working_dataset[[\"Region\"]].drop_duplicates()[\"Region\"].tolist()\n",
    "        channel_list_cr= working_dataset[[\"Channel\"]].drop_duplicates()[\"Channel\"].tolist()\n",
    "\n",
    "        for k in region_list:           \n",
    "            for j in channel_list_cr:                \n",
    "                iteration_dataset= working_dataset[(working_dataset[\"Region\"]==k) & (working_dataset[\"Channel\"]==j)]           \n",
    "                iteration_dataset[\"Neg_max\"]=iteration_dataset[\"Cann Vol%\"].min() \n",
    "                temp_neg=iteration_dataset[iteration_dataset['Cann Vol%']<0]\n",
    "                iteration_dataset[\"Neg_min\"]=temp_neg['Cann Vol%'].max()\n",
    "                iteration_dataset[\"Pos_max\"]=iteration_dataset[\"Cann Vol%\"].max() \n",
    "                temp_pos=iteration_dataset[iteration_dataset['Cann Vol%']>0]\n",
    "                iteration_dataset[\"Pos_min\"]=temp_pos['Cann Vol%'].min()\n",
    "\n",
    "                # Calculating positive in range\n",
    "                temp_pin=iteration_dataset[(iteration_dataset['Cann Vol%']>0)&(iteration_dataset['Cann Vol%']<upper_thresh)]['Cann Vol%'].max()\n",
    "                temp_nin=iteration_dataset[(iteration_dataset['Cann Vol%']<0)&(iteration_dataset['Cann Vol%']>lower_thresh)]['Cann Vol%'].min()\n",
    "                iteration_dataset[\"Positive_in_range\"]=temp_pin\n",
    "                iteration_dataset[\"Negative_in_range\"]=temp_nin\n",
    "                iteration_dataset1=iteration_dataset\n",
    "                iteration_dataset1['Negative_in_range'] = iteration_dataset1['Negative_in_range'].replace(np.nan, -0.0001)\n",
    "                iteration_dataset1[\"Recalculated_Can%\"]= np.where(iteration_dataset1[\"Cann Vol%\"]<0,\n",
    "                                                                  np.where(iteration_dataset1[\"Cann Vol%\"]<lower_thresh,\n",
    "                                                                        (iteration_dataset1[\"lower_thresh\"]-iteration_dataset1[\"Negative_in_range\"])\n",
    "                                                                           *(iteration_dataset1[\"Cann Vol%\"]-iteration_dataset1[\"Neg_min\"])/\n",
    "                                                                           (iteration_dataset1[\"Neg_max\"]-iteration_dataset1[\"Neg_min\"])+iteration_dataset1[\"Negative_in_range\"]\n",
    "                                                                           ,iteration_dataset1[\"Cann Vol%\"]),\n",
    "                                                                  np.where(iteration_dataset1[\"Cann Vol%\"]>upper_thresh,\n",
    "                                                                          (iteration_dataset1[\"upper_thresh\"]-iteration_dataset1[\"Positive_in_range\"])*\n",
    "                                                                          (iteration_dataset1[\"Cann Vol%\"]-iteration_dataset1[\"Pos_min\"])/\n",
    "                                                                           (iteration_dataset1[\"Pos_max\"]-iteration_dataset1[\"Pos_min\"])+iteration_dataset1[\"Positive_in_range\"],\n",
    "                                                                  iteration_dataset1[\"Cann Vol%\"])\n",
    "                                                                 )\n",
    "                iteration_dataset1['Recalculated_Can%'] = iteration_dataset1['Recalculated_Can%'].replace(np.nan, lower_thresh)\n",
    "\n",
    "                iteration_dataset1[\"Updated_Can_Vol\"]=iteration_dataset1[\"New Product Volume\"]*iteration_dataset1[\"Recalculated_Can%\"]\n",
    "                iteration_dataset1[\"Can_sum_neg\"]=iteration_dataset1[iteration_dataset1[\"Updated_Can_Vol\"]<0][\"Updated_Can_Vol\"].sum()    \n",
    "                iteration_dataset1[\"CR_negatives\"]=iteration_dataset1[\"Can_sum_neg\"]/iteration_dataset1[\"New Product Volume\"]    \n",
    "\n",
    "                results= iteration_dataset1[[\"Region\",\"Channel\",\"New_Product\",\"New Product Volume\",\"Can_sum_neg\",\"CR_negatives\",]]\n",
    "                results=results.drop_duplicates().reset_index(drop=True)\n",
    "                iteration_dataset_all=iteration_dataset_all.append(iteration_dataset1)\n",
    "                All_result=All_result.append(results)        \n",
    "\n",
    "    if len(iteration_dataset_all) == 0: # in case the user gave to use 2nd new product & upload adcal data feature, this will remove the 2nd iteration & just keep the uploaded adcal data comparison\n",
    "        continue\n",
    "    # Region-channel CR view\n",
    "    iteration_dataset_all_v2_iter = iteration_dataset_all[iteration_dataset_all.Updated_Can_Vol < 0].copy()\n",
    "    iteration_dataset_all_v2_iter = iteration_dataset_all_v2_iter.groupby(['Region','Channel','New_Product']).agg({'New Product Volume':'sum','Updated_Can_Vol':'sum','Existing_Product':'nunique'}).reset_index()\n",
    "    iteration_dataset_all_v2_iter['New_Product_Volume'] = iteration_dataset_all_v2_iter['New Product Volume']/iteration_dataset_all_v2_iter['Existing_Product']\n",
    "    iteration_dataset_all_v2_iter.drop(columns = {'New Product Volume'}, inplace = True)\n",
    "    iteration_dataset_all_v2_iter.drop(columns = {'Existing_Product'}, inplace = True)\n",
    "    iteration_dataset_all_v2_iter.drop(columns = {'New_Product'}, inplace = True)\n",
    "    iteration_dataset_all_v2_iter.rename(columns = {'Updated_Can_Vol':'Displaced_Volume'}, inplace = True)\n",
    "    iteration_dataset_all_v2_iter = iteration_dataset_all_v2_iter[['Region','Channel','New_Product_Volume','Displaced_Volume']]\n",
    "    iteration_dataset_all_v2_iter['iter'] = i    \n",
    "    iteration_dataset_all_v2 = iteration_dataset_all_v2.append(iteration_dataset_all_v2_iter, ignore_index = True)\n",
    "        \n",
    "    # Affected product cannibalization distribution\n",
    "    iteration_dataset_all_v3_iter = iteration_dataset_all[iteration_dataset_all.Updated_Can_Vol < 0]\n",
    "    iteration_dataset_all_v3_iter = iteration_dataset_all_v3_iter.groupby(['Region','Existing_Product']).agg({'Existing Product Vol Without':'sum','Updated_Can_Vol':'sum'}).reset_index()\n",
    "    iteration_dataset_all_v3_iter['% CR'] = iteration_dataset_all_v3_iter['Updated_Can_Vol']/iteration_dataset_all_v3_iter['Existing Product Vol Without']                \n",
    "    iteration_dataset_all_v3_iter['iter'] = i\n",
    "    iteration_dataset_all_v3 = iteration_dataset_all_v3.append(iteration_dataset_all_v3_iter, ignore_index = True)\n",
    "\n",
    "    clear_output()\n",
    "\n",
    "# Checking overall cannibalised units (limiting it to 95%)\n",
    "iteration_dataset_all_v2['Displaced_Volume'] = np.where(iteration_dataset_all_v2['Displaced_Volume']/iteration_dataset_all_v2['New_Product_Volume'] < -0.95, -0.95*iteration_dataset_all_v2['New_Product_Volume'], iteration_dataset_all_v2['Displaced_Volume'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(qtr_edv_baseline) != 0 :\n",
    "    \n",
    "    # In case the user selects 2 new product for simulation    \n",
    "    if (toggle_np2.value == 'Yes') & (toggle_upload.value != 'Yes'):        \n",
    "        ########### TOTAL VOLUME ###########\n",
    "        iteration_dataset_all_v2_result = iteration_dataset_all_v2.groupby('iter')['Displaced_Volume','New_Product_Volume'].sum().reset_index()\n",
    "        iteration_dataset_all_v2_result['CR'] = iteration_dataset_all_v2_result['Displaced_Volume']/iteration_dataset_all_v2_result['New_Product_Volume']        \n",
    "\n",
    "        # Results    \n",
    "        old_vol = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Predictions_rf_exist'].sum()/2\n",
    "        new_vol = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].Predictions_rf.sum()\n",
    "        new_vol2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].Predictions_rf.sum()\n",
    "        # initialize list of lists\n",
    "        vol_data = [['Product', old_vol,new_vol,new_vol2]]\n",
    "\n",
    "        # Create the pandas DataFrame\n",
    "        df = pd.DataFrame(vol_data, columns = ['Volume','Existing','New Product 1','New Product 2'])\n",
    "\n",
    "        kpi1 = {}\n",
    "        kpi1['Volume RU Change in % (New Product 1)'] = round(100*(new_vol - old_vol)/old_vol,2)\n",
    "        kpi1['Volume RU Change in % (New Product 2)'] = round(100*(new_vol2 - old_vol)/old_vol,2)\n",
    "        kpi_df1 = pd.DataFrame(kpi1, index=[''])\n",
    "\n",
    "         ########### TOTAL REVENUE ###########\n",
    "        # Results    \n",
    "        old_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Sales_derived_exist'].sum()/2\n",
    "        new_revenue = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].Sales_derived.sum()\n",
    "        new_revenue2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].Sales_derived.sum()\n",
    "        # initialize list of lists\n",
    "        revenue_data = [['Product', old_revenue,new_revenue,new_revenue2]]\n",
    "\n",
    "        # Create the pandas DataFrame\n",
    "        df = pd.DataFrame(revenue_data, columns = ['Revenue','Existing','New Product 1','New Product 2'])\n",
    "\n",
    "        kpi2 = {}\n",
    "        kpi2['Revenue Change in % (New Product 1)'] = round(100*(new_revenue - old_revenue)/old_revenue,2)\n",
    "        kpi2['Revenue Change in % (New Product 2)'] = round(100*(new_revenue2 - old_revenue)/old_revenue,2)\n",
    "        kpi_df2 = pd.DataFrame(kpi2, index=[''])\n",
    "\n",
    "        kpi_df3 = iteration_dataset_all_v2_result[['iter','CR']]\n",
    "\n",
    "        if len(kpi_df3) > 0:\n",
    "            kpi_df3_final_1 = kpi_df3[kpi_df3.iter == 1].reset_index()\n",
    "            kpi_df3_final_2 = kpi_df3[kpi_df3.iter == 2].reset_index()\n",
    "\n",
    "            del kpi_df3_final_1['index']\n",
    "            del kpi_df3_final_2['index']\n",
    "\n",
    "            kpi_df3_final_1.drop(columns = {'iter'}, inplace = True)\n",
    "            kpi_df3_final_2.drop(columns = {'iter'}, inplace = True)\n",
    "            kpi_df3_final_1.rename(columns={'CR':'Cannibalization Rate in %'}, index={0:''}, inplace=True)\n",
    "            kpi_df3_final_2.rename(columns={'CR':'Cannibalization Rate in %'}, index={0:''}, inplace=True)    \n",
    "\n",
    "            kpi_df3_final_1['Cannibalization Rate in %'] = 100*kpi_df3_final_1['Cannibalization Rate in %']\n",
    "            kpi_df3_final_2['Cannibalization Rate in %'] = 100*kpi_df3_final_2['Cannibalization Rate in %']\n",
    "\n",
    "            kpi_df3_final_1['Cannibalization Rate in %'] = kpi_df3_final_1['Cannibalization Rate in %'].round(2)\n",
    "            kpi_df3_final_2['Cannibalization Rate in %'] = kpi_df3_final_2['Cannibalization Rate in %'].round(2)\n",
    "            \n",
    "            kpi_df3_final_1.rename(columns={'Cannibalization Rate in %':'Cannibalization Rate in % (New Product 1)'}, index={0:''}, inplace=True)\n",
    "            kpi_df3_final_2.rename(columns={'Cannibalization Rate in %':'Cannibalization Rate in % (New Product 2)'}, index={0:''}, inplace=True)    \n",
    "            \n",
    "        else:\n",
    "            kpi_df3_dict = {}\n",
    "            kpi_df3_dict['Existing Product Category CR in %'] = 'CR not available for selected Product Category'\n",
    "            kpi_df3_final = pd.DataFrame(kpi_df3_dict, index=[''])\n",
    "            kpi_df3_final.rename(columns = {'Existing Product Category CR in %':'Cannibalization Rate in %'}, inplace = True)\n",
    "            kpi_df3_final['Cannibalization Rate in %'] = 100*iteration_dataset_all_v2['Displaced_Volume'].sum()/iteration_dataset_all_v2['New_Product_Volume'].sum()\n",
    "            kpi_df3_final['Cannibalization Rate in %'] = kpi_df3_final['Cannibalization Rate in %'].round(2)\n",
    "    \n",
    "    # In case the user selects 1 new product for simulation    \n",
    "    else:\n",
    "\n",
    "        ########### TOTAL VOLUME ###########\n",
    "\n",
    "        # Results \n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':\n",
    "            old_vol = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Predictions_rf_exist'].sum()\n",
    "            new_vol = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].Predictions_rf.sum()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            old_vol = Test_results_exist2['Predictions_rf_exist'].sum()\n",
    "            new_vol = Test_results.Predictions_rf.sum()\n",
    "\n",
    "        # initialize list of lists\n",
    "        vol_data = [['Product', old_vol,new_vol]]\n",
    "\n",
    "        # Create the pandas DataFrame\n",
    "        df = pd.DataFrame(vol_data, columns = ['Volume','Existing','New'])\n",
    "\n",
    "        kpi1 = {}\n",
    "        kpi1['Volume RU Change in %'] = round(100*(new_vol - old_vol)/old_vol,2)\n",
    "        kpi_df1 = pd.DataFrame(kpi1, index=[''])\n",
    "\n",
    "         ########### TOTAL REVENUE ###########\n",
    "        # Results  \n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':\n",
    "            old_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Sales_derived_exist'].sum()\n",
    "            new_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].Sales_derived.sum()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            old_revenue = Test_results_exist2['Sales_derived_exist'].sum()\n",
    "            new_revenue = Test_results.Sales_derived.sum()            \n",
    "            \n",
    "        # initialize list of lists\n",
    "        revenue_data = [['Product', old_revenue,new_revenue]]\n",
    "\n",
    "        # Create the pandas DataFrame\n",
    "        df = pd.DataFrame(revenue_data, columns = ['Revenue','Existing','New'])\n",
    "\n",
    "        kpi2 = {}\n",
    "        kpi2['Revenue Change in %'] = round(100*(new_revenue - old_revenue)/old_revenue,2)\n",
    "        kpi_df2 = pd.DataFrame(kpi2, index=[''])\n",
    "\n",
    "        # initialize CR dataframe\n",
    "        kpi_df3 = {}\n",
    "        kpi_df3['Cannibalization Rate in %'] = 100*iteration_dataset_all_v2['Displaced_Volume'].sum()/iteration_dataset_all_v2['New_Product_Volume'].sum()\n",
    "        kpi_df3_final = pd.DataFrame(kpi_df3, index=[''])\n",
    "        kpi_df3_final        \n",
    "        kpi_df3_final['Cannibalization Rate in %'] = kpi_df3_final['Cannibalization Rate in %'].round(2)\n",
    "\n",
    "else:    \n",
    "    clear_output()\n",
    "    print('Product is not present in that Region/Channel for selected period')        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "table.dataframe td, table.dataframe th {\n",
       "    border: 2px  black solid !important;\n",
       "  color: black !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 2px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(qtr_edv_baseline) != 0 :\n",
    "    if (toggle_np2.value == 'Yes') & (toggle_upload.value != 'Yes'):        \n",
    "\n",
    "        if kpi_df3_final_1['Cannibalization Rate in % (New Product 1)'].values < -95:\n",
    "            kpi_df3_final_1['Cannibalization Rate in % (New Product 1)'] = -95\n",
    "        if kpi_df3_final_2['Cannibalization Rate in % (New Product 2)'].values < -95:\n",
    "            kpi_df3_final_2['Cannibalization Rate in % (New Product 2)'] = -95    \n",
    "\n",
    "        final_df = pd.concat([kpi_df1,kpi_df2, kpi_df3_final_1,kpi_df3_final_2], axis=1)\n",
    "        print('New Product 1 - ', 'Count: ', int(count.value), ' | Size: '  , int(size.value), ' | Category: ', category_drop.value, ' | Pack Subtype: ', pack_subtype_drop.value, ' | Pack Content: ', pack_content_drop.value)\n",
    "\n",
    "        final_df1 = final_df[['Volume RU Change in % (New Product 1)','Revenue Change in % (New Product 1)','Cannibalization Rate in % (New Product 1)']]\n",
    "\n",
    "        display(HTML(final_df1.to_html(index=False)))\n",
    "\n",
    "        print('New Product 2 - ', 'Count: ', int(count2.value), ' | Size: '  , int(size2.value), ' | Category: ', category_drop2.value, ' | Pack Subtype: ', pack_subtype_drop2.value, ' | Pack Content: ', pack_content_drop2.value)\n",
    "        final_df2 = final_df[['Volume RU Change in % (New Product 2)','Revenue Change in % (New Product 2)','Cannibalization Rate in % (New Product 2)']]\n",
    "        display(HTML(final_df2.to_html(index=False)))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        final_df = pd.concat([kpi_df1,kpi_df2, kpi_df3_final], axis=1)\n",
    "        display(HTML(final_df.to_html(index=False)))\n",
    "\n",
    "else:    \n",
    "    clear_output()\n",
    "    print('Product is not present in that Region/Channel for selected period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(qtr_edv_baseline) != 0:\n",
    "    \n",
    "    # When CR is present across region-channel\n",
    "    if len(kpi_df3) > 0:\n",
    "\n",
    "        # In case the user selects 2 new product for simulation    \n",
    "        if (toggle_np2.value == 'Yes') & (toggle_upload.value != 'Yes'):\n",
    "\n",
    "            plt.rcParams.update({'font.size': 15})\n",
    "            ########### TOTAL VOLUME ###########\n",
    "\n",
    "            # Results    \n",
    "                        \n",
    "            old_vol = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Predictions_rf_exist'].sum()/2\n",
    "            new_vol = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].Predictions_rf.sum()\n",
    "            new_vol2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].Predictions_rf.sum()\n",
    "            # initialize list of lists\n",
    "            vol_data = [['Product', old_vol,new_vol,new_vol2]]\n",
    "\n",
    "            # Create the pandas DataFrame\n",
    "            df = pd.DataFrame(vol_data, columns = ['Volume RU','Existing','New Product 1','New Product 2'])\n",
    "\n",
    "            df['Existing'] = df['Existing'].astype('int64')\n",
    "            df['Existing_formatted'] = df['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(round(x,2)).format('{:,}'))\n",
    "\n",
    "            df['New Product 1_overall'] = df['New Product 1']\n",
    "            df['New Product 1'] = df['New Product 1']*(-1*(final_df['Cannibalization Rate in % (New Product 1)'].values[0]/100))        \n",
    "            df['New_formatted 1'] = df['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(round(x,2)).format('{:,}'))\n",
    "            df['New_formatted 1_overall'] = df['New Product 1_overall'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "\n",
    "            df['New Product 1_inc'] = new_vol*(1+final_df['Cannibalization Rate in % (New Product 1)'].values[0]/100)\n",
    "            df['New_formatted 1_inc'] = df['New Product 1_inc'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(round(x,2)).format('{:,}'))        \n",
    "\n",
    "            df['New Product 2_overall'] = df['New Product 2']\n",
    "            df['New Product 2'] = df['New Product 2']*(-1*(final_df['Cannibalization Rate in % (New Product 2)'].values[0]/100))\n",
    "            df['New_formatted 2'] = df['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(round(x,2)).format('{:,}'))\n",
    "            df['New_formatted 2_overall'] = df['New Product 2_overall'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(round(x,2)).format('{:,}'))\n",
    "\n",
    "            df['New Product 2_inc'] = new_vol2*(1+final_df['Cannibalization Rate in % (New Product 2)'].values[0]/100)\n",
    "            df['New_formatted 2_inc'] = df['New Product 2_inc'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(round(x,2)).format('{:,}'))        \n",
    "\n",
    "\n",
    "            ########### TOTAL REVENUE ###########\n",
    "\n",
    "            # Results    \n",
    "            old_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Sales_derived_exist'].sum()/2\n",
    "            new_revenue = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].Sales_derived.sum()\n",
    "            new_revenue2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].Sales_derived.sum()\n",
    "            # initialize list of lists\n",
    "            revenue_data = [['Product', old_revenue,new_revenue,new_revenue2]]\n",
    "\n",
    "            # Create the pandas DataFrame\n",
    "            df_rev = pd.DataFrame(revenue_data, columns = ['Revenue','Existing','New Product 1','New Product 2'])\n",
    "\n",
    "            df_rev['Existing'] = df_rev['Existing'].astype('int64')\n",
    "            df_rev['Existing_formatted'] = df_rev['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df_rev['New Product 1_overall'] = df_rev['New Product 1']\n",
    "            df_rev['New Product 1'] = df_rev['New Product 1']*(-1*(final_df['Cannibalization Rate in % (New Product 1)'].values[0]/100))\n",
    "            df_rev['New_formatted 1'] = df_rev['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "            df_rev['New_formatted 1_overall'] = df_rev['New Product 1_overall'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df_rev['New Product 1_inc'] = new_revenue*(1+final_df['Cannibalization Rate in % (New Product 1)'].values[0]/100)\n",
    "            df_rev['New_formatted 1_inc'] = df_rev['New Product 1_inc'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df_rev['New Product 2_overall'] = df_rev['New Product 2']\n",
    "            df_rev['New Product 2'] = df_rev['New Product 2']*(-1*(final_df['Cannibalization Rate in % (New Product 2)'].values[0]/100))\n",
    "            df_rev['New_formatted 2'] = df_rev['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "            df_rev['New_formatted 2_overall'] = df_rev['New Product 2_overall'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df_rev['New Product 2_inc'] = new_revenue2*(1+final_df['Cannibalization Rate in % (New Product 2)'].values[0]/100)\n",
    "            df_rev['New_formatted 2_inc'] = df_rev['New Product 2_inc'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))        \n",
    "\n",
    "            ###############PLOTS###################\n",
    "\n",
    "            x1 = np.arange(len([df.columns[0]]))  # the label locations\n",
    "            x2 = np.arange(len([df_rev.columns[0]]))  # the label locations    \n",
    "            width = 300  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/3, list(df['Existing']), width = 80, label='Existing')\n",
    "            rects2 = ax1.bar(x1, list(df['New Product 1']), width = 80, label='New Product 1 Cannibalised Units')\n",
    "            rects2_overall = ax1.bar(x1, list(df['New Product 1_overall']),  width =0)\n",
    "            rects2_inc = ax1.bar(x1, list(df['New Product 1_inc']), bottom = list(df['New Product 1']),width = 80, label='New Product 1 Incremental Units')\n",
    "            rects2_2 = ax1.bar(x1 + width/3, list(df['New Product 2']), width = 80, label='New Product 2 Cannibalised Units')\n",
    "            rects2_2_overall = ax1.bar(x1 + width/3, list(df['New Product 2_overall']),  width =0)\n",
    "            rects2_2_inc = ax1.bar(x1 + width/3, list(df['New Product 2_inc']), bottom = list(df['New Product 2']), width = 80, label='New Product 2 Incremental Units')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Overall)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels([df.columns[0]], fontsize=20)\n",
    "            ax1.legend(loc = 'upper left',bbox_to_anchor=(0, -0.05))\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, label_type = 'edge', labels=list(df['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, label_type = 'center', labels=[df['New_formatted 1'].values[0] + \" \" + \"(\" + str(int(-1*final_df['Cannibalization Rate in % (New Product 1)'].values[0].round(0))) + \"%\" \")\"])\n",
    "            ax1.bar_label(rects2_overall, padding=5, label_type = 'edge', labels=list(df['New_formatted 1_overall']))\n",
    "            ax1.bar_label(rects2_inc, padding=5, label_type = 'center', labels=[df['New_formatted 1_inc'].values[0] + \" \" +  \"(\" + str(int(100 + final_df['Cannibalization Rate in % (New Product 1)'].values[0].round(0))) + \"%\" \")\"])\n",
    "            ax1.bar_label(rects2_2, padding=5, label_type = 'center', labels=[df['New_formatted 2'].values[0] + \" \" + \"(\" + str(int(-1*final_df['Cannibalization Rate in % (New Product 2)'].values[0].round(0))) + \"%\" \")\"])\n",
    "            ax1.bar_label(rects2_2_overall, padding=5, label_type = 'edge', labels=list(df['New_formatted 2_overall']))\n",
    "            ax1.bar_label(rects2_2_inc, padding=5, label_type = 'center', labels=[df['New_formatted 2_inc'].values[0] + \" \" + \"(\" + str(int(100 + final_df['Cannibalization Rate in % (New Product 2)'].values[0].round(0))) + \"%\" \")\"])\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/3., list(df_rev['Existing']), width=80, label='Existing')\n",
    "            rects4 = ax2.bar(x2, list(df_rev['New Product 1']), width=80, label='New Product 1 Cannibalised Revenue')\n",
    "            rects4_overall = ax2.bar(x2, list(df_rev['New Product 1_overall']), width=0)\n",
    "            rects4_inc = ax2.bar(x2, list(df_rev['New Product 1_inc']), bottom = list(df_rev['New Product 1']),width=80, label='New Product 1 Incremental Revenue')\n",
    "            rects4_2 = ax2.bar(x2 + width/3., list(df_rev['New Product 2']), width=80, label='New Product 2 Cannibalised Revenue')\n",
    "            rects4_2_overall = ax2.bar(x2 + width/3, list(df_rev['New Product 2_overall']), width=0)\n",
    "            rects4_2_inc = ax2.bar(x2 + width/3., list(df_rev['New Product 2_inc']), bottom = list(df_rev['New Product 2']),width=80, label='New Product 2 Incremental Revenue')\n",
    "\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Overall)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels([df_rev.columns[0]], fontsize=20)\n",
    "            ax2.legend(loc = 'upper left',bbox_to_anchor=(0, -0.05))\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, label_type = 'edge', labels=list(df_rev['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, label_type = 'center', labels=[df_rev['New_formatted 1'].values[0] + \" \" + \"(\" + str(int(-1*final_df['Cannibalization Rate in % (New Product 1)'].values[0].round(0))) + \"%\" \")\"])\n",
    "            ax2.bar_label(rects4_overall, padding=5, label_type = 'edge', labels=list(df_rev['New_formatted 1_overall']))\n",
    "            ax2.bar_label(rects4_inc, padding=5, label_type = 'center', labels=[df_rev['New_formatted 1_inc'].values[0] + \" \" + \"(\" + str(int(100 + final_df['Cannibalization Rate in % (New Product 1)'].values[0].round(0))) + \"%\" \")\"])\n",
    "            ax2.bar_label(rects4_2, padding=5, label_type = 'center', labels=[df_rev['New_formatted 2'].values[0] + \" \" + \"(\" + str(int(-1*final_df['Cannibalization Rate in % (New Product 2)'].values[0].round(0))) + \"%\" \")\"])\n",
    "            ax2.bar_label(rects4_2_overall, padding=5, label_type = 'edge', labels=list(df_rev['New_formatted 2_overall']))\n",
    "            ax2.bar_label(rects4_2_inc, padding=5, label_type = 'center', labels=[df_rev['New_formatted 2_inc'].values[0] + \" \" + \"(\" + str(int(100 + final_df['Cannibalization Rate in % (New Product 2)'].values[0].round(0))) + \"%\" \")\"])\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        # In case the user selects 1 new product for simulation            \n",
    "        else:\n",
    "            plt.rcParams.update({'font.size': 15})\n",
    "            ########### TOTAL VOLUME ###########\n",
    "\n",
    "            # Results   \n",
    "            \n",
    "            # In case the user doesn't upload the data\n",
    "            if toggle_upload.value != 'Yes':            \n",
    "                old_vol = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Predictions_rf_exist'].sum()\n",
    "                new_vol = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].Predictions_rf.sum()\n",
    "            elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "                old_vol = Test_results_exist2['Predictions_rf_exist'].sum()\n",
    "                new_vol = Test_results.Predictions_rf.sum()\n",
    "                \n",
    "            # initialize list of lists\n",
    "            vol_data = [['Product', old_vol,new_vol]]\n",
    "\n",
    "            # Create the pandas DataFrame\n",
    "            df = pd.DataFrame(vol_data, columns = ['Volume RU','Existing','New'])\n",
    "\n",
    "            df['Existing'] = df['Existing'].astype('int64')\n",
    "            df['Existing_formatted'] = df['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(round(x,2)).format('{:,}'))\n",
    "\n",
    "            df['New_overall'] = df['New']\n",
    "            df['New'] = df['New']*(-1*(final_df['Cannibalization Rate in %'].values[0]/100))\n",
    "            df['New_formatted'] = df['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "            df['New_formatted_overall'] = df['New_overall'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(round(x,2)).format('{:,}'))\n",
    "\n",
    "            df['New_Inc'] = new_vol*(1+final_df['Cannibalization Rate in %'].values[0]/100)\n",
    "            df['New_formatted_Inc'] = df['New_Inc'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(round(x,2)).format('{:,}'))\n",
    "\n",
    "\n",
    "            ########### TOTAL REVENUE ###########\n",
    "\n",
    "            # Results  \n",
    "            # In case the user doesn't upload the data\n",
    "            if toggle_upload.value != 'Yes':\n",
    "                old_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Sales_derived_exist'].sum()\n",
    "                new_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].Sales_derived.sum()\n",
    "            elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "                old_revenue = Test_results_exist2['Sales_derived_exist'].sum()\n",
    "                new_revenue = Test_results.Sales_derived.sum()            \n",
    "            \n",
    "            # initialize list of lists\n",
    "            revenue_data = [['Product', old_revenue,new_revenue]]\n",
    "\n",
    "            # Create the pandas DataFrame\n",
    "            df_rev = pd.DataFrame(revenue_data, columns = ['Revenue','Existing','New'])\n",
    "\n",
    "            df_rev['Existing'] = df_rev['Existing'].astype('int64')\n",
    "            df_rev['Existing_formatted'] = df_rev['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df_rev['New_overall'] = df_rev['New']\n",
    "            df_rev['New'] = df_rev['New']*(-1*(final_df['Cannibalization Rate in %'].values[0]/100))\n",
    "            df_rev['New_formatted'] = df_rev['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "            df_rev['New_formatted_overall'] = df_rev['New_overall'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df_rev['New_Inc'] = new_revenue*(1+final_df['Cannibalization Rate in %'].values[0]/100)\n",
    "            df_rev['New_formatted_Inc'] = df_rev['New_Inc'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            ###############PLOTS###################\n",
    "\n",
    "            x1 = np.arange(len([df.columns[0]]))  # the label locations\n",
    "            x2 = np.arange(len([df_rev.columns[0]]))  # the label locations\n",
    "            width = 300  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(df['Existing']), width = 80, label='Existing')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(df['New']), width = 80, label='Cannibalised Units')\n",
    "            rects2_overall = ax1.bar(x1 + width/2, list(df['New_overall']), width = 0)\n",
    "            rects2_inc = ax1.bar(x1 + width/2, list(df['New_Inc']), bottom =  list(df['New']),width = 80, label='Incremental Units')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Overall)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels([df.columns[0]], fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, label_type = 'edge', labels=list(df['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, label_type = 'center', labels=[df['New_formatted'].values[0] + \" \" + \"(\" + str(int(-1*final_df['Cannibalization Rate in %'].values[0].round(0))) + \"%\" \")\"])\n",
    "            ax1.bar_label(rects2_overall, padding=5, label_type = 'edge', labels=list(df['New_formatted_overall']))\n",
    "            ax1.bar_label(rects2_inc, padding=5, label_type = 'center', labels=[df['New_formatted_Inc'].values[0] + \" \" + \"(\" + str(int(100+final_df['Cannibalization Rate in %'].values[0].round(0))) + \"%\" \")\"])\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2., list(df_rev['Existing']), width=80, label='Existing')\n",
    "            rects4 = ax2.bar(x2 + width/2., list(df_rev['New']), width=80, label='Cannibalised Revenue')\n",
    "            rects4_overall = ax2.bar(x2 + width/2., list(df_rev['New_overall']), width=0)\n",
    "            rects4_inc = ax2.bar(x2 + width/2., list(df_rev['New_Inc']), bottom =  list(df_rev['New']), width=80, label='Incremental Revenue')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Overall)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels([df_rev.columns[0]], fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, label_type = 'edge', labels=list(df_rev['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, label_type = 'center', labels=[df_rev['New_formatted'].values[0] +\" \" + \"(\" + str(int(-1*final_df['Cannibalization Rate in %'].values[0].round(0))) + \"%\" \")\"])\n",
    "            ax2.bar_label(rects4_overall, padding=5, label_type = 'edge', labels=list(df_rev['New_formatted_overall']))\n",
    "            ax2.bar_label(rects4_inc, padding=5, label_type = 'center', labels=[df_rev['New_formatted_Inc'].values[0] + \" \" + \"(\" + str(int(100+final_df['Cannibalization Rate in %'].values[0].round(0))) + \"%\" \")\"])\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    # When CR is not present across region-channel            \n",
    "    else:\n",
    "                \n",
    "        # In case the user selects 2 new product for simulation    \n",
    "        if (toggle_np2.value == 'Yes') & (toggle_upload.value != 'Yes'):\n",
    "\n",
    "            plt.rcParams.update({'font.size': 15})\n",
    "            ########### TOTAL VOLUME ###########\n",
    "\n",
    "            # Results    \n",
    "            old_vol = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Predictions_rf_exist'].sum()/2\n",
    "            new_vol = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].Predictions_rf.sum()\n",
    "            new_vol2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].Predictions_rf.sum()\n",
    "            # initialize list of lists\n",
    "            vol_data = [['Product', old_vol,new_vol,new_vol2]]\n",
    "\n",
    "            # Create the pandas DataFrame\n",
    "            df = pd.DataFrame(vol_data, columns = ['Volume RU','Existing','New Product 1','New Product 2'])\n",
    "\n",
    "            df['Existing'] = df['Existing'].astype('int64')\n",
    "            df['Existing_formatted'] = df['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df['New Product 1'] = df['New Product 1'].astype('int64')\n",
    "            df['New_formatted 1'] = df['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df['New Product 2'] = df['New Product 2'].astype('int64')\n",
    "            df['New_formatted 2'] = df['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "\n",
    "            ########### TOTAL REVENUE ###########\n",
    "\n",
    "            # Results    \n",
    "            old_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Sales_derived_exist'].sum()/2\n",
    "            new_revenue = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].Sales_derived.sum()\n",
    "            new_revenue2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].Sales_derived.sum()\n",
    "            # initialize list of lists\n",
    "            revenue_data = [['Product', old_revenue,new_revenue,new_revenue2]]\n",
    "\n",
    "            # Create the pandas DataFrame\n",
    "            df_rev = pd.DataFrame(revenue_data, columns = ['Revenue','Existing','New Product 1','New Product 2'])\n",
    "\n",
    "            df_rev['Existing'] = df_rev['Existing'].astype('int64')\n",
    "            df_rev['Existing_formatted'] = df_rev['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df_rev['New Product 1'] = df_rev['New Product 1'].astype('int64')\n",
    "            df_rev['New_formatted 1'] = df_rev['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df_rev['New Product 2'] = df_rev['New Product 2'].astype('int64')\n",
    "            df_rev['New_formatted 2'] = df_rev['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            ###############PLOTS###################\n",
    "\n",
    "            x1 = np.arange(len([df.columns[0]]))  # the label locations\n",
    "            x2 = np.arange(len([df_rev.columns[0]]))  # the label locations    \n",
    "            width = 300  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/3, list(df['Existing']), width = 80, label='Existing')\n",
    "            rects2 = ax1.bar(x1, list(df['New Product 1']), width = 80, label='New Product 1')\n",
    "            rects2_2 = ax1.bar(x1 + width/3, list(df['New Product 2']), width = 80, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Overall)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels([df.columns[0]], fontsize=20)\n",
    "            ax1.legend(loc = 'lower right')\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(df['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(df['New_formatted 1']))\n",
    "            ax1.bar_label(rects2_2, padding=5, labels=list(df['New_formatted 2']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/3., list(df_rev['Existing']), width=80, label='Existing')\n",
    "            rects4 = ax2.bar(x2, list(df_rev['New Product 1']), width=80, label='New Product 1')\n",
    "            rects4_2 = ax2.bar(x2 + width/3., list(df_rev['New Product 2']), width=80, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Overall)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels([df_rev.columns[0]], fontsize=20)\n",
    "            ax2.legend(loc = 'lower right')\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(df_rev['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(df_rev['New_formatted 1']))\n",
    "            ax2.bar_label(rects4_2, padding=5, labels=list(df_rev['New_formatted 2']))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        # In case the user selects 1 new product for simulation            \n",
    "        else:\n",
    "            plt.rcParams.update({'font.size': 15})\n",
    "            ########### TOTAL VOLUME ###########\n",
    "\n",
    "            # In case the user doesn't upload the data\n",
    "            if toggle_upload.value != 'Yes':            \n",
    "                old_vol = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Predictions_rf_exist'].sum()\n",
    "                new_vol = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].Predictions_rf.sum()\n",
    "            elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "                old_vol = Test_results['Predictions_rf_exist'].sum()\n",
    "                new_vol = Test_results.Predictions_rf.sum()\n",
    "\n",
    "            # initialize list of lists\n",
    "            vol_data = [['Product', old_vol,new_vol]]\n",
    "\n",
    "            # Create the pandas DataFrame\n",
    "            df = pd.DataFrame(vol_data, columns = ['Volume RU','Existing','New'])\n",
    "\n",
    "            df['Existing'] = df['Existing'].astype('int64')\n",
    "            df['Existing_formatted'] = df['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df['New'] = df['New'].astype('int64')\n",
    "            df['New_formatted'] = df['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "\n",
    "            ########### TOTAL REVENUE ###########\n",
    "\n",
    "            # In case the user doesn't upload the data\n",
    "            if toggle_upload.value != 'Yes':\n",
    "                old_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)]['Sales_derived_exist'].sum()\n",
    "                new_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].Sales_derived.sum()\n",
    "            elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "                old_revenue = Test_results['Sales_derived_exist'].sum()\n",
    "                new_revenue = Test_results.Sales_derived.sum()            \n",
    "            # initialize list of lists\n",
    "            revenue_data = [['Product', old_revenue,new_revenue]]\n",
    "\n",
    "            # Create the pandas DataFrame\n",
    "            df_rev = pd.DataFrame(revenue_data, columns = ['Revenue','Existing','New'])\n",
    "\n",
    "            df_rev['Existing'] = df_rev['Existing'].astype('int64')\n",
    "            df_rev['Existing_formatted'] = df_rev['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            df_rev['New'] = df_rev['New'].astype('int64')\n",
    "            df_rev['New_formatted'] = df_rev['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000,2))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "            ###############PLOTS###################\n",
    "\n",
    "            x1 = np.arange(len([df.columns[0]]))  # the label locations\n",
    "            x2 = np.arange(len([df_rev.columns[0]]))  # the label locations\n",
    "            width = 300  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(df['Existing']), width = 80, label='Existing')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(df['New']), width = 80, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Overall)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels([df.columns[0]], fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(df['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(df['New_formatted']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2., list(df_rev['Existing']), width=80, label='Existing')\n",
    "            rects4 = ax2.bar(x2 + width/2., list(df_rev['New']), width=80, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Overall)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels([df_rev.columns[0]], fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(df_rev['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(df_rev['New_formatted']))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "else:    \n",
    "    clear_output()\n",
    "    print('Product is not present in that Region/Channel for selected period')    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "\n",
    "########### REGION VOLUME ###########\n",
    "region_2 = widgets.Dropdown(options=list(np.sort(Volume_dataset_all_reg.Region.unique())) + ['All'], value=region.value, description='Region', layout = layout3)\n",
    "\n",
    "region_drop = region_2\n",
    "\n",
    "##############################################################################################################################\n",
    "#  Getting required data\n",
    "##############################################################################################################################\n",
    "\n",
    "if len(qtr_edv_baseline) != 0 :\n",
    "    # In case the user selects 2 new product for simulation    \n",
    "    if (toggle_np2.value == 'Yes') & (toggle_upload.value != 'Yes'):\n",
    "        \n",
    "        # Results    \n",
    "        old_reg = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "        new_reg = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        new_reg.rename(columns = {'Predictions_rf':'Predictions_rf_np1'}, inplace = True)\n",
    "        new_reg_2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        new_reg_2.rename(columns = {'Predictions_rf':'Predictions_rf_np2'}, inplace = True)\n",
    "\n",
    "        # merge w.r.t. region\n",
    "        tot_vol = old_reg.merge(new_reg).merge(new_reg_2)\n",
    "        tot_vol = tot_vol.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf_np1' : 'New Product 1','Predictions_rf_np2' : 'New Product 2'})\n",
    "\n",
    "        ########### REGION REVENUE ###########\n",
    "\n",
    "        # Results    \n",
    "        old_reg_revenue = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "        new_reg_revenue = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        new_reg_revenue.rename(columns = {'Sales_derived':'Sales_derived_np1'}, inplace = True)\n",
    "        new_reg_revenue_2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        new_reg_revenue_2.rename(columns = {'Sales_derived':'Sales_derived_np2'}, inplace = True)\n",
    "\n",
    "        merge_data_region = new_reg_revenue.merge(new_reg_revenue_2, on = 'Region', how = 'outer').sort_values('Region').merge(old_reg_revenue, on = 'Region', how = 'outer').sort_values('Region')\n",
    "        merge_data_region = merge_data_region.rename(columns = {'Sales_derived_np1':'New Product 1','Sales_derived_np2':'New Product 2','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        ########### Channel VOLUME ###########\n",
    "\n",
    "        # Results    \n",
    "        old_chan_volume = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby(['Channel','Region'])['Predictions_rf_exist'].sum().reset_index()\n",
    "        new_chan_volume = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby(['Channel','Region']).Predictions_rf.sum().reset_index()\n",
    "        new_chan_volume.rename(columns = {'Predictions_rf':'Predictions_rf_np1'}, inplace = True)\n",
    "        \n",
    "        new_chan_volume2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby(['Channel','Region']).Predictions_rf.sum().reset_index()\n",
    "        new_chan_volume2.rename(columns = {'Predictions_rf':'Predictions_rf_np2'}, inplace = True)\n",
    "        \n",
    "        merge_data_channel = new_chan_volume.merge(new_chan_volume2, on = ['Region','Channel'], how = 'outer').sort_values('Channel').merge(old_chan_volume, on = ['Region','Channel'], how = 'outer').sort_values('Channel')\n",
    "        merge_data_channel = merge_data_channel.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf_np1' : 'New Product 1','Predictions_rf_np2' : 'New Product 2'})\n",
    "\n",
    "        ########### Channel REVENUE ###########\n",
    "\n",
    "        # Results    \n",
    "        old_chan_revenue = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby(['Channel','Region'])['Sales_derived_exist'].sum().reset_index()\n",
    "        new_chan_revenue = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby(['Channel','Region']).Sales_derived.sum().reset_index()\n",
    "        new_chan_revenue.rename(columns = {'Sales_derived':'Sales_derived_np1'}, inplace = True)\n",
    "        \n",
    "        new_chan_revenue2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby(['Channel','Region']).Sales_derived.sum().reset_index()\n",
    "        new_chan_revenue2.rename(columns = {'Sales_derived':'Sales_derived_np2'}, inplace = True)\n",
    "        \n",
    "        merge_data_channel_revenue = new_chan_revenue.merge(new_chan_revenue2, on = ['Region','Channel'], how = 'outer').sort_values('Channel').merge(old_chan_revenue, on = ['Region','Channel'], how = 'outer').sort_values('Channel')\n",
    "        merge_data_channel_revenue = merge_data_channel_revenue.rename(columns = {'Sales_derived_np1':'New Product 1','Sales_derived_np2':'New Product 2','Sales_derived_exist':'Existing'})\n",
    "        \n",
    "        ########### QUARTER VOLUME ###########\n",
    "\n",
    "        # Results    \n",
    "        ref_product_quarter = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby(['Test_period','Region'])['Predictions_rf_exist'].sum().reset_index()\n",
    "        new_product_quarter = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby(['Test_period','Region']).Predictions_rf.sum().reset_index()\n",
    "        new_product_quarter.rename(columns = {'Predictions_rf':'Predictions_rf_np1'}, inplace = True)\n",
    "        new_product_quarter_2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby(['Test_period','Region']).Predictions_rf.sum().reset_index()\n",
    "        new_product_quarter_2.rename(columns = {'Predictions_rf':'Predictions_rf_np2'}, inplace = True)\n",
    "\n",
    "        merge_data_quarter = new_product_quarter.merge(new_product_quarter_2, on = ['Region','Test_period'], how = 'outer').sort_values(['Region','Test_period']).merge(ref_product_quarter, on = ['Region','Test_period'], how = 'outer').sort_values(['Region','Test_period'])\n",
    "        merge_data_quarter = merge_data_quarter.rename(columns = {'Predictions_rf_np1':'New Product 1','Predictions_rf_np2':'New Product 2','Predictions_rf_exist':'Existing'})\n",
    "\n",
    "        merge_data_quarter = merge_data_quarter.rename(columns = {'Test_period':'Year Quarter'})\n",
    "\n",
    "        ########### QUARTER REVENUE ###########\n",
    "\n",
    "        # Results    \n",
    "        ref_product_quarter_revenue = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby(['Test_period','Region'])['Sales_derived_exist'].sum().reset_index()\n",
    "        new_product_quarter_revenue = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby(['Test_period','Region']).Sales_derived.sum().reset_index()\n",
    "        new_product_quarter_revenue.rename(columns = {'Sales_derived':'Sales_derived_np1'}, inplace = True)\n",
    "\n",
    "        new_product_quarter_revenue_2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby(['Test_period','Region']).Sales_derived.sum().reset_index()\n",
    "        new_product_quarter_revenue_2.rename(columns = {'Sales_derived':'Sales_derived_np2'}, inplace = True)\n",
    "\n",
    "        merge_data_quarter_revenue = new_product_quarter_revenue.merge(new_product_quarter_revenue_2, on = ['Region','Test_period'], how = 'outer').sort_values(['Region','Test_period']).merge(ref_product_quarter_revenue, on = ['Region','Test_period'], how = 'outer').sort_values(['Region','Test_period'])\n",
    "        merge_data_quarter_revenue = merge_data_quarter_revenue.rename(columns = {'Sales_derived_np1':'New Product 1','Sales_derived_np2':'New Product 2','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        merge_data_quarter_revenue = merge_data_quarter_revenue.rename(columns = {'Test_period':'Year Quarter'})\n",
    "\n",
    "\n",
    "        ############################# HOLIDAY VOLUME #############################\n",
    "\n",
    "        # Holiday weeks\n",
    "        holiday_weeks = Volume_dataset[(Volume_dataset.Product == product_fil) & (Volume_dataset.Week >= '2019-01') & (Volume_dataset.Week <= '2020-52')][['Week','Pre.Holiday.Week', 'Holiday.Week', 'Post.Holiday.Week']].drop_duplicates().sort_values('Week')\n",
    "        pre_holiday_weeks = list(holiday_weeks[holiday_weeks['Pre.Holiday.Week'] == 1].Week)\n",
    "        during_holiday_weeks = list(holiday_weeks[holiday_weeks['Holiday.Week'] == 1].Week)\n",
    "        post_holiday_weeks = list(holiday_weeks[holiday_weeks['Post.Holiday.Week'] == 1].Week)\n",
    "\n",
    "        # Pre Results    \n",
    "        pre_holiday_vol_old = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Week.isin(pre_holiday_weeks))&(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "        # Product 1\n",
    "        pre_holiday_vol_new = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Week.isin(pre_holiday_weeks)) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        pre_holiday_vol_new.rename(columns = {'Predictions_rf':'Predictions_rf_np1'}, inplace = True)\n",
    "        # Product 2\n",
    "        pre_holiday_vol_new_2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Week.isin(pre_holiday_weeks)) & (Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        pre_holiday_vol_new_2.rename(columns = {'Predictions_rf':'Predictions_rf_np2'}, inplace = True)\n",
    "\n",
    "        pre_merge = pre_holiday_vol_old.merge(pre_holiday_vol_new).merge(pre_holiday_vol_new_2)\n",
    "        pre_merge = pre_merge.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf_np1':'New Product 1','Predictions_rf_np2':'New Product 2'})\n",
    "\n",
    "        # During Results    \n",
    "        during_holiday_vol_old = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Week.isin(during_holiday_weeks))&(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "        # Product 1\n",
    "        during_holiday_vol_new = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Week.isin(during_holiday_weeks)) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        during_holiday_vol_new.rename(columns = {'Predictions_rf':'Predictions_rf_np1'}, inplace = True)\n",
    "        # Product 2\n",
    "        during_holiday_vol_new_2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Week.isin(during_holiday_weeks)) & (Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        during_holiday_vol_new_2.rename(columns = {'Predictions_rf':'Predictions_rf_np2'}, inplace = True)\n",
    "\n",
    "        during_merge = during_holiday_vol_old.merge(during_holiday_vol_new).merge(during_holiday_vol_new_2)\n",
    "        during_merge = during_merge.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf_np1':'New Product 1','Predictions_rf_np2':'New Product 2'})\n",
    "\n",
    "        # Post Results    \n",
    "        post_holiday_vol_old = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Week.isin(post_holiday_weeks))&(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "        # Product 1\n",
    "        post_holiday_vol_new = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Week.isin(post_holiday_weeks)) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        post_holiday_vol_new.rename(columns = {'Predictions_rf':'Predictions_rf_np1'}, inplace = True)\n",
    "        # Product 2\n",
    "        post_holiday_vol_new_2 = Test_results[Test_results.iter == 2][(Test_results[Test_results.iter == 2].Week.isin(post_holiday_weeks)) & (Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        post_holiday_vol_new_2.rename(columns = {'Predictions_rf':'Predictions_rf_np2'}, inplace = True)\n",
    "\n",
    "        post_merge = post_holiday_vol_old.merge(post_holiday_vol_new).merge(post_holiday_vol_new_2)\n",
    "        post_merge = post_merge.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf_np1':'New Product 1','Predictions_rf_np2':'New Product 2'})\n",
    "\n",
    "        # Non holiday results\n",
    "        no_holiday_vol_old = Test_results[Test_results.iter == 1][~(Test_results[Test_results.iter == 1].Week.isin(post_holiday_weeks)) & ~((Test_results[Test_results.iter == 1].Week.isin(during_holiday_weeks))) & ~(Test_results[Test_results.iter == 1].Week.isin(pre_holiday_weeks)) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "        # Product 1\n",
    "        no_holiday_vol_new = Test_results[Test_results.iter == 1][~(Test_results[Test_results.iter == 1].Week.isin(post_holiday_weeks)) & ~(Test_results[Test_results.iter == 1].Week.isin(during_holiday_weeks)) & ~(Test_results[Test_results.iter == 1].Week.isin(pre_holiday_weeks)) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        no_holiday_vol_new.rename(columns = {'Predictions_rf':'Predictions_rf_np1'}, inplace = True)\n",
    "        # Product 2\n",
    "        no_holiday_vol_new_2 = Test_results[Test_results.iter == 2][~(Test_results[Test_results.iter == 2].Week.isin(post_holiday_weeks)) & ~(Test_results[Test_results.iter == 2].Week.isin(during_holiday_weeks)) & ~(Test_results[Test_results.iter == 2].Week.isin(pre_holiday_weeks)) & (Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        no_holiday_vol_new_2.rename(columns = {'Predictions_rf':'Predictions_rf_np2'}, inplace = True)\n",
    "\n",
    "        no_merge = no_holiday_vol_old.merge(no_holiday_vol_new).merge(no_holiday_vol_new_2)\n",
    "        no_merge = no_merge.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf_np1':'New Product 1','Predictions_rf_np2':'New Product 2'})\n",
    "\n",
    "        # All holiday merge\n",
    "        pre_merge['Holiday'] = 'Pre'\n",
    "        during_merge['Holiday'] = 'During'\n",
    "        post_merge['Holiday'] = 'Post'\n",
    "\n",
    "        no_merge['Holiday'] = 'No Holiday'\n",
    "        hol_merge_1 = pre_merge.append(during_merge, ignore_index = True)\n",
    "       \n",
    "        hol_merge_11 = hol_merge_1.append(post_merge, ignore_index = True)\n",
    "\n",
    "        hol_merge_2 = hol_merge_11.append(no_merge, ignore_index = True)\n",
    "        hol_merge_2 = hol_merge_2.sort_values(['Region'])\n",
    "\n",
    "\n",
    "        ######################## HOLIDAY REVENUE #############################\n",
    "        # Pre Results    \n",
    "        pre_holiday_rev_old = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Week.isin(pre_holiday_weeks))&(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "        pre_holiday_rev_new = Test_results[Test_results.iter == 1][Test_results[Test_results.iter == 1].Week.isin(pre_holiday_weeks) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        pre_holiday_rev_new.rename(columns = {'Sales_derived':'Sales_derived_np1'}, inplace = True)\n",
    "        pre_holiday_rev_new_2 = Test_results[Test_results.iter == 2][Test_results[Test_results.iter == 2].Week.isin(pre_holiday_weeks) & (Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        pre_holiday_rev_new_2.rename(columns = {'Sales_derived':'Sales_derived_np2'}, inplace = True)\n",
    "\n",
    "        pre_merge_rev = pre_holiday_rev_old.merge(pre_holiday_rev_new).merge(pre_holiday_rev_new_2)\n",
    "        pre_merge_rev = pre_merge_rev.rename(columns = {'Sales_derived_np1':'New Product 1','Sales_derived_np2':'New Product 2','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        # During Results    \n",
    "        during_holiday_rev_old = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Week.isin(during_holiday_weeks))&(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "        during_holiday_rev_new = Test_results[Test_results.iter == 1][Test_results[Test_results.iter == 1].Week.isin(during_holiday_weeks) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        during_holiday_rev_new.rename(columns = {'Sales_derived':'Sales_derived_np1'}, inplace = True)\n",
    "        during_holiday_rev_new_2 = Test_results[Test_results.iter == 2][Test_results[Test_results.iter == 2].Week.isin(during_holiday_weeks) & (Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        during_holiday_rev_new_2.rename(columns = {'Sales_derived':'Sales_derived_np2'}, inplace = True)\n",
    "\n",
    "        during_merge_rev = during_holiday_rev_old.merge(during_holiday_rev_new).merge(during_holiday_rev_new_2)\n",
    "        during_merge_rev = during_merge_rev.rename(columns = {'Sales_derived_np1':'New Product 1','Sales_derived_np2':'New Product 2','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        # Post Results    \n",
    "        post_holiday_rev_old = Test_results[Test_results.iter == 1][(Test_results[Test_results.iter == 1].Week.isin(post_holiday_weeks))&(Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "        post_holiday_rev_new = Test_results[Test_results.iter == 1][Test_results[Test_results.iter == 1].Week.isin(post_holiday_weeks) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        post_holiday_rev_new.rename(columns = {'Sales_derived':'Sales_derived_np1'}, inplace = True)\n",
    "        post_holiday_rev_new_2 = Test_results[Test_results.iter == 2][Test_results[Test_results.iter == 2].Week.isin(post_holiday_weeks) & (Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        post_holiday_rev_new_2.rename(columns = {'Sales_derived':'Sales_derived_np2'}, inplace = True)\n",
    "\n",
    "        post_merge_rev = post_holiday_rev_old.merge(post_holiday_rev_new).merge(post_holiday_rev_new_2)\n",
    "        post_merge_rev = post_merge_rev.rename(columns = {'Sales_derived_np1':'New Product 1','Sales_derived_np2':'New Product 2','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        # Non holiday results\n",
    "        no_holiday_rev_old = Test_results[Test_results.iter == 1][~(Test_results[Test_results.iter == 1].Week.isin(post_holiday_weeks)) & ~((Test_results[Test_results.iter == 1].Week.isin(during_holiday_weeks))) & ~(Test_results[Test_results.iter == 1].Week.isin(pre_holiday_weeks)) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "        no_holiday_rev_new = Test_results[Test_results.iter == 1][~(Test_results[Test_results.iter == 1].Week.isin(post_holiday_weeks)) & ~(Test_results[Test_results.iter == 1].Week.isin(during_holiday_weeks)) & ~(Test_results[Test_results.iter == 1].Week.isin(pre_holiday_weeks)) & (Test_results[Test_results.iter == 1].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 1].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        no_holiday_rev_new.rename(columns = {'Sales_derived':'Sales_derived_np1'}, inplace = True)\n",
    "\n",
    "        no_holiday_rev_new_2 = Test_results[Test_results.iter == 2][~(Test_results[Test_results.iter == 2].Week.isin(post_holiday_weeks)) & ~(Test_results[Test_results.iter == 2].Week.isin(during_holiday_weeks)) & ~(Test_results[Test_results.iter == 2].Week.isin(pre_holiday_weeks)) & (Test_results[Test_results.iter == 2].Test_period >= start_week.children[8].value) & (Test_results[Test_results.iter == 2].Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        no_holiday_rev_new_2.rename(columns = {'Sales_derived':'Sales_derived_np2'}, inplace = True)\n",
    "\n",
    "        no_merge_rev = no_holiday_rev_old.merge(no_holiday_rev_new).merge(no_holiday_rev_new_2)\n",
    "        no_merge_rev = no_merge_rev.rename(columns = {'Sales_derived_np1':'New Product 1','Sales_derived_np2':'New Product 2','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        # All holiday merge\n",
    "        pre_merge_rev['Holiday'] = 'Pre'\n",
    "        during_merge_rev['Holiday'] = 'During'\n",
    "        post_merge_rev['Holiday'] = 'Post'\n",
    "       \n",
    "        no_merge_rev['Holiday'] = 'No Holiday'\n",
    "        hol_merge_1_rev = pre_merge_rev.append(during_merge_rev, ignore_index = True)\n",
    "       \n",
    "        hol_merge_2_rev_1 = hol_merge_1_rev.append(post_merge_rev, ignore_index = True)\n",
    "       \n",
    "        hol_merge_2_rev = hol_merge_2_rev_1.append(no_merge_rev, ignore_index = True)\n",
    "        hol_merge_2_rev = hol_merge_2_rev.sort_values(['Region'])\n",
    "        \n",
    "        ######################## CR ACROSS AFFECTED PRODUCTS #############################9\n",
    "   \n",
    "    # In case only 1 new product is selected\n",
    "    else:\n",
    "\n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':            \n",
    "            old_reg = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            new_reg = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()        \n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            old_reg = Test_results_exist2.groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            new_reg = Test_results.groupby('Region').Predictions_rf.sum().reset_index()   \n",
    "                \n",
    "\n",
    "        # merge w.r.t. region\n",
    "        tot_vol = old_reg.merge(new_reg)\n",
    "        tot_vol = tot_vol.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf' : 'New'})\n",
    "\n",
    "        ########### REGION REVENUE ###########\n",
    "\n",
    "        # Results    \n",
    "        \n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':            \n",
    "            old_reg_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            new_reg_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            old_reg_revenue = Test_results_exist2.groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            new_reg_revenue = Test_results.groupby('Region').Sales_derived.sum().reset_index()\n",
    "            \n",
    "            \n",
    "        merge_data_region = new_reg_revenue.merge(old_reg_revenue, on = 'Region', how = 'outer').sort_values('Region')\n",
    "        merge_data_region = merge_data_region.rename(columns = {'Sales_derived':'New','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        ########### Channel VOLUME ###########\n",
    "\n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':            \n",
    "\n",
    "            # Results    \n",
    "            old_chan_volume = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby(['Channel','Region'])['Predictions_rf_exist'].sum().reset_index()\n",
    "            new_chan_volume = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby(['Channel','Region']).Predictions_rf.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            old_chan_volume = Test_results_exist2.groupby(['Channel','Region'])['Predictions_rf_exist'].sum().reset_index()\n",
    "            new_chan_volume = Test_results.groupby(['Channel','Region']).Predictions_rf.sum().reset_index()\n",
    "\n",
    "        merge_data_channel = new_chan_volume.merge(old_chan_volume, on = ['Region','Channel'], how = 'outer').sort_values('Channel')\n",
    "        merge_data_channel = merge_data_channel.rename(columns = {'Predictions_rf':'New','Predictions_rf_exist':'Existing'})\n",
    "\n",
    "        ########### Channel REVENUE ###########\n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':            \n",
    "            # Results    \n",
    "            old_chan_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby(['Channel','Region'])['Sales_derived_exist'].sum().reset_index()\n",
    "            new_chan_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby(['Channel','Region']).Sales_derived.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            old_chan_revenue = Test_results_exist2.groupby(['Channel','Region'])['Sales_derived_exist'].sum().reset_index()\n",
    "            new_chan_revenue = Test_results.groupby(['Channel','Region']).Sales_derived.sum().reset_index()\n",
    "\n",
    "        merge_data_channel_revenue = new_chan_revenue.merge(old_chan_revenue, on = ['Region','Channel'], how = 'outer').sort_values('Channel')\n",
    "        merge_data_channel_revenue = merge_data_channel_revenue.rename(columns = {'Sales_derived':'New','Sales_derived_exist':'Existing'})\n",
    "        \n",
    "        ########### QUARTER VOLUME ###########\n",
    "\n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':            \n",
    "            # Results    \n",
    "            ref_product_quarter = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby(['Test_period','Region'])['Predictions_rf_exist'].sum().reset_index()\n",
    "            new_product_quarter = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby(['Test_period','Region']).Predictions_rf.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            ref_product_quarter = Test_results_exist2.groupby(['Test_period','Region'])['Predictions_rf_exist'].sum().reset_index()\n",
    "            new_product_quarter = Test_results.groupby(['Test_period','Region']).Predictions_rf.sum().reset_index()\n",
    "\n",
    "        merge_data_quarter = new_product_quarter.merge(ref_product_quarter, on = ['Region','Test_period'], how = 'outer').sort_values(['Region','Test_period'])\n",
    "        merge_data_quarter = merge_data_quarter.rename(columns = {'Predictions_rf':'New','Predictions_rf_exist':'Existing'})\n",
    "\n",
    "        merge_data_quarter = merge_data_quarter.rename(columns = {'Test_period':'Year Quarter'})\n",
    "\n",
    "        ########### QUARTER REVENUE ###########\n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':            \n",
    "            # Results    \n",
    "            ref_product_quarter_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby(['Test_period','Region'])['Sales_derived_exist'].sum().reset_index()\n",
    "            new_product_quarter_revenue = Test_results[(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby(['Test_period','Region']).Sales_derived.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            ref_product_quarter_revenue = Test_results_exist2.groupby(['Test_period','Region'])['Sales_derived_exist'].sum().reset_index()\n",
    "            new_product_quarter_revenue = Test_results.groupby(['Test_period','Region']).Sales_derived.sum().reset_index()\n",
    "\n",
    "        merge_data_quarter_revenue = new_product_quarter_revenue.merge(ref_product_quarter_revenue, on = ['Region','Test_period'], how = 'outer').sort_values(['Region','Test_period'])\n",
    "        merge_data_quarter_revenue = merge_data_quarter_revenue.rename(columns = {'Sales_derived':'New','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        merge_data_quarter_revenue = merge_data_quarter_revenue.rename(columns = {'Test_period':'Year Quarter'})\n",
    "\n",
    "\n",
    "        ############################# HOLIDAY VOLUME #############################\n",
    "\n",
    "        # Holiday weeks\n",
    "        holiday_weeks = Volume_dataset[(Volume_dataset.Product == product_fil) & (Volume_dataset.Week >= '2019-01') & (Volume_dataset.Week <= '2020-52')][['Week','Pre.Holiday.Week', 'Holiday.Week', 'Post.Holiday.Week']].drop_duplicates().sort_values('Week')\n",
    "        pre_holiday_weeks = list(holiday_weeks[holiday_weeks['Pre.Holiday.Week'] == 1].Week)\n",
    "        during_holiday_weeks = list(holiday_weeks[holiday_weeks['Holiday.Week'] == 1].Week)\n",
    "        post_holiday_weeks = list(holiday_weeks[holiday_weeks['Post.Holiday.Week'] == 1].Week)\n",
    "\n",
    "        if (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            # Holiday weeks of uploaded data\n",
    "            holiday_weeks_exist = upload_data[['Week','Pre.Holiday.Week', 'Holiday.Week', 'Post.Holiday.Week']].drop_duplicates().sort_values('Week')\n",
    "            pre_holiday_weeks_exist = list(holiday_weeks_exist[holiday_weeks_exist['Pre.Holiday.Week'] == 1].Week)\n",
    "            during_holiday_weeks_exist = list(holiday_weeks_exist[holiday_weeks_exist['Holiday.Week'] == 1].Week)\n",
    "            post_holiday_weeks_exist = list(holiday_weeks_exist[holiday_weeks_exist['Post.Holiday.Week'] == 1].Week)\n",
    "\n",
    "        # Pre Results    \n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':\n",
    "            pre_holiday_vol_old = Test_results[(Test_results.Week.isin(pre_holiday_weeks))&(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            pre_holiday_vol_new = Test_results[(Test_results.Week.isin(pre_holiday_weeks)) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            pre_holiday_vol_old = Test_results_exist2[(Test_results_exist2.Week.isin(pre_holiday_weeks))].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            pre_holiday_vol_new = Test_results[(Test_results.Week.isin(pre_holiday_weeks_exist))].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "\n",
    "        pre_merge = pre_holiday_vol_old.merge(pre_holiday_vol_new)\n",
    "        pre_merge = pre_merge.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf':'New'})\n",
    "\n",
    "        # During Results   \n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':          \n",
    "            during_holiday_vol_old = Test_results[(Test_results.Week.isin(during_holiday_weeks))&(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            during_holiday_vol_new = Test_results[(Test_results.Week.isin(during_holiday_weeks)) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            during_holiday_vol_old = Test_results_exist2[(Test_results_exist2.Week.isin(during_holiday_weeks))].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            during_holiday_vol_new = Test_results[(Test_results.Week.isin(during_holiday_weeks_exist))].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "\n",
    "        during_merge = during_holiday_vol_old.merge(during_holiday_vol_new)\n",
    "        during_merge = during_merge.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf':'New'})\n",
    "\n",
    "        # Post Results   \n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':           \n",
    "            post_holiday_vol_old = Test_results[(Test_results.Week.isin(post_holiday_weeks))&(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            post_holiday_vol_new = Test_results[(Test_results.Week.isin(post_holiday_weeks)) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            post_holiday_vol_old = Test_results_exist2[(Test_results_exist2.Week.isin(post_holiday_weeks))].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            post_holiday_vol_new = Test_results[(Test_results.Week.isin(post_holiday_weeks_exist))].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "\n",
    "\n",
    "        post_merge = post_holiday_vol_old.merge(post_holiday_vol_new)\n",
    "        post_merge = post_merge.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf':'New'})\n",
    "\n",
    "        # Non holiday results\n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':         \n",
    "            no_holiday_vol_old = Test_results[~(Test_results.Week.isin(post_holiday_weeks)) & ~(Test_results.Week.isin(during_holiday_weeks)) & ~(Test_results.Week.isin(pre_holiday_weeks)) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            no_holiday_vol_new = Test_results[~(Test_results.Week.isin(post_holiday_weeks)) & ~(Test_results.Week.isin(during_holiday_weeks)) & ~(Test_results.Week.isin(pre_holiday_weeks)) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            no_holiday_vol_old = Test_results_exist2[~(Test_results_exist2.Week.isin(post_holiday_weeks)) & ~(Test_results_exist2.Week.isin(during_holiday_weeks)) & ~(Test_results_exist2.Week.isin(pre_holiday_weeks))].groupby('Region')['Predictions_rf_exist'].sum().reset_index()\n",
    "            no_holiday_vol_new = Test_results[~(Test_results.Week.isin(post_holiday_weeks_exist)) & ~(Test_results.Week.isin(during_holiday_weeks_exist)) & ~(Test_results.Week.isin(pre_holiday_weeks_exist))].groupby('Region').Predictions_rf.sum().reset_index()\n",
    "\n",
    "        no_merge = no_holiday_vol_old.merge(no_holiday_vol_new)\n",
    "        no_merge = no_merge.rename(columns = {'Predictions_rf_exist':'Existing','Predictions_rf':'New'})\n",
    "\n",
    "        # All holiday merge\n",
    "        pre_merge['Holiday'] = 'Pre'\n",
    "        during_merge['Holiday'] = 'During'\n",
    "        post_merge['Holiday'] = 'Post'\n",
    "       \n",
    "        no_merge['Holiday'] = 'No Holiday'\n",
    "        hol_merge_1 = pre_merge.append(during_merge, ignore_index = True)\n",
    "       \n",
    "        hol_merge_11 = hol_merge_1.append(post_merge, ignore_index = True)\n",
    "       \n",
    "        hol_merge_2 = hol_merge_11.append(no_merge, ignore_index = True)\n",
    "        hol_merge_2 = hol_merge_2.sort_values(['Region'])\n",
    "\n",
    "\n",
    "        ######################## HOLIDAY REVENUE #############################\n",
    "        # Pre Results    \n",
    "        # In case the user doesn't upload the dat\n",
    "        if toggle_upload.value != 'Yes':      \n",
    "\n",
    "            pre_holiday_rev_old = Test_results[(Test_results.Week.isin(pre_holiday_weeks))&(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            pre_holiday_rev_new = Test_results[Test_results.Week.isin(pre_holiday_weeks) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            pre_holiday_rev_old = Test_results_exist2[Test_results_exist2.Week.isin(pre_holiday_weeks)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            pre_holiday_rev_new = Test_results[Test_results.Week.isin(pre_holiday_weeks_exist)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "\n",
    "        pre_merge_rev = pre_holiday_rev_old.merge(pre_holiday_rev_new)\n",
    "        pre_merge_rev = pre_merge_rev.rename(columns = {'Sales_derived':'New','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        # During Results  \n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':\n",
    "            during_holiday_rev_old = Test_results[(Test_results.Week.isin(during_holiday_weeks))&(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            during_holiday_rev_new = Test_results[Test_results.Week.isin(during_holiday_weeks) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            during_holiday_rev_old = Test_results_exist2[Test_results_exist2.Week.isin(during_holiday_weeks)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            during_holiday_rev_new = Test_results[Test_results.Week.isin(during_holiday_weeks_exist)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "\n",
    "        during_merge_rev = during_holiday_rev_old.merge(during_holiday_rev_new)\n",
    "\n",
    "        during_merge_rev = during_merge_rev.rename(columns = {'Sales_derived':'New','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        # Post Results\n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':        \n",
    "            post_holiday_rev_old = Test_results[(Test_results.Week.isin(post_holiday_weeks))&(Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            post_holiday_rev_new = Test_results[(Test_results.Week.isin(post_holiday_weeks)) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "            post_holiday_rev_old = Test_results_exist2[(Test_results_exist2.Week.isin(post_holiday_weeks))].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            post_holiday_rev_new = Test_results[Test_results.Week.isin(post_holiday_weeks_exist)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "\n",
    "        post_merge_rev = post_holiday_rev_old.merge(post_holiday_rev_new)\n",
    "        post_merge_rev = post_merge_rev.rename(columns = {'Sales_derived':'New','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        # Non holiday results\n",
    "        # In case the user doesn't upload the data\n",
    "        if toggle_upload.value != 'Yes':        \n",
    "            no_holiday_rev_old = Test_results[~(Test_results.Week.isin(post_holiday_weeks)) & ~((Test_results.Week.isin(during_holiday_weeks))) & ~(Test_results.Week.isin(pre_holiday_weeks)) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            no_holiday_rev_new = Test_results[~(Test_results.Week.isin(post_holiday_weeks)) & ~(Test_results.Week.isin(during_holiday_weeks)) & ~(Test_results.Week.isin(pre_holiday_weeks)) & (Test_results.Test_period >= start_week.children[8].value) & (Test_results.Test_period <= end_week.children[8].value)].groupby('Region').Sales_derived.sum().reset_index()\n",
    "        elif (toggle_upload.value == 'Yes') & (len(uploader.value)) != 0:\n",
    "\n",
    "            no_holiday_rev_old = Test_results_exist2[~(Test_results_exist2.Week.isin(post_holiday_weeks)) & ~((Test_results_exist2.Week.isin(during_holiday_weeks))) & ~(Test_results_exist2.Week.isin(pre_holiday_weeks))].groupby('Region')['Sales_derived_exist'].sum().reset_index()\n",
    "            no_holiday_rev_new = Test_results[~(Test_results.Week.isin(post_holiday_weeks_exist)) & ~(Test_results.Week.isin(during_holiday_weeks_exist)) & ~(Test_results.Week.isin(pre_holiday_weeks_exist))].groupby('Region').Sales_derived.sum().reset_index()\n",
    "\n",
    "        no_merge_rev = no_holiday_rev_old.merge(no_holiday_rev_new)\n",
    "        no_merge_rev = no_merge_rev.rename(columns = {'Sales_derived':'New','Sales_derived_exist':'Existing'})\n",
    "\n",
    "        # All holiday merge\n",
    "        pre_merge_rev['Holiday'] = 'Pre'\n",
    "        during_merge_rev['Holiday'] = 'During'\n",
    "        post_merge_rev['Holiday'] = 'Post'\n",
    "        \n",
    "        no_merge_rev['Holiday'] = 'No Holiday'\n",
    "        hol_merge_1_rev = pre_merge_rev.append(during_merge_rev, ignore_index = True)\n",
    "       \n",
    "        hol_merge_2_rev_1 = hol_merge_1_rev.append(post_merge_rev, ignore_index = True)\n",
    "       \n",
    "        hol_merge_2_rev = hol_merge_2_rev_1.append(no_merge_rev, ignore_index = True)\n",
    "        hol_merge_2_rev = hol_merge_2_rev.sort_values(['Region'])\n",
    "                        \n",
    "else:    \n",
    "    clear_output()\n",
    "    print('Product is not present in that Region/Channel for selected period')   \n",
    "\n",
    "    \n",
    "##############################################################################################################################\n",
    "#  Plotting Charts\n",
    "##############################################################################################################################\n",
    "\n",
    "# In case the user selects 2 new product for simulation    \n",
    "if (toggle_np2.value == 'Yes') & (toggle_upload.value != 'Yes'):\n",
    "\n",
    "    def plotit(Region):\n",
    "\n",
    "        if region_drop.value != 'All':\n",
    "            Region_List_filter = [region_drop.value]\n",
    "        else:\n",
    "            Region_List_filter = Volume_dataset_all_reg.Region.unique()\n",
    "        \n",
    "        tot_vol2 = tot_vol[tot_vol.Region.isin(Region_List_filter)]\n",
    "        merge_data_region2 = merge_data_region[merge_data_region.Region.isin(Region_List_filter)]\n",
    "        merge_data_channel2 = merge_data_channel[merge_data_channel.Region.isin(Region_List_filter)]\n",
    "        merge_data_channel_revenue2 = merge_data_channel_revenue[merge_data_channel_revenue.Region.isin(Region_List_filter)]\n",
    "        merge_data_quarter2 = merge_data_quarter[merge_data_quarter.Region.isin(Region_List_filter)]  \n",
    "        merge_data_quarter_revenue2 = merge_data_quarter_revenue[merge_data_quarter_revenue.Region.isin(Region_List_filter)]\n",
    "        hol_merge_2_copy = hol_merge_2[hol_merge_2.Region.isin(Region_List_filter)]    \n",
    "        hol_merge_2_rev_copy = hol_merge_2_rev[hol_merge_2_rev.Region.isin(Region_List_filter)] \n",
    "    \n",
    "        cr_dist_prod_copy = iteration_dataset_all_v3[iteration_dataset_all_v3.Region.isin(Region_List_filter)].copy()\n",
    "        if len(Region_List_filter) > 1:\n",
    "            cr_dist_prod_copy = iteration_dataset_all_v3[iteration_dataset_all_v3.Region.isin(data_all.Region.unique())].groupby(['Existing_Product','iter']).agg({'Existing Product Vol Without':'sum','Updated_Can_Vol':'sum'}).reset_index()\n",
    "            cr_dist_prod_copy['% CR'] = cr_dist_prod_copy['Updated_Can_Vol']/cr_dist_prod_copy['Existing Product Vol Without']\n",
    "    \n",
    "\n",
    "        tot_vol2['Existing'] = tot_vol2['Existing'].astype('int64')\n",
    "        tot_vol2['Existing_formatted'] = tot_vol2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        tot_vol2['New Product 1'] = tot_vol2['New Product 1'].astype('int64')\n",
    "        tot_vol2['New Product 2'] = tot_vol2['New Product 2'].astype('int64')\n",
    "        tot_vol2['New_formatted 1'] = tot_vol2['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        tot_vol2['New_formatted 2'] = tot_vol2['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        merge_data_region2['Existing'] = merge_data_region2['Existing'].astype('int64')\n",
    "        merge_data_region2['Existing_formatted'] = merge_data_region2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_region2['New Product 1'] = merge_data_region2['New Product 1'].astype('int64')\n",
    "        merge_data_region2['New Product 2'] = merge_data_region2['New Product 2'].astype('int64')\n",
    "        merge_data_region2['New_formatted 1'] = merge_data_region2['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_region2['New_formatted 2'] = merge_data_region2['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        merge_data_channel2 = merge_data_channel2.groupby('Channel').sum().reset_index()\n",
    "        merge_data_channel2['Existing'] = merge_data_channel2['Existing'].astype('int64')\n",
    "        merge_data_channel2['Existing_formatted'] = merge_data_channel2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_channel2['New Product 1'] = merge_data_channel2['New Product 1'].astype('int64')\n",
    "        merge_data_channel2['New Product 2'] = merge_data_channel2['New Product 2'].astype('int64')\n",
    "        merge_data_channel2['New_formatted 1'] = merge_data_channel2['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_channel2['New_formatted 2'] = merge_data_channel2['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        \n",
    "        merge_data_channel_revenue2 = merge_data_channel_revenue2.groupby('Channel').sum().reset_index()\n",
    "        merge_data_channel_revenue2['Existing'] = merge_data_channel_revenue2['Existing'].astype('int64')\n",
    "        merge_data_channel_revenue2['Existing_formatted'] = merge_data_channel_revenue2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_channel_revenue2['New Product 1'] = merge_data_channel_revenue2['New Product 1'].astype('int64')\n",
    "        merge_data_channel_revenue2['New Product 2'] = merge_data_channel_revenue2['New Product 2'].astype('int64')\n",
    "        merge_data_channel_revenue2['New_formatted 1'] = merge_data_channel_revenue2['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_channel_revenue2['New_formatted 2'] = merge_data_channel_revenue2['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        \n",
    "        merge_data_quarter2 = merge_data_quarter2.groupby('Year Quarter').sum().reset_index()\n",
    "        merge_data_quarter2['Existing'] = merge_data_quarter2['Existing'].astype('int64')\n",
    "        merge_data_quarter2['Existing_formatted'] = merge_data_quarter2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_quarter2['New Product 1'] = merge_data_quarter2['New Product 1'].astype('int64')\n",
    "        merge_data_quarter2['New Product 2'] = merge_data_quarter2['New Product 2'].astype('int64')\n",
    "        merge_data_quarter2['New_formatted 1'] = merge_data_quarter2['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_quarter2['New_formatted 2'] = merge_data_quarter2['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        merge_data_quarter_revenue2 = merge_data_quarter_revenue2.groupby('Year Quarter').sum().reset_index()\n",
    "        merge_data_quarter_revenue2['Existing'] = merge_data_quarter_revenue2['Existing'].astype('int64')\n",
    "        merge_data_quarter_revenue2['Existing_formatted'] = merge_data_quarter_revenue2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_quarter_revenue2['New Product 1'] = merge_data_quarter_revenue2['New Product 1'].astype('int64')\n",
    "        merge_data_quarter_revenue2['New Product 2'] = merge_data_quarter_revenue2['New Product 2'].astype('int64')\n",
    "        merge_data_quarter_revenue2['New_formatted 1'] = merge_data_quarter_revenue2['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_quarter_revenue2['New_formatted 2'] = merge_data_quarter_revenue2['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        hol_merge_2_copy = hol_merge_2_copy.groupby('Holiday').sum().reset_index()\n",
    "        hol_merge_2_copy['Existing'] = hol_merge_2_copy['Existing'].astype('int64')\n",
    "        hol_merge_2_copy['Existing_formatted'] = hol_merge_2_copy['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_copy['New Product 1'] = hol_merge_2_copy['New Product 1'].astype('int64')\n",
    "        hol_merge_2_copy['New Product 2'] = hol_merge_2_copy['New Product 2'].astype('int64')\n",
    "        hol_merge_2_copy['New_formatted 1'] = hol_merge_2_copy['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_copy['New_formatted 2'] = hol_merge_2_copy['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_copy = hol_merge_2_copy.reindex([3,0,2,1])\n",
    "\n",
    "        hol_merge_2_rev_copy = hol_merge_2_rev_copy.groupby('Holiday').sum().reset_index()\n",
    "        hol_merge_2_rev_copy['Existing'] = hol_merge_2_rev_copy['Existing'].astype('int64')\n",
    "        hol_merge_2_rev_copy['Existing_formatted'] = hol_merge_2_rev_copy['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_rev_copy['New Product 1'] = hol_merge_2_rev_copy['New Product 1'].astype('int64')\n",
    "        hol_merge_2_rev_copy['New Product 2'] = hol_merge_2_rev_copy['New Product 2'].astype('int64')\n",
    "        hol_merge_2_rev_copy['New_formatted 1'] = hol_merge_2_rev_copy['New Product 1'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_rev_copy['New_formatted 2'] = hol_merge_2_rev_copy['New Product 2'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_rev_copy = hol_merge_2_rev_copy.reindex([3,0,2,1])\n",
    "\n",
    "        # CR distribution across affected products ## ITER 1##\n",
    "        cr_dist_prod_copy_1 = cr_dist_prod_copy[cr_dist_prod_copy.iter == 1]\n",
    "        cr_dist_prod_copy_1 = cr_dist_prod_copy_1.sort_values('Updated_Can_Vol').reset_index(drop=True)\n",
    "        cr_dist_prod_copy_1['% CR'] = 100*cr_dist_prod_copy_1['% CR']\n",
    "        cr_dist_prod_copy_1['% CR'] = cr_dist_prod_copy_1['% CR'].round(1)\n",
    "        cr_dist_prod_copy_1['% CR'] = -1*cr_dist_prod_copy_1['% CR'] \n",
    "        cr_dist_prod_copy_1['Updated_Can_Vol'] = -1*cr_dist_prod_copy_1['Updated_Can_Vol']\n",
    "        cr_dist_prod_copy_1['CR_Vol_formatted'] = cr_dist_prod_copy_1['Updated_Can_Vol'].astype('int').apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        cr_dist_prod_copy_2 = cr_dist_prod_copy[cr_dist_prod_copy.iter == 2]\n",
    "        cr_dist_prod_copy_2 = cr_dist_prod_copy_2.sort_values('Updated_Can_Vol').reset_index(drop=True)\n",
    "        cr_dist_prod_copy_2['% CR'] = 100*cr_dist_prod_copy_2['% CR']\n",
    "        cr_dist_prod_copy_2['% CR'] = cr_dist_prod_copy_2['% CR'].round(1)\n",
    "        cr_dist_prod_copy_2['% CR'] = -1*cr_dist_prod_copy_2['% CR'] \n",
    "        cr_dist_prod_copy_2['Updated_Can_Vol'] = -1*cr_dist_prod_copy_2['Updated_Can_Vol']\n",
    "        cr_dist_prod_copy_2['CR_Vol_formatted'] = cr_dist_prod_copy_2['Updated_Can_Vol'].astype('int').apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        \n",
    "        \n",
    "        # Plot it (only if there's data to plot)\n",
    "        if len(tot_vol2) > 0:        \n",
    "            ############# REGION PLOTS ##################\n",
    "            if region_drop.value == 'All':\n",
    "                width = 0.78\n",
    "            else:\n",
    "                width = 300\n",
    "                \n",
    "            x1 = np.arange(len(list(tot_vol2['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(merge_data_region2['Region'])))  # the label locations\n",
    "            width = width # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/3, list(tot_vol2['Existing']), width = width/3, label='Existing')\n",
    "            rects2 = ax1.bar(x1, list(tot_vol2['New Product 1']), width = width/3, label='New Product 1')\n",
    "            rects2_2 = ax1.bar(x1 + width/3, list(tot_vol2['New Product 2']), width = width/3, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Region)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(tot_vol2['Region']), fontsize=20)\n",
    "            ax1.legend(loc = 'lower right')\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(tot_vol2['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(tot_vol2['New_formatted 1']))\n",
    "            ax1.bar_label(rects2_2, padding=5, labels=list(tot_vol2['New_formatted 2']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/3, list(merge_data_region2['Existing']), width =width/3, label='Existing')\n",
    "            rects4 = ax2.bar(x2, list(merge_data_region2['New Product 1']), width = width/3, label='New Product 1')\n",
    "            rects4_2 = ax2.bar(x2 + width/3, list(merge_data_region2['New Product 2']), width = width/3, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Region)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(merge_data_region2['Region']), fontsize=20)\n",
    "            ax2.legend(loc = 'lower right')\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(merge_data_region2['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(merge_data_region2['New_formatted 1']))\n",
    "            ax2.bar_label(rects4_2, padding=5, labels=list(merge_data_region2['New_formatted 2']))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            ################## CHANNEL PLOTS ################\n",
    "            x1 = np.arange(len(list(merge_data_channel2['Channel'])))  # the label locations\n",
    "            x2 = np.arange(len(list(merge_data_channel_revenue2['Channel'])))  # the label locations\n",
    "            width = 0.78  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/3, list(merge_data_channel2['Existing']), width = width/3, label='Existing')\n",
    "            rects2 = ax1.bar(x1, list(merge_data_channel2['New Product 1']), width = width/3, label='New Product 1')\n",
    "            rects2_2 = ax1.bar(x1 + width/3, list(merge_data_channel2['New Product 2']), width = width/3, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Channel)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(merge_data_channel2['Channel']), fontsize=20)\n",
    "            ax1.legend(loc = 'lower left')\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(merge_data_channel2['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(merge_data_channel2['New_formatted 1']))\n",
    "            ax1.bar_label(rects2_2, padding=5, labels=list(merge_data_channel2['New_formatted 2']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/3, list(merge_data_channel_revenue2['Existing']), width = width/3, label='Existing')\n",
    "            rects4 = ax2.bar(x2, list(merge_data_channel_revenue2['New Product 1']), width =width/3, label='New Product 1')\n",
    "            rects4_2 = ax2.bar(x2 + width/3, list(merge_data_channel_revenue2['New Product 2']), width = width/3, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Channel)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(merge_data_channel_revenue2['Channel']), fontsize=20)\n",
    "            ax2.legend(loc = 'lower left')\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(merge_data_channel_revenue2['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(merge_data_channel_revenue2['New_formatted 1']))\n",
    "            ax2.bar_label(rects4_2, padding=5, labels=list(merge_data_channel_revenue2['New_formatted 2']))\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "            ################## QUARTER PLOTS ################\n",
    "            x1 = np.arange(len(list(merge_data_quarter2['Year Quarter'])))  # the label locations\n",
    "            x2 = np.arange(len(list(merge_data_quarter_revenue2['Year Quarter'])))  # the label locations\n",
    "            width = 0.78  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/3, list(merge_data_quarter2['Existing']), width = width/3, label='Existing')\n",
    "            rects2 = ax1.bar(x1, list(merge_data_quarter2['New Product 1']), width = width/3, label='New Product 1')\n",
    "            rects2_2 = ax1.bar(x1 + width/3, list(merge_data_quarter2['New Product 2']), width = width/3, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (All Quarters)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(merge_data_quarter2['Year Quarter']), fontsize=20)\n",
    "            ax1.legend(loc = 'lower right')\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(merge_data_quarter2['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(merge_data_quarter2['New_formatted 1']))\n",
    "            ax1.bar_label(rects2_2, padding=5, labels=list(merge_data_quarter2['New_formatted 2']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/3, list(merge_data_quarter_revenue2['Existing']), width = width/3, label='Existing')\n",
    "            rects4 = ax2.bar(x2, list(merge_data_quarter_revenue2['New Product 1']), width =width/3, label='New Product 1')\n",
    "            rects4_2 = ax2.bar(x2 + width/3, list(merge_data_quarter_revenue2['New Product 2']), width = width/3, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (All Quarters)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(merge_data_quarter_revenue2['Year Quarter']), fontsize=20)\n",
    "            ax2.legend(loc = 'lower right')\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(merge_data_quarter_revenue2['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(merge_data_quarter_revenue2['New_formatted 1']))\n",
    "            ax2.bar_label(rects4_2, padding=5, labels=list(merge_data_quarter_revenue2['New_formatted 2']))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            ################## HOLIDAY PLOTS ################\n",
    "            x1 = np.arange(len(list(hol_merge_2_copy['Holiday'])))  # the label locations\n",
    "            x2 = np.arange(len(list(hol_merge_2_rev_copy['Holiday'])))  # the label locations\n",
    "            width = 0.78  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/3, list(hol_merge_2_copy['Existing']), width = width/3, label='Existing')\n",
    "            rects2 = ax1.bar(x1, list(hol_merge_2_copy['New Product 1']), width = width/3, label='New Product 1')\n",
    "            rects2_2 = ax1.bar(x1 + width/3, list(hol_merge_2_copy['New Product 2']), width = width/3, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Holidays)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(hol_merge_2_copy['Holiday']), fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(hol_merge_2_copy['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(hol_merge_2_copy['New_formatted 1']))\n",
    "            ax1.bar_label(rects2_2, padding=5, labels=list(hol_merge_2_copy['New_formatted 2']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/3, list(hol_merge_2_rev_copy['Existing']), width = width/3, label='Existing')\n",
    "            rects4 = ax2.bar(x2, list(hol_merge_2_rev_copy['New Product 1']), width = width/3, label='New Product 1')\n",
    "            rects4_2 = ax2.bar(x2 + width/3, list(hol_merge_2_rev_copy['New Product 2']), width =width/3, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Holidays)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(hol_merge_2_rev_copy['Holiday']), fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(hol_merge_2_rev_copy['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(hol_merge_2_rev_copy['New_formatted 1']))\n",
    "            ax2.bar_label(rects4_2, padding=5, labels=list(hol_merge_2_rev_copy['New_formatted 2']))\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "            # When CR is present across region-channel ## ITER 1##\n",
    "            if len(kpi_df3) > 0:\n",
    "\n",
    "                ################## CR AFFECTED PRODUCTS PLOTS 2 ##################                \n",
    "                x1 = np.arange(len(list(cr_dist_prod_copy_1['Existing_Product'])))  # the label locations            \n",
    "                width = 0.35  # the width of the bars\n",
    "\n",
    "                fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize = (25,8))\n",
    "                ax2 = ax1.twinx()\n",
    "                rects1 = ax1.bar(x1, list(cr_dist_prod_copy_1['Updated_Can_Vol']), width,label='Cannibalised Units')\n",
    "                rects2 = ax2.plot(x1, list(cr_dist_prod_copy_1['% CR']), width, linewidth=4, color = '#F97306',label='% Cannibalisation Rate', marker='o')\n",
    "\n",
    "                # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "                ax1.set_ylabel('Cannibalised Units', fontsize=20)\n",
    "                ax2.set_ylabel('% Cannibalisation Rate', fontsize=20)\n",
    "                ax1.set_title('% Cannibalization Rate across affected products (New Product 1)', fontsize=25)\n",
    "                ax1.set_xticks(x1)\n",
    "                ax1.set_xticklabels(list(cr_dist_prod_copy_1['Existing_Product']), fontsize=15,rotation=45, ha='right')\n",
    "                ax1.legend(loc = 'upper left',bbox_to_anchor=(0.5, -0.7))\n",
    "                ax2.legend(loc = 'upper right',bbox_to_anchor=(0.5, -0.7))\n",
    "\n",
    "                ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "                ax1.bar_label(rects1, padding=5, labels=list(cr_dist_prod_copy_1['CR_Vol_formatted']),label_type = 'center')\n",
    "                \n",
    "                for i,j in cr_dist_prod_copy_1['% CR'].items():\n",
    "                    ax2.annotate(str(j) + \"%\", xy=(i, j))\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "            # When CR is present across region-channel ## ITER 2##\n",
    "            if len(kpi_df3) > 0:\n",
    "\n",
    "                ################## CR AFFECTED PRODUCTS PLOTS 2 ##################                \n",
    "                x1 = np.arange(len(list(cr_dist_prod_copy_2['Existing_Product'])))  # the label locations            \n",
    "                width = 0.35  # the width of the bars\n",
    "\n",
    "                fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize = (25,8))\n",
    "                ax2 = ax1.twinx()\n",
    "                rects1 = ax1.bar(x1, list(cr_dist_prod_copy_2['Updated_Can_Vol']), width,label='Cannibalised Units')\n",
    "                rects2 = ax2.plot(x1, list(cr_dist_prod_copy_2['% CR']), width, linewidth=4, color = '#F97306',label='% Cannibalisation Rate', marker='o')\n",
    "\n",
    "                # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "                ax1.set_ylabel('Cannibalised Units', fontsize=20)\n",
    "                ax2.set_ylabel('% Cannibalisation Rate', fontsize=20)\n",
    "                ax1.set_title('% Cannibalization Rate across affected products (New Product 2)', fontsize=25)\n",
    "                ax1.set_xticks(x1)\n",
    "                ax1.set_xticklabels(list(cr_dist_prod_copy_2['Existing_Product']), fontsize=15,rotation=45, ha='right')\n",
    "                ax1.legend(loc = 'upper left',bbox_to_anchor=(0.5, -0.7))\n",
    "                ax2.legend(loc = 'upper right',bbox_to_anchor=(0.5, -0.7))\n",
    "\n",
    "                ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "                ax1.bar_label(rects1, padding=5, labels=list(cr_dist_prod_copy_2['CR_Vol_formatted']), label_type = 'center')\n",
    "                \n",
    "                for i,j in cr_dist_prod_copy_2['% CR'].items():\n",
    "                    ax2.annotate(str(j) + \"%\", xy=(i, j))\n",
    "\n",
    "                plt.show()\n",
    "                print(\"Note1: Cannibalization is measured across the same category products\")\n",
    "                print(\"Note2: For ease of showing numbers, cannibalisation rate is shown positive, in reality it is negative cannibalisation\")\n",
    "\n",
    "        else:\n",
    "            print(\"No data to show for current selection\")\n",
    " \n",
    "# In case the user selects 1 new product for simulation    \n",
    "elif (toggle_np2.value == 'No'):\n",
    "        \n",
    "    def plotit(Region):\n",
    "\n",
    "        if region_drop.value != 'All':\n",
    "            Region_List_filter = [region_drop.value]\n",
    "        else:\n",
    "            Region_List_filter = Volume_dataset_all_reg.Region.unique()\n",
    "\n",
    "        tot_vol2 = tot_vol[tot_vol.Region.isin(Region_List_filter)]\n",
    "        merge_data_region2 = merge_data_region[merge_data_region.Region.isin(Region_List_filter)]\n",
    "        merge_data_channel2 = merge_data_channel[merge_data_channel.Region.isin(Region_List_filter)]\n",
    "        merge_data_channel_revenue2 = merge_data_channel_revenue[merge_data_channel_revenue.Region.isin(Region_List_filter)]\n",
    "        merge_data_quarter2 = merge_data_quarter[merge_data_quarter.Region.isin(Region_List_filter)]  \n",
    "        merge_data_quarter_revenue2 = merge_data_quarter_revenue[merge_data_quarter_revenue.Region.isin(Region_List_filter)]\n",
    "        hol_merge_2_copy = hol_merge_2[hol_merge_2.Region.isin(Region_List_filter)]    \n",
    "        hol_merge_2_rev_copy = hol_merge_2_rev[hol_merge_2_rev.Region.isin(Region_List_filter)] \n",
    "        \n",
    "        cr_dist_prod_copy = iteration_dataset_all_v3[iteration_dataset_all_v3.Region.isin(Region_List_filter)].copy()\n",
    "        if len(Region_List_filter) > 1:\n",
    "            cr_dist_prod_copy = iteration_dataset_all_v3[iteration_dataset_all_v3.Region.isin(data_all.Region.unique())].groupby(['Existing_Product','iter']).agg({'Existing Product Vol Without':'sum','Updated_Can_Vol':'sum'}).reset_index()\n",
    "            cr_dist_prod_copy['% CR'] = cr_dist_prod_copy['Updated_Can_Vol']/cr_dist_prod_copy['Existing Product Vol Without']\n",
    "        \n",
    "        \n",
    "        tot_vol2['Existing'] = tot_vol2['Existing'].astype('int64')\n",
    "        tot_vol2['Existing_formatted'] = tot_vol2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        tot_vol2['New'] = tot_vol2['New'].astype('int64')\n",
    "        tot_vol2['New_formatted'] = tot_vol2['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        merge_data_region2['Existing'] = merge_data_region2['Existing'].astype('int64')\n",
    "        merge_data_region2['Existing_formatted'] = merge_data_region2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_region2['New'] = merge_data_region2['New'].astype('int64')\n",
    "        merge_data_region2['New_formatted'] = merge_data_region2['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        merge_data_channel2 = merge_data_channel2.groupby('Channel').sum().reset_index()\n",
    "        merge_data_channel2['Existing'] = merge_data_channel2['Existing'].astype('int64')\n",
    "        merge_data_channel2['Existing_formatted'] = merge_data_channel2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_channel2['New'] = merge_data_channel2['New'].astype('int64')\n",
    "        merge_data_channel2['New_formatted'] = merge_data_channel2['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))        \n",
    "        \n",
    "        merge_data_channel_revenue2 = merge_data_channel_revenue2.groupby('Channel').sum().reset_index()\n",
    "        merge_data_channel_revenue2['Existing'] = merge_data_channel_revenue2['Existing'].astype('int64')\n",
    "        merge_data_channel_revenue2['Existing_formatted'] = merge_data_channel_revenue2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_channel_revenue2['New'] = merge_data_channel_revenue2['New'].astype('int64')\n",
    "        merge_data_channel_revenue2['New_formatted'] = merge_data_channel_revenue2['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        merge_data_quarter2 = merge_data_quarter2.groupby('Year Quarter').sum().reset_index()\n",
    "        merge_data_quarter2['Existing'] = merge_data_quarter2['Existing'].astype('int64')\n",
    "        merge_data_quarter2['Existing_formatted'] = merge_data_quarter2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_quarter2['New'] = merge_data_quarter2['New'].astype('int64')\n",
    "        merge_data_quarter2['New_formatted'] = merge_data_quarter2['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        merge_data_quarter_revenue2 = merge_data_quarter_revenue2.groupby('Year Quarter').sum().reset_index()\n",
    "        merge_data_quarter_revenue2['Existing'] = merge_data_quarter_revenue2['Existing'].astype('int64')\n",
    "        merge_data_quarter_revenue2['Existing_formatted'] = merge_data_quarter_revenue2['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        merge_data_quarter_revenue2['New'] = merge_data_quarter_revenue2['New'].astype('int64')\n",
    "        merge_data_quarter_revenue2['New_formatted'] = merge_data_quarter_revenue2['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        hol_merge_2_copy = hol_merge_2_copy.groupby('Holiday').sum().reset_index()\n",
    "        hol_merge_2_copy['Existing'] = hol_merge_2_copy['Existing'].astype('int64')\n",
    "        hol_merge_2_copy['Existing_formatted'] = hol_merge_2_copy['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_copy['New'] = hol_merge_2_copy['New'].astype('int64')\n",
    "        hol_merge_2_copy['New_formatted'] = hol_merge_2_copy['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_copy = hol_merge_2_copy.reindex([3,0,2,1])\n",
    "\n",
    "        hol_merge_2_rev_copy = hol_merge_2_rev_copy.groupby('Holiday').sum().reset_index()\n",
    "        hol_merge_2_rev_copy['Existing'] = hol_merge_2_rev_copy['Existing'].astype('int64')\n",
    "        hol_merge_2_rev_copy['Existing_formatted'] = hol_merge_2_rev_copy['Existing'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_rev_copy['New'] = hol_merge_2_rev_copy['New'].astype('int64')\n",
    "        hol_merge_2_rev_copy['New_formatted'] = hol_merge_2_rev_copy['New'].apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "        hol_merge_2_rev_copy = hol_merge_2_rev_copy.reindex([3,0,2,1])\n",
    "\n",
    "        # CR distribution across affected products\n",
    "        cr_dist_prod_copy = cr_dist_prod_copy.sort_values('Updated_Can_Vol').reset_index(drop=True)\n",
    "        cr_dist_prod_copy['% CR'] = 100*cr_dist_prod_copy['% CR']\n",
    "        cr_dist_prod_copy['% CR'] = cr_dist_prod_copy['% CR'].round(1)\n",
    "        cr_dist_prod_copy['% CR'] = -1*cr_dist_prod_copy['% CR'] \n",
    "        cr_dist_prod_copy['Updated_Can_Vol'] = -1*cr_dist_prod_copy['Updated_Can_Vol']\n",
    "        cr_dist_prod_copy['CR_Vol_formatted'] = cr_dist_prod_copy['Updated_Can_Vol'].astype('int').apply(lambda x: str(round(x/1000000,2))+'M' if x >= 1000000 else str(round(x/1000))+'k' if x > 1000 else str(x).format('{:,}'))\n",
    "\n",
    "        # Plot it (only if there's data to plot)\n",
    "        if len(tot_vol2) > 0:        \n",
    "            ############# REGION PLOTS ##################\n",
    "            if region_drop.value == 'All':\n",
    "                width = 0.35\n",
    "            else:\n",
    "                width = 300\n",
    "\n",
    "            x1 = np.arange(len(list(tot_vol2['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(merge_data_region2['Region'])))  # the label locations\n",
    "            width = width  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(tot_vol2['Existing']), width, label='Existing')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(tot_vol2['New']), width, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Region)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(tot_vol2['Region']), fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(tot_vol2['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(tot_vol2['New_formatted']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2, list(merge_data_region2['Existing']), width, label='Existing')\n",
    "            rects4 = ax2.bar(x2 + width/2, list(merge_data_region2['New']), width, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Region)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(merge_data_region2['Region']), fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(merge_data_region2['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(merge_data_region2['New_formatted']))\n",
    "\n",
    "            plt.show()\n",
    "           \n",
    "            ################## CHANNEL PLOTS ################\n",
    "            x1 = np.arange(len(list(merge_data_channel2['Channel'])))  # the label locations\n",
    "            x2 = np.arange(len(list(merge_data_channel_revenue2['Channel'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(merge_data_channel2['Existing']), width, label='Existing')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(merge_data_channel2['New']), width, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Channel)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(merge_data_channel2['Channel']), fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(merge_data_channel2['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(merge_data_channel2['New_formatted']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2, list(merge_data_channel_revenue2['Existing']), width, label='Existing')\n",
    "            rects4 = ax2.bar(x2 + width/2, list(merge_data_channel_revenue2['New']), width, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Channel)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(merge_data_channel_revenue2['Channel']), fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(merge_data_channel_revenue2['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(merge_data_channel_revenue2['New_formatted']))\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "            ################## QUARTER PLOTS ################\n",
    "            x1 = np.arange(len(list(merge_data_quarter2['Year Quarter'])))  # the label locations\n",
    "            x2 = np.arange(len(list(merge_data_quarter_revenue2['Year Quarter'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(merge_data_quarter2['Existing']), width, label='Existing')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(merge_data_quarter2['New']), width, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (All Quarters)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(merge_data_quarter2['Year Quarter']), fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(merge_data_quarter2['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(merge_data_quarter2['New_formatted']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2, list(merge_data_quarter_revenue2['Existing']), width, label='Existing')\n",
    "            rects4 = ax2.bar(x2 + width/2, list(merge_data_quarter_revenue2['New']), width, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (All Quarters)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(merge_data_quarter_revenue2['Year Quarter']), fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(merge_data_quarter_revenue2['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(merge_data_quarter_revenue2['New_formatted']))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            ################## HOLIDAY PLOTS ################\n",
    "            x1 = np.arange(len(list(hol_merge_2_copy['Holiday'])))  # the label locations\n",
    "            x2 = np.arange(len(list(hol_merge_2_rev_copy['Holiday'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(hol_merge_2_copy['Existing']), width, label='Existing')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(hol_merge_2_copy['New']), width, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Volume RU', fontsize=20)\n",
    "            ax1.set_title('Existing vs New Product Volume RU (Holidays)', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(hol_merge_2_copy['Holiday']), fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels=list(hol_merge_2_copy['Existing_formatted']))\n",
    "            ax1.bar_label(rects2, padding=5, labels=list(hol_merge_2_copy['New_formatted']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2, list(hol_merge_2_rev_copy['Existing']), width, label='Existing')\n",
    "            rects4 = ax2.bar(x2 + width/2, list(hol_merge_2_rev_copy['New']), width, label='New')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Revenue', fontsize=20)\n",
    "            ax2.set_title('Existing vs New Product Revenue (Holidays)', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(hol_merge_2_rev_copy['Holiday']), fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels=list(hol_merge_2_rev_copy['Existing_formatted']))\n",
    "            ax2.bar_label(rects4, padding=5, labels=list(hol_merge_2_rev_copy['New_formatted']))\n",
    "            plt.show()\n",
    "           \n",
    "            # When CR is present across region-channel\n",
    "            if len(kpi_df3) > 0:\n",
    "\n",
    "                ################## CR AFFECTED PRODUCTS PLOTS 2 ##################                \n",
    "                x1 = np.arange(len(list(cr_dist_prod_copy['Existing_Product'])))  # the label locations            \n",
    "                width = 0.35  # the width of the bars\n",
    "\n",
    "                fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize = (25,8))\n",
    "                ax2 = ax1.twinx()\n",
    "                rects1 = ax1.bar(x1, list(cr_dist_prod_copy['Updated_Can_Vol']), width,label='Cannibalised Units')\n",
    "                rects2 = ax2.plot(x1, list(cr_dist_prod_copy['% CR']), linewidth=4, color = '#F97306',label='% Cannibalisation Rate', marker='o')\n",
    "\n",
    "                # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "                ax1.set_ylabel('Cannibalised Units', fontsize=20)\n",
    "                ax2.set_ylabel('% Cannibalisation Rate', fontsize=20)\n",
    "                ax1.set_title('% Cannibalization Rate across affected products', fontsize=25)\n",
    "                ax1.set_xticks(x1)\n",
    "                ax1.set_xticklabels(list(cr_dist_prod_copy['Existing_Product']), fontsize=15,rotation=45, ha='right')\n",
    "                ax1.legend(loc = 'upper left',bbox_to_anchor=(0.5, -0.7))\n",
    "                ax2.legend(loc = 'upper right',bbox_to_anchor=(0.5, -0.7))\n",
    "\n",
    "                ax1.get_yaxis().set_major_formatter(matplotlib.ticker.EngFormatter())\n",
    "\n",
    "                ax1.bar_label(rects1, padding=5, labels=list(cr_dist_prod_copy['CR_Vol_formatted']),label_type = 'center')\n",
    "                for i,j in cr_dist_prod_copy['% CR'].items():\n",
    "                    ax2.annotate(str(j) + \"%\", xy=(i, j))\n",
    "                print(\"Note1: Cannibalization is measured across the same category products\")\n",
    "                print(\"Note2: For ease of showing numbers, cannibalisation rate is shown positive, in reality it is negative cannibalisation\")\n",
    "                plt.show()\n",
    "            \n",
    "        else:\n",
    "            print(\"No data to show for current selection\")\n",
    "        \n",
    "        \n",
    "if len(qtr_edv_baseline) != 0 :\n",
    "    display(interactive(plotit, Region=region_drop))\n",
    "else:\n",
    "    print(\"Product is not present in that Region/Channel for selected period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cannibalization View__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Region CR dropdown\n",
    "region_CR = widgets.Dropdown(options=['All','EAST','ONTARIO','QUEBEC','WEST'] , description='Region')\n",
    "# Channel CR dropdown\n",
    "channel_CR = widgets.Dropdown(options=['All','CONVENTIONAL','DISCOUNT','DRUG','WHOLESALE'] , description='Channel')\n",
    "\n",
    "region_drop_CR = region_CR\n",
    "channel_drop_CR = channel_CR\n",
    "\n",
    "# In case of 1 new products    \n",
    "if len(iter_change_list) == 1:\n",
    "\n",
    "    def plotit_CR(R, C):\n",
    "        if((channel_drop_CR.value == 'All') and (region_drop_CR.value == 'All')):\n",
    "            final_df = iteration_dataset_all_v2.copy()\n",
    "            final_df['CR'] = (final_df['Displaced_Volume']/final_df['New_Product_Volume'])*100\n",
    "            final_df['CR'] = final_df['CR'].round(decimals=2)\n",
    "            region_cr_final_data = iteration_dataset_all_v2.groupby(['Region']).sum().reset_index()\n",
    "            region_cr_final_data['CR'] = (region_cr_final_data['Displaced_Volume']/region_cr_final_data['New_Product_Volume'])*100\n",
    "            region_cr_final_data['CR'] = region_cr_final_data['CR'].round(decimals=2)\n",
    "            channel_cr_final_data = iteration_dataset_all_v2.groupby(['Channel']).sum().reset_index()\n",
    "            channel_cr_final_data['CR'] = (channel_cr_final_data['Displaced_Volume']/channel_cr_final_data['New_Product_Volume'])*100\n",
    "            channel_cr_final_data['CR'] = channel_cr_final_data['CR'].round(decimals=2)\n",
    "            \n",
    "            ####################### PLOTS ###################\n",
    "            x1 = np.arange(len(list(region_cr_final_data['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(channel_cr_final_data['Channel'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(region_cr_final_data['CR']), width, label='Region')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax1.set_title('Region Cannibalization Rate in %', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(region_cr_final_data['Region']), horizontalalignment=\"right\", fontsize=20)        \n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels= list(region_cr_final_data['CR']))\n",
    "\n",
    "            rects2 = ax2.bar(x2 - width/2,  list(channel_cr_final_data['CR']), width, label='Channel')\n",
    "\n",
    "            # # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax2.set_title('Channel Cannibalization Rate in %', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(channel_cr_final_data['Channel']), horizontalalignment=\"right\", fontsize=20)           \n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax2.bar_label(rects2, padding=5, labels= list(channel_cr_final_data['CR']))\n",
    "            plt.show()\n",
    "\n",
    "        elif region_drop_CR.value == 'All':\n",
    "            final_df = iteration_dataset_all_v2[(iteration_dataset_all_v2['Channel']==channel_drop_CR.value)]\n",
    "            final_df['CR'] = (final_df['Displaced_Volume']/final_df['New_Product_Volume'])*100\n",
    "            final_df['CR'] = final_df['CR'].round(decimals=2)\n",
    "            region_cr_final_data = final_df[['Region','New_Product_Volume','Displaced_Volume']]\n",
    "            region_cr_final_data['CR'] = (region_cr_final_data['Displaced_Volume']/region_cr_final_data['New_Product_Volume'])*100\n",
    "            region_cr_final_data['CR'] = region_cr_final_data['CR'].round(decimals=2)\n",
    "            channel_cr_final_data = final_df.groupby(['Channel']).sum().reset_index()\n",
    "            channel_cr_final_data['CR'] = (channel_cr_final_data['Displaced_Volume']/channel_cr_final_data['New_Product_Volume'])*100\n",
    "            channel_cr_final_data['CR'] = channel_cr_final_data['CR'].round(decimals=2)\n",
    "\n",
    "            ####################### PLOTS ###################\n",
    "            x1 = np.arange(len(list(region_cr_final_data['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(channel_cr_final_data['Channel'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(region_cr_final_data['CR']), width, label='Region')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax1.set_title('Region Cannibalization Rate in %', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(region_cr_final_data['Region']), horizontalalignment=\"right\", fontsize=20)           \n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels= list(region_cr_final_data['CR']))\n",
    "\n",
    "            rects2 = ax2.bar(x2 - width/2,  list(channel_cr_final_data['CR']), width, label='Channel')\n",
    "\n",
    "            # # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax2.set_title('Channel Cannibalization Rate in %', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(channel_cr_final_data['Channel']), horizontalalignment=\"right\", fontsize=20)            \n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax2.bar_label(rects2, padding=5, labels= list(channel_cr_final_data['CR']))\n",
    "            plt.show()\n",
    "\n",
    "        elif channel_drop_CR.value == 'All':\n",
    "            final_df = iteration_dataset_all_v2[(iteration_dataset_all_v2['Region']==region_drop_CR.value)]\n",
    "            final_df['CR'] = (final_df['Displaced_Volume']/final_df['New_Product_Volume'])*100\n",
    "            final_df['CR'] = final_df['CR'].round(decimals=2)\n",
    "            region_cr_final_data = final_df.groupby(['Region']).sum().reset_index()\n",
    "            region_cr_final_data['CR'] = (region_cr_final_data['Displaced_Volume']/region_cr_final_data['New_Product_Volume'])*100\n",
    "            region_cr_final_data['CR'] = region_cr_final_data['CR'].round(decimals=2)\n",
    "            channel_cr_final_data = final_df[['Channel','New_Product_Volume','Displaced_Volume']]\n",
    "            channel_cr_final_data['CR'] = (channel_cr_final_data['Displaced_Volume']/channel_cr_final_data['New_Product_Volume'])*100\n",
    "            channel_cr_final_data['CR'] = channel_cr_final_data['CR'].round(decimals=2)\n",
    "            \n",
    "            ####################### PLOTS ###################\n",
    "            x1 = np.arange(len(list(region_cr_final_data['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(channel_cr_final_data['Channel'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(region_cr_final_data['CR']), width, label='Region')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax1.set_title('Region Cannibalization Rate in %', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(region_cr_final_data['Region']), horizontalalignment=\"right\", fontsize=20)           \n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels= list(region_cr_final_data['CR']))\n",
    "\n",
    "            rects2 = ax2.bar(x2 - width/2,  list(channel_cr_final_data['CR']), width, label='Channel')\n",
    "\n",
    "            # # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax2.set_title('Channel Cannibalization Rate in %', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(channel_cr_final_data['Channel']), horizontalalignment=\"right\", fontsize=20)\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax2.bar_label(rects2, padding=5, labels= list(channel_cr_final_data['CR']))\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            final_df = iteration_dataset_all_v2[(iteration_dataset_all_v2['Region']==region_drop_CR.value)\n",
    "                           &(iteration_dataset_all_v2['Channel']==channel_drop_CR.value)]\n",
    "            final_df['CR'] = (final_df['Displaced_Volume']/final_df['New_Product_Volume'])*100\n",
    "            final_df['CR'] = final_df['CR'].round(decimals=2)\n",
    "            region_cr_final_data = final_df[['Region','New_Product_Volume','Displaced_Volume']]\n",
    "            region_cr_final_data['CR'] = (region_cr_final_data['Displaced_Volume']/region_cr_final_data['New_Product_Volume'])*100\n",
    "            region_cr_final_data['CR'] = region_cr_final_data['CR'].round(decimals=2)\n",
    "            channel_cr_final_data = final_df[['Channel','New_Product_Volume','Displaced_Volume']]\n",
    "            channel_cr_final_data['CR'] = (channel_cr_final_data['Displaced_Volume']/channel_cr_final_data['New_Product_Volume'])*100\n",
    "            channel_cr_final_data['CR'] = channel_cr_final_data['CR'].round(decimals=2)\n",
    "\n",
    "            ####################### PLOTS ###################\n",
    "            x1 = np.arange(len(list(region_cr_final_data['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(channel_cr_final_data['Channel'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize = (8,6))\n",
    "            rects1 = ax1.bar(x1 - width/2, list(region_cr_final_data['CR']), width, label='Region')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax1.set_title('Region-Channel Cannibalization Rate in %', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(region_cr_final_data['Region']+'-'+channel_cr_final_data['Channel']), horizontalalignment=\"right\", fontsize=20)\n",
    "\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax1.bar_label(rects1, labels= list(region_cr_final_data['CR']))\n",
    "            plt.show()\n",
    "\n",
    "    interactive_plot = interactive(plotit_CR, R=region_drop_CR, C=channel_drop_CR)\n",
    "    display(interactive_plot)\n",
    "    \n",
    "# In case of 2 new products    \n",
    "elif len(iter_change_list) == 2:\n",
    "    \n",
    "    def plotit_CR(R, C):\n",
    "        # All regions, all channel\n",
    "        if((channel_drop_CR.value == 'All') and (region_drop_CR.value == 'All')):\n",
    "            final_df_1 = iteration_dataset_all_v2[iteration_dataset_all_v2.iter == 1].copy()\n",
    "            final_df_2 = iteration_dataset_all_v2[iteration_dataset_all_v2.iter == 2].copy()\n",
    "            final_df_1['CR'] = (final_df_1['Displaced_Volume']/final_df_1['New_Product_Volume'])*100\n",
    "            final_df_2['CR'] = (final_df_2['Displaced_Volume']/final_df_2['New_Product_Volume'])*100\n",
    "            final_df_1['CR'] = final_df_1['CR'].round(decimals=2)\n",
    "            final_df_2['CR'] = final_df_2['CR'].round(decimals=2)\n",
    "            region_cr_final_data_1 = iteration_dataset_all_v2[iteration_dataset_all_v2.iter == 1].groupby(['Region']).sum().reset_index()\n",
    "            region_cr_final_data_2 = iteration_dataset_all_v2[iteration_dataset_all_v2.iter == 2].groupby(['Region']).sum().reset_index()\n",
    "            region_cr_final_data_1['CR'] = (region_cr_final_data_1['Displaced_Volume']/region_cr_final_data_1['New_Product_Volume'])*100\n",
    "            region_cr_final_data_2['CR'] = (region_cr_final_data_2['Displaced_Volume']/region_cr_final_data_2['New_Product_Volume'])*100\n",
    "            region_cr_final_data_1['CR'] = region_cr_final_data_1['CR'].round(decimals=2)\n",
    "            region_cr_final_data_2['CR'] = region_cr_final_data_2['CR'].round(decimals=2)\n",
    "            channel_cr_final_data_1 = iteration_dataset_all_v2[iteration_dataset_all_v2.iter == 1].groupby(['Channel']).sum().reset_index()\n",
    "            channel_cr_final_data_2 = iteration_dataset_all_v2[iteration_dataset_all_v2.iter == 2].groupby(['Channel']).sum().reset_index()\n",
    "            channel_cr_final_data_1['CR'] = (channel_cr_final_data_1['Displaced_Volume']/channel_cr_final_data_1['New_Product_Volume'])*100\n",
    "            channel_cr_final_data_2['CR'] = (channel_cr_final_data_2['Displaced_Volume']/channel_cr_final_data_2['New_Product_Volume'])*100\n",
    "            channel_cr_final_data_1['CR'] = channel_cr_final_data_1['CR'].round(decimals=2)\n",
    "            channel_cr_final_data_2['CR'] = channel_cr_final_data_2['CR'].round(decimals=2)\n",
    "\n",
    "            channel_cr_final_data_1 = channel_cr_final_data_1.sort_values('Channel')\n",
    "            channel_cr_final_data_2 = channel_cr_final_data_2.sort_values('Channel')\n",
    "            \n",
    "            region_cr_final_data_1 = region_cr_final_data_1.sort_values('Region')\n",
    "            region_cr_final_data_2 = region_cr_final_data_2.sort_values('Region')\n",
    "\n",
    "            ####################### PLOTS ################### ITER 1 & ITER 2\n",
    "            x1 = np.arange(len(list(region_cr_final_data_1['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(channel_cr_final_data_1['Channel'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(region_cr_final_data_1['CR']), width, label='New Product 1')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(region_cr_final_data_2['CR']), width, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax1.set_title('Region Cannibalization Rate in %', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(region_cr_final_data_1['Region']), horizontalalignment=\"right\", fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels= list(region_cr_final_data_1['CR']))\n",
    "            ax1.bar_label(rects2, padding=5, labels= list(region_cr_final_data_2['CR']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2,  list(channel_cr_final_data_1['CR']), width, label='New Product 1')\n",
    "            rects4 = ax2.bar(x2 + width/2,  list(channel_cr_final_data_2['CR']), width, label='New Product 2')\n",
    "\n",
    "            # # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax2.set_title('Channel Cannibalization Rate in %', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(channel_cr_final_data_1['Channel']), horizontalalignment=\"right\", fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels= list(channel_cr_final_data_1['CR']))\n",
    "            ax2.bar_label(rects4, padding=5, labels= list(channel_cr_final_data_2['CR']))\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "        # All region, one channel\n",
    "        elif region_drop_CR.value == 'All':\n",
    "            final_df_1 = iteration_dataset_all_v2[(iteration_dataset_all_v2.iter == 1) & (iteration_dataset_all_v2['Channel']==channel_drop_CR.value)]\n",
    "            final_df_2 = iteration_dataset_all_v2[(iteration_dataset_all_v2.iter == 2) & (iteration_dataset_all_v2['Channel']==channel_drop_CR.value)]\n",
    "            final_df_1['CR'] = (final_df_1['Displaced_Volume']/final_df_1['New_Product_Volume'])*100\n",
    "            final_df_2['CR'] = (final_df_2['Displaced_Volume']/final_df_1['New_Product_Volume'])*100\n",
    "            final_df_1['CR'] = final_df_1['CR'].round(decimals=2)\n",
    "            region_cr_final_data_1 = final_df_1[['Region','New_Product_Volume','Displaced_Volume']]\n",
    "            region_cr_final_data_2 = final_df_2[['Region','New_Product_Volume','Displaced_Volume']]\n",
    "            region_cr_final_data_1['CR'] = (region_cr_final_data_1['Displaced_Volume']/region_cr_final_data_1['New_Product_Volume'])*100\n",
    "            region_cr_final_data_2['CR'] = (region_cr_final_data_2['Displaced_Volume']/region_cr_final_data_2['New_Product_Volume'])*100\n",
    "            \n",
    "            region_cr_final_data_1['CR'] = region_cr_final_data_1['CR'].round(decimals=2)\n",
    "            region_cr_final_data_2['CR'] = region_cr_final_data_2['CR'].round(decimals=2)\n",
    "            \n",
    "            channel_cr_final_data_1 = final_df_1.groupby(['Channel']).sum().reset_index()\n",
    "            channel_cr_final_data_2 = final_df_2.groupby(['Channel']).sum().reset_index()\n",
    "            \n",
    "            channel_cr_final_data_1['CR'] = (channel_cr_final_data_1['Displaced_Volume']/channel_cr_final_data_1['New_Product_Volume'])*100\n",
    "            channel_cr_final_data_2['CR'] = (channel_cr_final_data_2['Displaced_Volume']/channel_cr_final_data_2['New_Product_Volume'])*100\n",
    "            \n",
    "            channel_cr_final_data_1['CR'] = channel_cr_final_data_1['CR'].round(decimals=2)\n",
    "            channel_cr_final_data_2['CR'] = channel_cr_final_data_2['CR'].round(decimals=2)\n",
    "\n",
    "            channel_cr_final_data_1 = channel_cr_final_data_1.sort_values('Channel')\n",
    "            channel_cr_final_data_2 = channel_cr_final_data_2.sort_values('Channel')\n",
    "            \n",
    "            region_cr_final_data_1 = region_cr_final_data_1.sort_values('Region')\n",
    "            region_cr_final_data_2 = region_cr_final_data_2.sort_values('Region')\n",
    "            \n",
    "            ####################### PLOTS ################### ITER 1 & ITER 2\n",
    "            x1 = np.arange(len(list(region_cr_final_data_1['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(channel_cr_final_data_1['Channel'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(region_cr_final_data_1['CR']), width, label='New Product 1')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(region_cr_final_data_2['CR']), width, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax1.set_title('Region Cannibalization Rate in %', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(region_cr_final_data_1['Region']), horizontalalignment=\"right\", fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels= list(region_cr_final_data_1['CR']))\n",
    "            ax1.bar_label(rects2, padding=5, labels= list(region_cr_final_data_2['CR']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2,  list(channel_cr_final_data_1['CR']), width, label='New Product 1')\n",
    "            rects4 = ax2.bar(x2 + width/2,  list(channel_cr_final_data_2['CR']), width, label='New Product 2')\n",
    "\n",
    "            # # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax2.set_title('Channel Cannibalization Rate in %', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(channel_cr_final_data_1['Channel']), horizontalalignment=\"right\", fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels= list(channel_cr_final_data_1['CR']))\n",
    "            ax2.bar_label(rects4, padding=5, labels= list(channel_cr_final_data_2['CR']))\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "        # All channel one region\n",
    "        elif channel_drop_CR.value == 'All':\n",
    "            final_df_1 = iteration_dataset_all_v2[(iteration_dataset_all_v2.iter == 1) & (iteration_dataset_all_v2['Region']==region_drop_CR.value)]\n",
    "            final_df_1['CR'] = (final_df_1['Displaced_Volume']/final_df_1['New_Product_Volume'])*100\n",
    "            final_df_1['CR'] = final_df_1['CR'].round(decimals=2)\n",
    "            region_cr_final_data_1 = final_df_1.groupby(['Region']).sum().reset_index()\n",
    "            region_cr_final_data_1['CR'] = (region_cr_final_data_1['Displaced_Volume']/region_cr_final_data_1['New_Product_Volume'])*100\n",
    "            region_cr_final_data_1['CR'] = region_cr_final_data_1['CR'].round(decimals=2)\n",
    "            region_cr_final_data_1 = region_cr_final_data_1.sort_values('Region')\n",
    "            channel_cr_final_data_1 = final_df_1[['Channel','New_Product_Volume','Displaced_Volume']]\n",
    "            channel_cr_final_data_1['CR'] = (channel_cr_final_data_1['Displaced_Volume']/channel_cr_final_data_1['New_Product_Volume'])*100\n",
    "            channel_cr_final_data_1['CR'] = channel_cr_final_data_1['CR'].round(decimals=2)\n",
    "            channel_cr_final_data_1 = channel_cr_final_data_1.sort_values('Channel')\n",
    "\n",
    "            final_df_2 = iteration_dataset_all_v2[(iteration_dataset_all_v2.iter == 2) & (iteration_dataset_all_v2['Region']==region_drop_CR.value)]\n",
    "            final_df_2['CR'] = (final_df_2['Displaced_Volume']/final_df_2['New_Product_Volume'])*100\n",
    "            final_df_2['CR'] = final_df_2['CR'].round(decimals=2)\n",
    "            region_cr_final_data_2 = final_df_2.groupby(['Region']).sum().reset_index()\n",
    "            region_cr_final_data_2['CR'] = (region_cr_final_data_2['Displaced_Volume']/region_cr_final_data_2['New_Product_Volume'])*100\n",
    "            region_cr_final_data_2['CR'] = region_cr_final_data_2['CR'].round(decimals=2)\n",
    "            region_cr_final_data_2 = region_cr_final_data_2.sort_values('Region')\n",
    "            channel_cr_final_data_2 = final_df_2[['Channel','New_Product_Volume','Displaced_Volume']]\n",
    "            channel_cr_final_data_2['CR'] = (channel_cr_final_data_2['Displaced_Volume']/channel_cr_final_data_2['New_Product_Volume'])*100\n",
    "            channel_cr_final_data_2['CR'] = channel_cr_final_data_2['CR'].round(decimals=2)            \n",
    "            channel_cr_final_data_2 = channel_cr_final_data_2.sort_values('Channel')\n",
    "\n",
    "            ####################### PLOTS ################### ITER 1 & ITER 2\n",
    "            x1 = np.arange(len(list(region_cr_final_data_1['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(channel_cr_final_data_1['Channel'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(region_cr_final_data_1['CR']), width, label='New Product 1')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(region_cr_final_data_2['CR']), width, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax1.set_title('Region Cannibalization Rate in %', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(region_cr_final_data_1['Region']), horizontalalignment=\"right\", fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels= list(region_cr_final_data_1['CR']))\n",
    "            ax1.bar_label(rects2, padding=5, labels= list(region_cr_final_data_2['CR']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2,  list(channel_cr_final_data_1['CR']), width, label='New Product 1')\n",
    "            rects4 = ax2.bar(x2 + width/2,  list(channel_cr_final_data_2['CR']), width, label='New Product 2')\n",
    "\n",
    "            # # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax2.set_title('Channel Cannibalization Rate in %', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(channel_cr_final_data_1['Channel']), horizontalalignment=\"right\", fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels= list(channel_cr_final_data_1['CR']))\n",
    "            ax2.bar_label(rects4, padding=5, labels= list(channel_cr_final_data_2['CR']))\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "        # One region, one channel\n",
    "        else:\n",
    "            final_df_1 = iteration_dataset_all_v2[(iteration_dataset_all_v2.iter == 1) & (iteration_dataset_all_v2['Region']==region_drop_CR.value)\n",
    "                           &(iteration_dataset_all_v2['Channel']==channel_drop_CR.value)]\n",
    "            final_df_2 = iteration_dataset_all_v2[(iteration_dataset_all_v2.iter == 2) & (iteration_dataset_all_v2['Region']==region_drop_CR.value)\n",
    "                           &(iteration_dataset_all_v2['Channel']==channel_drop_CR.value)]\n",
    "            final_df_1['CR'] = (final_df_1['Displaced_Volume']/final_df_1['New_Product_Volume'])*100\n",
    "            final_df_1['CR'] = final_df_1['CR'].round(decimals=2)\n",
    "            region_cr_final_data_1 = final_df_1[['Region','New_Product_Volume','Displaced_Volume']]\n",
    "            region_cr_final_data_1['CR'] = (region_cr_final_data_1['Displaced_Volume']/region_cr_final_data_1['New_Product_Volume'])*100\n",
    "            region_cr_final_data_1['CR'] = region_cr_final_data_1['CR'].round(decimals=2)\n",
    "            channel_cr_final_data_1 = final_df_1[['Channel','New_Product_Volume','Displaced_Volume']]\n",
    "            channel_cr_final_data_1['CR'] = (channel_cr_final_data_1['Displaced_Volume']/channel_cr_final_data_1['New_Product_Volume'])*100\n",
    "            channel_cr_final_data_1['CR'] = channel_cr_final_data_1['CR'].round(decimals=2)\n",
    "\n",
    "            final_df_2['CR'] = (final_df_2['Displaced_Volume']/final_df_2['New_Product_Volume'])*100\n",
    "            final_df_2['CR'] = final_df_2['CR'].round(decimals=2)\n",
    "            region_cr_final_data_2 = final_df_2[['Region','New_Product_Volume','Displaced_Volume']]\n",
    "            region_cr_final_data_2['CR'] = (region_cr_final_data_2['Displaced_Volume']/region_cr_final_data_2['New_Product_Volume'])*100\n",
    "            region_cr_final_data_2['CR'] = region_cr_final_data_2['CR'].round(decimals=2)\n",
    "            channel_cr_final_data_2 = final_df_2[['Channel','New_Product_Volume','Displaced_Volume']]\n",
    "            channel_cr_final_data_2['CR'] = (channel_cr_final_data_2['Displaced_Volume']/channel_cr_final_data_2['New_Product_Volume'])*100\n",
    "            channel_cr_final_data_2['CR'] = channel_cr_final_data_2['CR'].round(decimals=2)\n",
    "            \n",
    "            channel_cr_final_data_1 = channel_cr_final_data_1.sort_values('Channel')\n",
    "            channel_cr_final_data_2 = channel_cr_final_data_2.sort_values('Channel')\n",
    "            \n",
    "            region_cr_final_data_1 = region_cr_final_data_1.sort_values('Region')\n",
    "            region_cr_final_data_2 = region_cr_final_data_2.sort_values('Region')\n",
    "            \n",
    "            ####################### PLOTS ################### ITER 1 & ITER 2\n",
    "            x1 = np.arange(len(list(region_cr_final_data_1['Region'])))  # the label locations\n",
    "            x2 = np.arange(len(list(channel_cr_final_data_1['Channel'])))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            rects1 = ax1.bar(x1 - width/2, list(region_cr_final_data_1['CR']), width, label='New Product 1')\n",
    "            rects2 = ax1.bar(x1 + width/2, list(region_cr_final_data_2['CR']), width, label='New Product 2')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax1.set_title('Region Cannibalization Rate in %', fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(region_cr_final_data_1['Region']), horizontalalignment=\"right\", fontsize=20)\n",
    "            ax1.legend()\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax1.bar_label(rects1, padding=5, labels= list(region_cr_final_data_1['CR']))\n",
    "            ax1.bar_label(rects2, padding=5, labels= list(region_cr_final_data_2['CR']))\n",
    "\n",
    "            rects3 = ax2.bar(x2 - width/2,  list(channel_cr_final_data_1['CR']), width, label='New Product 1')\n",
    "            rects4 = ax2.bar(x2 + width/2,  list(channel_cr_final_data_2['CR']), width, label='New Product 2')\n",
    "\n",
    "            # # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax2.set_ylabel('Cannibalization Rate', fontsize=20)\n",
    "            ax2.set_title('Channel Cannibalization Rate in', fontsize=25)\n",
    "            ax2.set_xticks(x2)\n",
    "            ax2.set_xticklabels(list(channel_cr_final_data_1['Channel']), horizontalalignment=\"right\", fontsize=20)\n",
    "            ax2.legend()\n",
    "\n",
    "            ax2.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "            ax2.bar_label(rects3, padding=5, labels= list(channel_cr_final_data_1['CR']))\n",
    "            ax2.bar_label(rects4, padding=5, labels= list(channel_cr_final_data_2['CR']))\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "    interactive_plot = interactive(plotit_CR, R=region_drop_CR, C=channel_drop_CR)\n",
    "    display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Net Gain due to Promotion of New Product__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Product_fil assigned as most similar product in ensemble approach\n",
    "if toggle_ensemble_normal.value == 'Multiple Product': # In case of ensemble approach    \n",
    "    \n",
    "    if len(prod_ensemble_list) != 3: # if 3 products are not selected\n",
    "        clear_output()\n",
    "        print('Select 3 products as existing model pack')\n",
    "    else:\n",
    "        score_data_cann = score_data_cann[score_data_cann.Product.isin(prod_ensemble_list)].sort_values('Similarity_Score', ascending = False)\n",
    "        product_fil = score_data_cann.loc[:,'Product'][:1].values[0]\n",
    "\n",
    "\n",
    "# In case the user doesn't upload the data or is not an ensemle approach\n",
    "if (toggle_upload.value != 'Yes'):\n",
    "    # In case no product is present in selected region-channel\n",
    "    if len(qtr_edv_baseline) != 0 :\n",
    "        display(Markdown('<div><div class=\"loader\"></div><h2> &nbsp;Measuring Net Gain due to Promotions/Discounts</h2></div>'))\n",
    "\n",
    "        #Defining empty dataframes for model training\n",
    "        combined_dataset = pd.DataFrame({'Week': pd.Series([], dtype='object')})\n",
    "        Test_results_net = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "        Test_results_net_cann = pd.DataFrame({'Region': pd.Series([], dtype='object')})\n",
    "        Test_Data = pd.DataFrame()\n",
    "\n",
    "        # Defining test week list\n",
    "        Test_week_list = {\"2019Q1\" : [\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-10\",\"2019-11\",\"2019-12\",\"2019-13\"],\n",
    "             \"2019Q2\" : [\"2019-14\",\"2019-15\",\"2019-16\",\"2019-17\",\"2019-18\",\"2019-19\",\"2019-20\",\"2019-21\",\"2019-22\",\"2019-23\",\"2019-24\",\"2019-25\",\"2019-26\"],\n",
    "             \"2019Q3\" : [\"2019-27\",\"2019-28\",\"2019-29\",\"2019-30\",\"2019-31\",\"2019-32\",\"2019-33\",\"2019-34\",\"2019-35\",\"2019-36\",\"2019-37\",\"2019-38\",\"2019-39\"],\n",
    "             \"2019Q4\" : [\"2019-40\",\"2019-41\",\"2019-42\",\"2019-43\",\"2019-44\",\"2019-45\",\"2019-46\",\"2019-47\",\"2019-48\",\"2019-49\",\"2019-50\",\"2019-51\",\"2019-52\"],\n",
    "\n",
    "        # CHECK : ADDING 2020 weeks                  \n",
    "             \"2020Q1\" : [\"2020-01\",\"2020-02\",\"2020-03\",\"2020-04\",\"2020-05\",\"2020-06\",\"2020-07\",\"2020-08\",\"2020-09\",\"2020-10\",\"2020-11\",\"2020-12\",\"2020-13\"],\n",
    "             \"2020Q2\" : [\"2020-14\",\"2020-15\",\"2020-16\",\"2020-17\",\"2020-18\",\"2020-19\",\"2020-20\",\"2020-21\",\"2020-22\",\"2020-23\",\"2020-24\",\"2020-25\",\"2020-26\"],\n",
    "             \"2020Q3\" : [\"2020-27\",\"2020-28\",\"2020-29\",\"2020-30\",\"2020-31\",\"2020-32\",\"2020-33\",\"2020-34\",\"2020-35\",\"2020-36\",\"2020-37\",\"2020-38\",\"2020-39\"],\n",
    "             \"2020Q4\" : [\"2020-40\",\"2020-41\",\"2020-42\",\"2020-43\",\"2020-44\",\"2020-45\",\"2020-46\",\"2020-47\",\"2020-48\",\"2020-49\",\"2020-50\",\"2020-51\",\"2020-52\"],\n",
    "             \"fulltrain\" : []}\n",
    "\n",
    "        if toggle_np2.value == 'Yes':\n",
    "            iter_change_list = [1,2]\n",
    "        else:\n",
    "            iter_change_list = [1]\n",
    "\n",
    "        for i in iter_change_list:\n",
    "                        \n",
    "            #Model training and prediction\n",
    "            for Region_key in Region_List:\n",
    "        \n",
    "                Volume_dataset = Volume_dataset_all_reg.loc[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                           &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))]\n",
    "                Volume_dataset['Pantry2'] = Volume_dataset['Pantry2'].fillna(0)\n",
    "                brand_mapping = Volume_dataset_all_reg[(Volume_dataset_all_reg[\"Region\"] == Region_key)\n",
    "                                                      &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))][['Banner','Product','brand',\"Category\",\"Pack.Subtype\",\"Brand\"]].drop_duplicates()\n",
    "                sel_brands = Volume_dataset_all_reg.loc[((Volume_dataset_all_reg[\"Week\"]> \"2019-01\")\n",
    "                                                        &(Volume_dataset_all_reg[\"Region\"]== Region_key)\n",
    "                                                        &(Volume_dataset_all_reg[\"Channel\"].isin(channel_list))\n",
    "                                                        &(Volume_dataset_all_reg[\"Eq.Unit.Sales\"].notnull())), \"brand\"].unique()            \n",
    "\n",
    "                #################### ALL REGION COMBINED DATASET ####################\n",
    "                \n",
    "                if(Region_key == 'EAST'):\n",
    "                    combined_dataset_adcal_temp = combined_dataset_adcal_temp_EAST.copy()\n",
    "                    combined_dataset_edv_temp = combined_dataset_edv_temp_EAST.copy()\n",
    "                    combined_dataset_other_promo_temp = combined_dataset_other_promo_temp_EAST.copy()\n",
    "                    combined_dataset_self_promo_temp = combined_dataset_self_promo_temp_EAST.copy()\n",
    "\n",
    "                elif(Region_key == 'ONTARIO'):\n",
    "                    combined_dataset_adcal_temp = combined_dataset_adcal_temp_ONTARIO.copy()\n",
    "                    combined_dataset_edv_temp = combined_dataset_edv_temp_ONTARIO.copy()\n",
    "                    combined_dataset_other_promo_temp = combined_dataset_other_promo_temp_ONTARIO.copy()\n",
    "                    combined_dataset_self_promo_temp = combined_dataset_self_promo_temp_ONTARIO.copy()\n",
    "\n",
    "                elif(Region_key == 'QUEBEC'):\n",
    "                    combined_dataset_adcal_temp = combined_dataset_adcal_temp_QUEBEC.copy()\n",
    "                    combined_dataset_edv_temp = combined_dataset_edv_temp_QUEBEC.copy()\n",
    "                    combined_dataset_other_promo_temp = combined_dataset_other_promo_temp_QUEBEC.copy()\n",
    "                    combined_dataset_self_promo_temp = combined_dataset_self_promo_temp_QUEBEC.copy()\n",
    "\n",
    "                else:\n",
    "                    combined_dataset_adcal_temp = combined_dataset_adcal_temp_WEST.copy()\n",
    "                    combined_dataset_edv_temp = combined_dataset_edv_temp_WEST.copy()\n",
    "                    combined_dataset_other_promo_temp = combined_dataset_other_promo_temp_WEST.copy()\n",
    "                    combined_dataset_self_promo_temp = combined_dataset_self_promo_temp_WEST.copy()\n",
    "    \n",
    "                # Get category columns\n",
    "                combined_dataset_adcal_temp = combined_dataset_adcal_temp.merge(Volume_dataset_all_reg[['Product','Category']].drop_duplicates(), how = 'left')\n",
    "                combined_dataset_edv_temp = combined_dataset_edv_temp.merge(Volume_dataset_all_reg[['Product','Category']].drop_duplicates(), how = 'left')\n",
    "                combined_dataset_other_promo_temp = combined_dataset_other_promo_temp.merge(Volume_dataset_all_reg[['Product','Category']].drop_duplicates(), how = 'left')\n",
    "                combined_dataset_self_promo_temp = combined_dataset_self_promo_temp.merge(Volume_dataset_all_reg[['Product','Category']].drop_duplicates(), how = 'left')\n",
    "\n",
    "                # 4 scenarios\n",
    "                combined_dataset_adcal = combined_dataset_adcal_temp.copy()\n",
    "                combined_dataset_edv = combined_dataset_edv_temp[combined_dataset_edv_temp.Product == product_fil].copy()\n",
    "                combined_dataset_other_promo = combined_dataset_other_promo_temp[combined_dataset_other_promo_temp.Product == product_fil].copy()\n",
    "                combined_dataset_self_promo = combined_dataset_self_promo_temp[combined_dataset_self_promo_temp.Product == product_fil].copy()\n",
    "                \n",
    "            #######################################################################################################################\n",
    "                                        # 1. Adcal promo Data\n",
    "            #######################################################################################################################\n",
    "\n",
    "                banner_dummies = pd.get_dummies(combined_dataset_adcal.Banner)\n",
    "                banner_dummies_common = banner_dummies.copy()\n",
    "                combined_dataset_adcal = pd.concat([combined_dataset_adcal, banner_dummies], axis=1)\n",
    "\n",
    "                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "\n",
    "                combined_dataset_adcal = combined_dataset_adcal.loc[combined_dataset_adcal[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                combined_dataset_adcal.sort_values('Week',inplace = True)\n",
    "                combined_dataset_adcal = pd.merge(combined_dataset_adcal,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset_adcal['Intial_weeks'] = combined_dataset_adcal['Intial_weeks'].fillna('Stabilization')\n",
    "                combined_dataset_adcal.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                #Adding dummies for category\n",
    "                category_dummies = pd.get_dummies(combined_dataset_adcal['Category'])\n",
    "                category_dummies_common = category_dummies.copy()\n",
    "                combined_dataset_adcal = pd.concat([combined_dataset_adcal, category_dummies], axis=1)\n",
    "                combined_dataset_adcal.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                #Adding dummies for product attribute - pack subtype\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset_adcal['Pack.Subtype'])\n",
    "                pack_subtypes_dummies_common = pack_subtypes_dummies.copy()\n",
    "                combined_dataset_adcal = pd.concat([combined_dataset_adcal, pack_subtypes_dummies], axis=1)\n",
    "                combined_dataset_adcal.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                # Adding pack content dummies\n",
    "                pack_content_dummies = pd.get_dummies(combined_dataset_adcal['PACK_CONTENT'])\n",
    "                pack_content_dummies_common = pack_content_dummies.copy()\n",
    "                combined_dataset_adcal = pd.concat([combined_dataset_adcal, pack_content_dummies], axis=1)\n",
    "                combined_dataset_adcal.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                combined_dataset_adcal_test = combined_dataset_adcal.copy()\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies = pd.get_dummies(combined_dataset_adcal['Intial_weeks'])\n",
    "                combined_dataset_adcal = pd.concat([combined_dataset_adcal, stage_dummies], axis=1)\n",
    "                combined_dataset_adcal.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # Changing product stage as per start date provided by the user\n",
    "                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                # Getting updated product stages\n",
    "                combined_dataset_test_check = combined_dataset_adcal_test[combined_dataset_adcal_test.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                combined_dataset_adcal_test = combined_dataset_test_check.copy()\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies_test = pd.get_dummies(combined_dataset_adcal_test['Intial_weeks'])\n",
    "\n",
    "                # If all product stages are not present for the uploaded data\n",
    "                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    stage_dummies_test[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                combined_dataset_adcal_test = pd.concat([combined_dataset_adcal_test, stage_dummies_test], axis=1)\n",
    "                combined_dataset_adcal_test.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # In case the user doesn't want to upload adcal data\n",
    "                if toggle_upload.value != 'Yes':\n",
    "                    # CHANGES : For adcal discounts\n",
    "                    if 'All' in list(test_period.value):\n",
    "                        # Discount change\n",
    "                        if i == 1:                \n",
    "                            combined_dataset_adcal_test['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_adcal_test['Adcal_DD']\n",
    "                            combined_dataset_adcal_test.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_adcal_test.loc[:,'EDV.Price']\n",
    "                        elif i == 2:\n",
    "                            combined_dataset_adcal_test['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_adcal_test['Adcal_DD']\n",
    "                            combined_dataset_adcal_test.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_adcal_test.loc[:,'EDV.Price']          \n",
    "                    else:                \n",
    "                        if i == 1:                \n",
    "                            # Discount change\n",
    "                            combined_dataset_adcal_test.loc[combined_dataset_adcal_test['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_adcal_test[combined_dataset_adcal_test['Test_period'].isin(test_period_fil)]['Adcal_DD'] \n",
    "                            # Baseline pricing\n",
    "                            combined_dataset_adcal_test.loc[(combined_dataset_adcal_test['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_adcal_test[(combined_dataset_adcal_test['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                        elif i == 2:\n",
    "                            # Discount change\n",
    "                            combined_dataset_adcal_test.loc[combined_dataset_adcal_test['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_adcal_test[combined_dataset_adcal_test['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                            # Baseline pricing\n",
    "                            combined_dataset_adcal_test.loc[(combined_dataset_adcal_test['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_adcal_test[(combined_dataset_adcal_test['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "                #Adding discount depth columns\n",
    "                combined_dataset_adcal[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset_adcal[\"Adcal_DD\"]]\n",
    "                combined_dataset_adcal[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset_adcal[\"Adcal_DD\"]]\n",
    "\n",
    "                combined_dataset_adcal_test[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset_adcal_test[\"Adcal_DD\"]]\n",
    "                combined_dataset_adcal_test[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset_adcal_test[\"Adcal_DD\"]]            \n",
    "\n",
    "                # Adcal Price calculation\n",
    "                # In case the discounts go over 100%, limit to 100%\n",
    "                combined_dataset_adcal_test.loc[combined_dataset_adcal_test.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "                combined_dataset_adcal_test.loc[combined_dataset_adcal_test['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_adcal_test.loc[combined_dataset_adcal_test['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_adcal_test.loc[combined_dataset_adcal_test['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset_adcal_test.loc[combined_dataset_adcal_test['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_adcal_test.loc[combined_dataset_adcal_test['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Adcal Price calculation\n",
    "                combined_dataset_adcal.loc[combined_dataset_adcal['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_adcal.loc[combined_dataset_adcal['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_adcal.loc[combined_dataset_adcal['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset_adcal.loc[combined_dataset_adcal['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_adcal.loc[combined_dataset_adcal['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                combined_dataset_adcal.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "                combined_dataset_adcal_test.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                # Dropping null volumes\n",
    "                complete_dataset_adcal_test = combined_dataset_adcal_test.copy()\n",
    "                complete_dataset_adcal_test = complete_dataset_adcal_test.loc[complete_dataset_adcal_test[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                complete_dataset_adcal = combined_dataset_adcal.copy()\n",
    "                complete_dataset_adcal = complete_dataset_adcal.loc[complete_dataset_adcal[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                complete_dataset_adcal = complete_dataset_adcal.loc[((complete_dataset_adcal[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (complete_dataset_adcal[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                complete_dataset_adcal_test = complete_dataset_adcal_test.loc[((complete_dataset_adcal_test[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (complete_dataset_adcal_test[\"Week\"] <= \"2019-52\"))]\n",
    "\n",
    "                complete_dataset_adcal['Adcal_Price'] = np.log(complete_dataset_adcal['Adcal_Price'])              \n",
    "\n",
    "                complete_dataset_adcal_test['Adcal_Price'] = np.log(complete_dataset_adcal_test['Adcal_Price'])\n",
    "                complete_dataset_adcal_test.sort_values(by=['Banner','Product','Week'], inplace=True)           \n",
    "\n",
    "\n",
    "                #######################################################################################################################\n",
    "                                        # 2. EDV promo Data\n",
    "                #######################################################################################################################\n",
    "\n",
    "                banner_dummies = pd.get_dummies(combined_dataset_edv.Banner)\n",
    "                # If all category dummies are not present for the uploaded data\n",
    "                c = [i for i in list(banner_dummies_common.columns) if i not in list(banner_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    banner_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)\n",
    "                combined_dataset_edv = pd.concat([combined_dataset_edv, banner_dummies], axis=1)\n",
    "\n",
    "                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "\n",
    "                combined_dataset_edv = combined_dataset_edv.loc[combined_dataset_edv[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                combined_dataset_edv.sort_values('Week',inplace = True)\n",
    "                combined_dataset_edv = pd.merge(combined_dataset_edv,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset_edv['Intial_weeks'] = combined_dataset_edv['Intial_weeks'].fillna('Stabilization')\n",
    "                combined_dataset_edv.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                #Adding dummies for category\n",
    "                category_dummies = pd.get_dummies(combined_dataset_edv['Category'])\n",
    "\n",
    "                # If all category dummies are not present for the uploaded data\n",
    "                c = [i for i in list(category_dummies_common.columns) if i not in list(category_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    category_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)\n",
    "                combined_dataset_edv = pd.concat([combined_dataset_edv, category_dummies], axis=1)\n",
    "                combined_dataset_edv.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                #Adding dummies for product attribute - pack subtype\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset_edv['Pack.Subtype'])\n",
    "\n",
    "                # If all pack subtype dummies are not present for the uploaded data\n",
    "                c = [i for i in list(pack_subtypes_dummies_common.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)\n",
    "                combined_dataset_edv = pd.concat([combined_dataset_edv, pack_subtypes_dummies], axis=1)\n",
    "                combined_dataset_edv.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                # Adding pack content dummies    \n",
    "                pack_content_dummies = pd.get_dummies(combined_dataset_edv['PACK_CONTENT'])\n",
    "\n",
    "                # If all pack content dummies are not present for the uploaded data\n",
    "                c = [i for i in list(pack_content_dummies_common.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    pack_content_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)\n",
    "                combined_dataset_edv = pd.concat([combined_dataset_edv, pack_content_dummies], axis=1)\n",
    "                combined_dataset_edv.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                # Changing product stage as per start date provided by the user\n",
    "                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                # Getting updated product stages\n",
    "                combined_dataset_test_check = combined_dataset_edv[combined_dataset_edv.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                combined_dataset_edv = combined_dataset_test_check.copy()\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies_test = pd.get_dummies(combined_dataset_edv['Intial_weeks'])\n",
    "\n",
    "                # If all product stages are not present for the uploaded data\n",
    "                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    stage_dummies_test[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                combined_dataset_edv = pd.concat([combined_dataset_edv, stage_dummies_test], axis=1)\n",
    "                combined_dataset_edv.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                combined_dataset_edv.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                complete_dataset_edv = combined_dataset_edv.copy()\n",
    "                complete_dataset_edv = complete_dataset_edv.loc[complete_dataset_edv[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                complete_dataset_edv = complete_dataset_edv.loc[((complete_dataset_edv[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (complete_dataset_edv[\"Week\"] <= \"2019-52\"))]\n",
    "                complete_dataset_edv.sort_values(by=['Banner','Product','Week'], inplace=True)\n",
    "\n",
    "                #######################################################################################################################\n",
    "                                        # 3. Other promo Data\n",
    "                #######################################################################################################################\n",
    "\n",
    "                banner_dummies = pd.get_dummies(combined_dataset_other_promo.Banner)\n",
    "                # If all category dummies are not present for the uploaded data\n",
    "                c = [i for i in list(banner_dummies_common.columns) if i not in list(banner_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    banner_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)\n",
    "                combined_dataset_other_promo = pd.concat([combined_dataset_other_promo, banner_dummies], axis=1)\n",
    "\n",
    "                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "\n",
    "                combined_dataset_other_promo = combined_dataset_other_promo.loc[combined_dataset_other_promo[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                combined_dataset_other_promo.sort_values('Week',inplace = True)\n",
    "                combined_dataset_other_promo = pd.merge(combined_dataset_other_promo,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset_other_promo['Intial_weeks'] = combined_dataset_other_promo['Intial_weeks'].fillna('Stabilization')\n",
    "                combined_dataset_other_promo.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                #Adding dummies for category\n",
    "                category_dummies = pd.get_dummies(combined_dataset_other_promo['Category'])\n",
    "\n",
    "                # If all category dummies are not present for the uploaded data\n",
    "                c = [i for i in list(category_dummies_common.columns) if i not in list(category_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    category_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)\n",
    "                combined_dataset_other_promo = pd.concat([combined_dataset_other_promo, category_dummies], axis=1)\n",
    "                combined_dataset_other_promo.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                #Adding dummies for product attribute - pack subtype\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset_other_promo['Pack.Subtype'])\n",
    "\n",
    "                # If all pack subtype dummies are not present for the uploaded data\n",
    "                c = [i for i in list(pack_subtypes_dummies_common.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)\n",
    "                combined_dataset_other_promo = pd.concat([combined_dataset_other_promo, pack_subtypes_dummies], axis=1)\n",
    "                combined_dataset_other_promo.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                # Adding pack content dummies    \n",
    "                pack_content_dummies = pd.get_dummies(combined_dataset_other_promo['PACK_CONTENT'])\n",
    "\n",
    "                # If all pack content dummies are not present for the uploaded data\n",
    "                c = [i for i in list(pack_content_dummies_common.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    pack_content_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)\n",
    "                combined_dataset_other_promo = pd.concat([combined_dataset_other_promo, pack_content_dummies], axis=1)\n",
    "                combined_dataset_other_promo.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                # Changing product stage as per start date provided by the user\n",
    "                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                # Getting updated product stages\n",
    "                combined_dataset_test_check = combined_dataset_other_promo[combined_dataset_other_promo.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                combined_dataset_other_promo = combined_dataset_test_check.copy()\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies_test = pd.get_dummies(combined_dataset_other_promo['Intial_weeks'])\n",
    "\n",
    "                # If all product stages are not present for the uploaded data\n",
    "                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    stage_dummies_test[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                combined_dataset_other_promo = pd.concat([combined_dataset_other_promo, stage_dummies_test], axis=1)\n",
    "                combined_dataset_other_promo.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                combined_dataset_other_promo.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                complete_dataset_other_promo = combined_dataset_other_promo.copy()\n",
    "                complete_dataset_other_promo = complete_dataset_other_promo.loc[complete_dataset_other_promo[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                complete_dataset_other_promo = complete_dataset_other_promo.loc[((complete_dataset_other_promo[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (complete_dataset_other_promo[\"Week\"] <= \"2019-52\"))]\n",
    "                complete_dataset_other_promo.sort_values(by=['Banner','Product','Week'], inplace=True)\n",
    "\n",
    "                #######################################################################################################################\n",
    "                                        # 4. Self promo Data\n",
    "                #######################################################################################################################\n",
    "\n",
    "                banner_dummies = pd.get_dummies(combined_dataset_self_promo.Banner)\n",
    "                # If all category dummies are not present for the uploaded data\n",
    "                c = [i for i in list(banner_dummies_common.columns) if i not in list(banner_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    banner_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                banner_dummies = banner_dummies.reindex(sorted(banner_dummies.columns), axis=1)\n",
    "                combined_dataset_self_promo = pd.concat([combined_dataset_self_promo, banner_dummies], axis=1)\n",
    "\n",
    "                ## Not adding Product dummies since for New Product Simulator other Product dummies are not significant\n",
    "\n",
    "                combined_dataset_self_promo = combined_dataset_self_promo.loc[combined_dataset_self_promo[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                combined_dataset_self_promo.sort_values('Week',inplace = True)\n",
    "                combined_dataset_self_promo = pd.merge(combined_dataset_self_promo,new_prod_stage,on = ['Product','Week'],how = 'left')\n",
    "                combined_dataset_self_promo['Intial_weeks'] = combined_dataset_self_promo['Intial_weeks'].fillna('Stabilization')\n",
    "                combined_dataset_self_promo.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "                #Adding dummies for category\n",
    "                category_dummies = pd.get_dummies(combined_dataset_self_promo['Category'])\n",
    "\n",
    "                # If all category dummies are not present for the uploaded data\n",
    "                c = [i for i in list(category_dummies_common.columns) if i not in list(category_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    category_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                category_dummies = category_dummies.reindex(sorted(category_dummies.columns), axis=1)\n",
    "                combined_dataset_self_promo = pd.concat([combined_dataset_self_promo, category_dummies], axis=1)\n",
    "                combined_dataset_self_promo.drop([\"Category\"], inplace=True, axis=1)\n",
    "\n",
    "                #Adding dummies for product attribute - pack subtype\n",
    "                pack_subtypes_dummies = pd.get_dummies(combined_dataset_self_promo['Pack.Subtype'])\n",
    "\n",
    "                # If all pack subtype dummies are not present for the uploaded data\n",
    "                c = [i for i in list(pack_subtypes_dummies_common.columns) if i not in list(pack_subtypes_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    pack_subtypes_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_subtypes_dummies = pack_subtypes_dummies.reindex(sorted(pack_subtypes_dummies.columns), axis=1)\n",
    "                combined_dataset_self_promo = pd.concat([combined_dataset_self_promo, pack_subtypes_dummies], axis=1)\n",
    "                combined_dataset_self_promo.drop([\"Pack.Subtype\"], inplace=True, axis=1)\n",
    "\n",
    "                # Adding pack content dummies    \n",
    "                pack_content_dummies = pd.get_dummies(combined_dataset_self_promo['PACK_CONTENT'])\n",
    "\n",
    "                # If all pack content dummies are not present for the uploaded data\n",
    "                c = [i for i in list(pack_content_dummies_common.columns) if i not in list(pack_content_dummies.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    pack_content_dummies[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                pack_content_dummies = pack_content_dummies.reindex(sorted(pack_content_dummies.columns), axis=1)\n",
    "                combined_dataset_self_promo = pd.concat([combined_dataset_self_promo, pack_content_dummies], axis=1)\n",
    "                combined_dataset_self_promo.drop([\"PACK_CONTENT\"], inplace=True, axis=1)\n",
    "\n",
    "                # Changing product stage as per start date provided by the user\n",
    "                prod_stage_data = prod_stage_check(product_fil)            \n",
    "\n",
    "                # Getting updated product stages\n",
    "                combined_dataset_test_check = combined_dataset_self_promo[combined_dataset_self_promo.Product == product_fil].merge(prod_stage_data[['Week','Initial_weeks1']], how = 'left')\n",
    "                combined_dataset_test_check['Initial_weeks1'] = combined_dataset_test_check['Initial_weeks1'].fillna(combined_dataset_test_check['Intial_weeks'])\n",
    "                combined_dataset_test_check.drop(columns = {'Intial_weeks'}, inplace = True)\n",
    "                combined_dataset_test_check.rename(columns = {'Initial_weeks1':'Intial_weeks'}, inplace = True)\n",
    "\n",
    "                combined_dataset_self_promo = combined_dataset_test_check.copy()\n",
    "\n",
    "                # Product stage dummies\n",
    "                stage_dummies_test = pd.get_dummies(combined_dataset_self_promo['Intial_weeks'])\n",
    "\n",
    "                # If all product stages are not present for the uploaded data\n",
    "                c = [i for i in list(stage_dummies.columns) if i not in list(stage_dummies_test.columns)]\n",
    "                if len(c) != 0 :\n",
    "                    stage_dummies_test[c] = 0\n",
    "\n",
    "                # Reordering columns                \n",
    "                stage_dummies_test = stage_dummies_test.reindex(sorted(stage_dummies_test.columns), axis=1)\n",
    "                combined_dataset_self_promo = pd.concat([combined_dataset_self_promo, stage_dummies_test], axis=1)\n",
    "                combined_dataset_self_promo.drop([\"Intial_weeks\"], inplace=True, axis=1)\n",
    "\n",
    "                # In case the user doesn't want to upload adcal data\n",
    "                if toggle_upload.value != 'Yes':\n",
    "                    # CHANGES : For adcal discounts\n",
    "                    if 'All' in list(test_period.value):\n",
    "                        # Discount change\n",
    "                        if i == 1:                \n",
    "                            combined_dataset_self_promo['Adcal_DD'] = (1+disc_change_fil)*combined_dataset_self_promo['Adcal_DD']\n",
    "                            combined_dataset_self_promo.loc[:,'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_self_promo.loc[:,'EDV.Price']\n",
    "                        elif i == 2:\n",
    "                            combined_dataset_self_promo['Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_self_promo['Adcal_DD']\n",
    "                            combined_dataset_self_promo.loc[:,'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_self_promo.loc[:,'EDV.Price']          \n",
    "                    else:                \n",
    "                        if i == 1:                \n",
    "                            # Discount change\n",
    "                            combined_dataset_self_promo.loc[combined_dataset_self_promo['Test_period'].isin(test_period_fil),'Adcal_DD'] = (1+disc_change_fil)*combined_dataset_self_promo[combined_dataset_self_promo['Test_period'].isin(test_period_fil)]['Adcal_DD'] \n",
    "                            # Baseline pricing\n",
    "                            combined_dataset_self_promo.loc[(combined_dataset_self_promo['Test_period'].isin(test_period_fil)),'EDV.Price'] = (1+base_price_change_fil)*combined_dataset_self_promo[(combined_dataset_self_promo['Test_period'].isin(test_period_fil))]['EDV.Price']\n",
    "                        elif i == 2:\n",
    "                            # Discount change\n",
    "                            combined_dataset_self_promo.loc[combined_dataset_self_promo['Test_period'].isin(test_period_fil2),'Adcal_DD'] = (1+disc_change_fil2)*combined_dataset_self_promo[combined_dataset_self_promo['Test_period'].isin(test_period_fil2)]['Adcal_DD']             \n",
    "                            # Baseline pricing\n",
    "                            combined_dataset_self_promo.loc[(combined_dataset_self_promo['Test_period'].isin(test_period_fil2)),'EDV.Price'] = (1+base_price_change_fil2)*combined_dataset_self_promo[(combined_dataset_self_promo['Test_period'].isin(test_period_fil2))]['EDV.Price']\n",
    "\n",
    "                #Adding discount depth columns\n",
    "                combined_dataset_self_promo[\"DD_1\"] = [1 if (0.25> x >0.1)\n",
    "                                      else 0 for x in combined_dataset_self_promo[\"Adcal_DD\"]]\n",
    "                combined_dataset_self_promo[\"DD_2\"] = [1 if (x>=0.25)\n",
    "                                                 else 0 for x in combined_dataset_self_promo[\"Adcal_DD\"]]            \n",
    "\n",
    "                # Adcal Price calculation\n",
    "                # In case the discounts go over 100%, limit to 100%\n",
    "                combined_dataset_self_promo.loc[combined_dataset_self_promo.Adcal_DD >= 1, 'Adcal_DD'] = 0.99\n",
    "                combined_dataset_self_promo.loc[combined_dataset_self_promo['Adcal_DD'] >= 0.1, 'Adcal_Price'] = (1-combined_dataset_self_promo.loc[combined_dataset_self_promo['Adcal_DD']>=0.1,'Adcal_DD'])*combined_dataset_self_promo.loc[combined_dataset_self_promo['Adcal_DD']>=0.1,'EDV.Price']\n",
    "                # Adcal_DD < 10 then Adcal Price = EDV Price            \n",
    "                combined_dataset_self_promo.loc[combined_dataset_self_promo['Adcal_DD'] < 0.1, 'Adcal_Price'] = combined_dataset_self_promo.loc[combined_dataset_self_promo['Adcal_DD'] < 0.1, 'EDV.Price']\n",
    "\n",
    "                # Dropping adcal DD as DD1 & DD2 are added\n",
    "                combined_dataset_self_promo.drop([\"Adcal_DD\"], inplace=True, axis=1)\n",
    "\n",
    "                complete_dataset_self_promo = combined_dataset_self_promo.copy()\n",
    "                complete_dataset_self_promo = complete_dataset_self_promo.loc[complete_dataset_self_promo[\"Eq.Unit.Sales\"].notnull()]\n",
    "\n",
    "                complete_dataset_self_promo = complete_dataset_self_promo.loc[((complete_dataset_self_promo[\"Week\"] >= \"2017-01\") & \n",
    "                                                         (complete_dataset_self_promo[\"Week\"] <= \"2019-52\"))]    \n",
    "\n",
    "                complete_dataset_self_promo['Adcal_Price'] = np.log(complete_dataset_self_promo['Adcal_Price'])\n",
    "                complete_dataset_self_promo.sort_values(by=['Banner','Product','Week'], inplace=True)           \n",
    "\n",
    "                Test_periods = ['2019Q1','2019Q2','2019Q3','2019Q4']\n",
    "\n",
    "                for Test_period in Test_periods:\n",
    "                    if (Test_period>end_week.children[8].value) | (Test_period > Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].max()):\n",
    "                        continue\n",
    "                    if (Test_period<start_week.children[8].value) | (Test_period < Volume_dataset_all_reg.loc[Volume_dataset_all_reg.Product == product_fil, 'Test_period'].min()):\n",
    "                        continue                \n",
    "\n",
    "                    #Filtering test weeks\n",
    "                    Test_weeks = Test_week_list[Test_period]\n",
    "\n",
    "                    # Dividing train & test data\n",
    "                    complete_train_data_set = complete_dataset_adcal.loc[~complete_dataset_adcal[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                    complete_test_data_set_adcal = complete_dataset_adcal_test.loc[complete_dataset_adcal_test[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                    complete_test_data_set_edv = complete_dataset_edv.loc[complete_dataset_edv[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                    complete_test_data_set_other_promo = complete_dataset_other_promo.loc[complete_dataset_other_promo[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "                    complete_test_data_set_self_promo = complete_dataset_self_promo.loc[complete_dataset_self_promo[\"Week\"].isin(Test_weeks)].reset_index(drop=True)\n",
    "\n",
    "                    # Dropping unneccessary columns from train dataset\n",
    "                    train_data_set = complete_train_data_set.copy()\n",
    "\n",
    "                    train_data_brand = train_data_set.copy()\n",
    "                    train_data_brand.drop([\"Week\",\"Banner\",\"Product\",\"EDV.Price\",\"Eq.Unit.Sales\",\"Dollar.Sales\",\"brand\"], inplace=True, axis=1)\n",
    "\n",
    "                    # Filtering for New product\n",
    "                    test_data_set = complete_test_data_set_adcal.copy()\n",
    "                    test_data_set_edv = complete_test_data_set_edv.copy()\n",
    "                    test_data_set_other_promo = complete_test_data_set_other_promo.copy()\n",
    "                    test_data_set_self_promo = complete_test_data_set_self_promo.copy()            \n",
    "\n",
    "                    # In case the user doesn't want to upload adcal data\n",
    "                    if toggle_upload.value != 'Yes':\n",
    "\n",
    "                        #################### INCREASING/DECREASING PROMOTIONS FOR BANNER WEEKS COMBINATIONS ####################\n",
    "\n",
    "                        if (promo_change_toggle.value == 'Yes (All Banners)') | (promo_change_toggle.value == 'Yes (Specific Banners)'):\n",
    "                            if (front_change.children[1].value != 0) | (middle_change.children[1].value != 0) | (back_change.children[1].value != 0) | (front_change2.children[1].value != 0) | (middle_change2.children[1].value != 0) | (back_change2.children[1].value != 0) : \n",
    "\n",
    "                                ## FRONT PAGE PROMO CHANGES\n",
    "                                # New Product 1\n",
    "                                if i == 1:\n",
    "                                    if front_change.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_fp['Week'], ban_week_fp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] + 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Front.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Front.Page'] + 1\n",
    "                                    elif front_change.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_fp['Week'], ban_week_fp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] - 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Front.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Front.Page'] - 1\n",
    "\n",
    "                                elif i == 2:\n",
    "                                    if front_change2.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_fp2['Week'], ban_week_fp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] + 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Front.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Front.Page'] + 1\n",
    "                                    elif front_change2.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_fp2['Week'], ban_week_fp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Front.Page'] - 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Front.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Front.Page'] - 1\n",
    "\n",
    "                                ## MIDDLE PAGE PROMO CHANGES\n",
    "                                if i == 1:\n",
    "                                    if middle_change.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_mp['Week'], ban_week_mp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] + 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Middle.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Middle.Page'] + 1\n",
    "                                    elif middle_change.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_mp['Week'], ban_week_mp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] - 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Middle.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Middle.Page'] - 1\n",
    "                                elif i == 2:\n",
    "                                    if middle_change2.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_mp2['Week'], ban_week_mp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] + 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Middle.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Middle.Page'] + 1\n",
    "                                    elif middle_change2.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_mp2['Week'], ban_week_mp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Middle.Page'] - 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Middle.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Middle.Page'] - 1\n",
    "\n",
    "                                ## BACK PAGE PROMO CHANGES\n",
    "                                if i == 1:\n",
    "                                    if back_change.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_bp['Week'], ban_week_bp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] + 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Back.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Back.Page'] + 1\n",
    "                                    elif back_change.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_bp['Week'], ban_week_bp['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] - 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Back.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Back.Page'] - 1\n",
    "                                elif i == 2:\n",
    "                                    if back_change2.children[1].value > 0:\n",
    "                                        # Adding additional promotions in the required banner-weeks\n",
    "                                        for week,banner in zip(ban_week_bp2['Week'], ban_week_bp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] + 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Back.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Back.Page'] + 1\n",
    "                                    elif back_change2.children[1].value < 0:\n",
    "                                        for week,banner in zip(ban_week_bp2['Week'], ban_week_bp2['Banner']):                            \n",
    "                                            test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] = test_data_set.loc[(test_data_set.Week == week) & (test_data_set.Banner == banner), 'Back.Page'] - 1\n",
    "                                            test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Back.Page'] = test_data_set_self_promo.loc[(test_data_set_self_promo.Week == week) & (test_data_set_self_promo.Banner == banner), 'Back.Page'] - 1\n",
    "\n",
    "\n",
    "                    # New Category\n",
    "                    if i == 1:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                            if (cols == category_drop.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                                test_data_set_edv[cols] = 1\n",
    "                                test_data_set_other_promo[cols] = 1\n",
    "                                test_data_set_self_promo[cols] = 1\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                                test_data_set_edv[cols] = 0\n",
    "                                test_data_set_other_promo[cols] = 0\n",
    "                                test_data_set_self_promo[cols] = 0\n",
    "\n",
    "                    if i == 2:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)].Category.unique():\n",
    "                            if (cols == category_drop2.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                                test_data_set_edv[cols] = 1\n",
    "                                test_data_set_other_promo[cols] = 1\n",
    "                                test_data_set_self_promo[cols] = 1\n",
    "\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                                test_data_set_edv[cols] = 0\n",
    "                                test_data_set_other_promo[cols] = 0\n",
    "                                test_data_set_self_promo[cols] = 0\n",
    "\n",
    "                    # New Pack Subtype\n",
    "                    if i == 1:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                            if (cols == pack_subtype_drop.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                                test_data_set_edv[cols] = 1\n",
    "                                test_data_set_other_promo[cols] = 1\n",
    "                                test_data_set_self_promo[cols] = 1\n",
    "\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                                test_data_set_edv[cols] = 0\n",
    "                                test_data_set_other_promo[cols] = 0\n",
    "                                test_data_set_self_promo[cols] = 0\n",
    "\n",
    "                    if i == 2:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['Pack.Subtype'].unique():\n",
    "                            if (cols == pack_subtype_drop2.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                                test_data_set_edv[cols] = 1\n",
    "                                test_data_set_other_promo[cols] = 1\n",
    "                                test_data_set_self_promo[cols] = 1\n",
    "\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                                test_data_set_edv[cols] = 0\n",
    "                                test_data_set_other_promo[cols] = 0\n",
    "                                test_data_set_self_promo[cols] = 0\n",
    "\n",
    "                    # New Pack Content\n",
    "                    if i == 1:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                            if (cols == pack_content_drop.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                                test_data_set_edv[cols] = 1\n",
    "                                test_data_set_other_promo[cols] = 1\n",
    "                                test_data_set_self_promo[cols] = 1\n",
    "\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                                test_data_set_edv[cols] = 0\n",
    "                                test_data_set_other_promo[cols] = 0\n",
    "                                test_data_set_self_promo[cols] = 0\n",
    "\n",
    "                    if i == 2:\n",
    "                        for cols in Volume_dataset_all_reg[(Volume_dataset_all_reg.Region == Region_key) & (Volume_dataset_all_reg.Channel.isin(channel_list)) & (Volume_dataset_all_reg.Test_period >= start_week.children[8].value) & (Volume_dataset_all_reg.Test_period <= end_week.children[8].value)]['PACK_CONTENT'].unique():\n",
    "                            if (cols == pack_content_drop2.value):                      \n",
    "                                test_data_set[cols] = 1\n",
    "                                test_data_set_edv[cols] = 1\n",
    "                                test_data_set_other_promo[cols] = 1\n",
    "                                test_data_set_self_promo[cols] = 1\n",
    "\n",
    "                            else:\n",
    "                                test_data_set[cols] = 0\n",
    "                                test_data_set_edv[cols] = 0\n",
    "                                test_data_set_other_promo[cols] = 0\n",
    "                                test_data_set_self_promo[cols] = 0\n",
    "\n",
    "                    if product_fil == 'TCCC CORE POWER 414 ML BTTL':\n",
    "                        test_data_set = test_data_set.drop_duplicates()\n",
    "                        test_data_set_edv = test_data_set_edv.drop_duplicates()\n",
    "                        test_data_set_other_promo = test_data_set_other_promo.drop_duplicates()\n",
    "                        test_data_set_self_promo = test_data_set_self_promo.drop_duplicates()\n",
    "\n",
    "                    test_data_brand = test_data_set.copy()\n",
    "                    test_data_brand_edv = test_data_set_edv.copy()\n",
    "                    test_data_brand_other_promo = test_data_set_other_promo.copy()\n",
    "                    test_data_brand_self_promo = test_data_set_self_promo.copy()\n",
    "\n",
    "                    if i == 1 :\n",
    "                        # For New SIZE\n",
    "                        test_data_brand['SIZE_ML'] = size.value\n",
    "                        test_data_brand_edv['SIZE_ML'] = size.value\n",
    "                        test_data_brand_other_promo['SIZE_ML'] = size.value\n",
    "                        test_data_brand_self_promo['SIZE_ML'] = size.value\n",
    "\n",
    "                        # For New Count\n",
    "                        test_data_brand['COUNT'] = count.value\n",
    "                        test_data_brand_edv['COUNT'] = count.value\n",
    "                        test_data_brand_other_promo['COUNT'] = count.value\n",
    "                        test_data_brand_self_promo['COUNT'] = count.value\n",
    "\n",
    "                    elif i == 2:\n",
    "                        # For New SIZE\n",
    "                        test_data_brand['SIZE_ML'] = size2.value\n",
    "                        test_data_brand_edv['SIZE_ML'] = size2.value\n",
    "                        test_data_brand_other_promo['SIZE_ML'] = size2.value\n",
    "                        test_data_brand_self_promo['SIZE_ML'] = size2.value\n",
    "\n",
    "\n",
    "                        # For New Count\n",
    "                        test_data_brand['COUNT'] = count2.value\n",
    "                        test_data_brand_edv['COUNT'] = count2.value\n",
    "                        test_data_brand_other_promo['COUNT'] = count2.value\n",
    "                        test_data_brand_self_promo['COUNT'] = count2.value\n",
    "\n",
    "                    if(test_data_brand.shape[0]==0):\n",
    "                        continue\n",
    "\n",
    "                    # Dropping unneccessary columns from test dataset\n",
    "                    test_data_brand.drop([\"Week\",\"Banner\",\"Product\",\"EDV.Price\",\"Eq.Unit.Sales\",\"Dollar.Sales\",\"brand\"], inplace=True, axis=1)\n",
    "                    test_data_brand_edv.drop([\"Week\",\"Banner\",\"Product\",\"EDV.Price\",\"Eq.Unit.Sales\",\"Dollar.Sales\",\"brand\"], inplace=True, axis=1)\n",
    "                    test_data_brand_other_promo.drop([\"Week\",\"Banner\",\"Product\",\"EDV.Price\",\"Eq.Unit.Sales\",\"Dollar.Sales\",\"brand\"], inplace=True, axis=1)\n",
    "                    test_data_brand_self_promo.drop([\"Week\",\"Banner\",\"Product\",\"EDV.Price\",\"Eq.Unit.Sales\",\"Dollar.Sales\",\"brand\"], inplace=True, axis=1)\n",
    "\n",
    "                    #Creating test and train data - independent & dependent columns\n",
    "                    X_train = train_data_brand\n",
    "                    y_train = train_data_set[\"Eq.Unit.Sales\"]\n",
    "                    X_test = test_data_brand\n",
    "                    X_test_edv = test_data_brand_edv\n",
    "                    X_test_other_promo = test_data_brand_other_promo\n",
    "                    X_test_self_promo = test_data_brand_self_promo.copy()\n",
    "                    X_test_dd_promo = test_data_brand_self_promo.copy()\n",
    "                    X_test_dd_promo['Front.Page'] = 0\n",
    "                    X_test_dd_promo['Middle.Page'] = 0\n",
    "                    X_test_dd_promo['Back.Page'] = 0\n",
    "                    X_test_page_promo = test_data_brand_edv.copy()\n",
    "                    X_test_page_promo['Front.Page'] = X_test_self_promo['Front.Page']\n",
    "                    X_test_page_promo['Middle.Page'] = X_test_self_promo['Middle.Page']\n",
    "                    X_test_page_promo['Back.Page'] = X_test_self_promo['Back.Page']\n",
    "                    y_test = test_data_set[\"Eq.Unit.Sales\"]\n",
    "                    \n",
    "\n",
    "                    # Appending test data\n",
    "                    test_data_set[\"Region\"] = Region_key\n",
    "                    test_data_set[\"Test_period\"] = Test_period\n",
    "\n",
    "                    Test_Data = Test_Data.append(test_data_set, ignore_index = True)\n",
    "\n",
    "                    #Setting seed\n",
    "                    random.seed(42)\n",
    "                    \n",
    "\n",
    "                    if((Region_key == 'EAST') and (Test_period == '2019Q1')):\n",
    "                            rf = model_object_EAST_2019Q1\n",
    "                    elif((Region_key == 'EAST') and (Test_period == '2019Q2')):\n",
    "                            rf = model_object_EAST_2019Q2\n",
    "                    elif((Region_key == 'EAST') and (Test_period == '2019Q3')):\n",
    "                            rf = model_object_EAST_2019Q3\n",
    "                    elif((Region_key == 'EAST') and (Test_period == '2019Q4')):\n",
    "                            rf = model_object_EAST_2019Q4\n",
    "                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q1')):\n",
    "                            rf = model_object_ONTARIO_2019Q1\n",
    "                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q2')):\n",
    "                            rf = model_object_ONTARIO_2019Q2\n",
    "                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q3')):\n",
    "                            rf = model_object_ONTARIO_2019Q3\n",
    "                    elif((Region_key == 'ONTARIO') and (Test_period == '2019Q4')):\n",
    "                            rf = model_object_ONTARIO_2019Q4\n",
    "                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q1')):\n",
    "                            rf = model_object_QUEBEC_2019Q1\n",
    "                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q2')):\n",
    "                            rf = model_object_QUEBEC_2019Q2\n",
    "                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q3')):\n",
    "                            rf = model_object_QUEBEC_2019Q3\n",
    "                    elif((Region_key == 'QUEBEC') and (Test_period == '2019Q4')):\n",
    "                            rf = model_object_QUEBEC_2019Q4\n",
    "                    elif((Region_key == 'WEST') and (Test_period == '2019Q1')):\n",
    "                            rf = model_object_WEST_2019Q1\n",
    "                    elif((Region_key == 'WEST') and (Test_period == '2019Q2')):\n",
    "                            rf = model_object_WEST_2019Q2\n",
    "                    elif((Region_key == 'WEST') and (Test_period == '2019Q3')):\n",
    "                            rf = model_object_WEST_2019Q3\n",
    "                    else:\n",
    "                            rf = model_object_WEST_2019Q4\n",
    "\n",
    "                    #Model predictions\n",
    "                    predictions_rf_adcal = rf.predict(X_test)\n",
    "                    predictions_rf_edv = rf.predict(X_test_edv)\n",
    "                    predictions_rf_other_promo = rf.predict(X_test_other_promo)\n",
    "                    predictions_rf_self_promo = rf.predict(X_test_self_promo)\n",
    "                    predictions_rf_dd_promo = rf.predict(X_test_dd_promo)\n",
    "                    predictions_rf_page_promo = rf.predict(X_test_page_promo)\n",
    "\n",
    "                    #Storing test results\n",
    "                    result = X_test.copy()\n",
    "                    result[\"Predictions_rf_adcal\"] = np.exp(predictions_rf_adcal)\n",
    "                    result[\"Predictions_rf_edv\"] = np.exp(predictions_rf_edv)\n",
    "                    result[\"Predictions_rf_other_promo\"] = np.exp(predictions_rf_other_promo)\n",
    "                    result[\"Predictions_rf_self_promo\"] = np.exp(predictions_rf_self_promo)\n",
    "                    result[\"Predictions_rf_dd_promo\"] = np.exp(predictions_rf_dd_promo)\n",
    "                    result[\"Predictions_rf_page_promo\"] = np.exp(predictions_rf_page_promo)\n",
    "                    result[\"Actuals\"] = np.exp(y_test)\n",
    "\n",
    "                    #Calculating APE for the models        \n",
    "                    result[\"Banner\"] = test_data_set[\"Banner\"]\n",
    "                    result[\"Product\"] = test_data_set[\"Product\"]\n",
    "                    result[\"brand\"] = test_data_set[\"brand\"]\n",
    "                    result[\"Region\"] = Region_key\n",
    "                    result[\"Test_period\"] = Test_period\n",
    "                    result[\"Week\"] = test_data_set[\"Week\"]\n",
    "                    result['iter'] = i\n",
    "                    result['Total_Promo'] = result['Front.Page'] + result['Middle.Page'] + result['Back.Page']\n",
    "                    result['Total_DD'] = result['DD_1'] + result['DD_2']\n",
    "                    result['flag'] = result['Total_Promo'] + result['Total_DD']\n",
    "\n",
    "                    result.loc[result['flag']==2, 'Predictions_rf_page_promo'] = result.loc[result['flag']==2, 'Predictions_rf_edv'] + result.loc[result['flag']==2, 'Predictions_rf_self_promo'] - result.loc[result['flag']==2, 'Predictions_rf_dd_promo']\n",
    "\n",
    "                    result = result[[\"Region\",\"Banner\",\"Product\",\"brand\",\"Test_period\",\"Week\",\"Actuals\",\"Predictions_rf_adcal\",\"Predictions_rf_edv\",\"Predictions_rf_other_promo\",\"Predictions_rf_self_promo\",\"Predictions_rf_dd_promo\",\"Predictions_rf_page_promo\",'iter','Front.Page','Middle.Page','Back.Page','DD_1','DD_2']]\n",
    "\n",
    "                    #Appending results to the final results dataframe\n",
    "                    Test_results_net = Test_results_net.append(result, ignore_index=True) \n",
    "                    \n",
    "                    ## Cann Contibution code ##\n",
    "                    sel_brands_new_product = list(result['brand'].unique())\n",
    "\n",
    "                    Affecting_brand_weeks_final = pd.DataFrame()\n",
    "                    for brand_id in sel_brands_new_product:\n",
    "                            Affecting_brands = find_affecting_brands(brand_id)\n",
    "                            Affecting_brand_weeks = Volume_dataset[(Volume_dataset['brand'].isin(Affecting_brands))\n",
    "                                                          &(Volume_dataset['Adcal_DD']>0.25)\n",
    "                                                          &(Volume_dataset['brand']!=brand_id)\n",
    "                                                          &(Volume_dataset['Week'].isin(Test_weeks))].sort_values(by='Week')\n",
    "                            Affecting_brand_weeks.rename(columns={'brand':'Affecting_brand'}, inplace=True)\n",
    "                            Affecting_brand_weeks['Affected_brand'] = brand_id\n",
    "                            Affecting_brand_weeks_final = Affecting_brand_weeks_final.append(Affecting_brand_weeks[['Week','Affected_brand','Affecting_brand']])\n",
    "\n",
    "                    # Filtering for existing product\n",
    "                    test_data_set_edv_simulation = pd.merge(test_data_set_edv, Affecting_brand_weeks_final, how='inner', left_on=['Week','brand'], right_on=['Week','Affected_brand'])\n",
    "                    test_data_brand_edv_simulation = test_data_set_edv_simulation.copy()\n",
    "\n",
    "                    # # Dropping unneccessary columns from test dataset\n",
    "                    test_data_brand_edv_simulation.drop([\"Week\",\"Banner\",\"Product\",\"EDV.Price\",\"Eq.Unit.Sales\",\"Dollar.Sales\",\"brand\",\"Affected_brand\",\"Affecting_brand\"], inplace=True, axis=1)\n",
    "\n",
    "                    # Filtering for existing product\n",
    "                    Adcal_data = test_data_set.copy()\n",
    "\n",
    "                    columns = list(Adcal_data.columns)\n",
    "                    cross_columns = [s for s in columns if \"Cross\" in s]\n",
    "                    cross_columns.append('Week')\n",
    "                    cross_columns.append('brand')\n",
    "                    Adcal_data = Adcal_data[cross_columns]\n",
    "                    Adcal_data.columns = [str(col) + '_Adcal' for col in Adcal_data.columns]\n",
    "\n",
    "                    test_data_set_edv_simulation_v2 = pd.merge(test_data_set_edv_simulation, Adcal_data, how='inner', left_on=['Week','brand'], right_on=['Week_Adcal','brand_Adcal'])\n",
    "                    test_data_set_edv_simulation_v2 = test_data_set_edv_simulation_v2.reset_index()\n",
    "\n",
    "                    index_list = (list(test_data_set_edv_simulation_v2['index']))\n",
    "                    for index in index_list:\n",
    "                        brand = int(test_data_set_edv_simulation_v2[test_data_set_edv_simulation_v2['index']==index]['Affecting_brand'].unique()[0])\n",
    "                        original_cross = 'Cross_'+str(brand)\n",
    "                        adcal_cross = 'Cross_'+str(brand)+'_Adcal'\n",
    "                        test_data_set_edv_simulation_v2.loc[(test_data_set_edv_simulation_v2['Affecting_brand']==brand)&(test_data_set_edv_simulation_v2['index']==index), original_cross] = test_data_set_edv_simulation_v2.loc[(test_data_set_edv_simulation_v2['Affecting_brand']==brand)&(test_data_set_edv_simulation_v2['index']==index), adcal_cross]\n",
    "\n",
    "                    final_columns = list(test_data_set_edv.columns)\n",
    "                    final_columns.remove('brand')\n",
    "                    final_columns.append('Affected_brand')\n",
    "                    final_columns.append('Affecting_brand')\n",
    "\n",
    "                    test_data_set_edv_simulation_v2 = test_data_set_edv_simulation_v2[final_columns]\n",
    "\n",
    "                    # # Dropping unneccessary columns from test dataset\n",
    "                    test_data_brand_edv_simulation = test_data_set_edv_simulation_v2.copy()\n",
    "                    test_data_brand_edv_simulation.drop([\"Week\",\"Banner\",\"Product\",\"EDV.Price\",\"Eq.Unit.Sales\",\"Dollar.Sales\",\"Affected_brand\",\"Affecting_brand\"], inplace=True, axis=1)\n",
    "\n",
    "                    #Creating test and train data - independent & dependent columns\n",
    "                    X_test_edv_simulation = test_data_brand_edv_simulation\n",
    "\n",
    "                    #Model predictions\n",
    "                    predictions_rf_edv_simulation = rf.predict(X_test_edv_simulation)\n",
    "\n",
    "                    #Storing test results\n",
    "                    result2 = X_test_edv_simulation.copy()\n",
    "                    result2[\"Predictions_rf_edv_simulation\"] = np.exp(predictions_rf_edv_simulation)\n",
    "\n",
    "                    result2[\"Actuals\"] = np.exp(y_test)\n",
    "\n",
    "                    #Calculating APE for the models\n",
    "                    result2[\"Affected_brand\"] = test_data_set_edv_simulation_v2[\"Affected_brand\"]\n",
    "                    result2[\"Affecting_brand\"] = test_data_set_edv_simulation_v2[\"Affecting_brand\"]\n",
    "                    result2[\"Banner\"] = test_data_set_edv_simulation_v2[\"Banner\"]\n",
    "                    result2[\"Product\"] = test_data_set_edv_simulation_v2[\"Product\"]\n",
    "                    result2[\"Region\"] = Region_key\n",
    "                    result2[\"Test_period\"] = Test_period\n",
    "                    result2[\"Week\"] = test_data_set_edv_simulation_v2[\"Week\"]\n",
    "                    result2['iter'] = i\n",
    "\n",
    "                    result2 = result2[[\"Region\",\"Banner\",\"Product\",\"Affected_brand\",\"Affecting_brand\",\"Test_period\",\"Week\",\"Actuals\",\"Predictions_rf_edv_simulation\",\"iter\"]]\n",
    "\n",
    "                    Sim_results = pd.merge(result[['Region','brand','Banner','Product','Week','Predictions_rf_edv']], result2[['Region','Affected_brand','Affecting_brand','Banner','Product','Week','Predictions_rf_edv_simulation']], how='inner', on=['Banner','Product','Week'])\n",
    "\n",
    "                    Sim_results = Sim_results[Sim_results['Predictions_rf_edv'] != Sim_results['Predictions_rf_edv_simulation']]\n",
    "\n",
    "                    Sim_results['Diff'] = Sim_results['Predictions_rf_edv'] - Sim_results['Predictions_rf_edv_simulation']\n",
    "                    Sim_results = Sim_results[Sim_results['Diff']>0]\n",
    "                    Sim_results['iter'] = i\n",
    "\n",
    "                    #Appending results to the final results dataframe\n",
    "                    Test_results_net_cann = Test_results_net_cann.append(Sim_results, ignore_index=True)\n",
    "\n",
    "        clear_output()\n",
    "        \n",
    "        # Net gain calculation\n",
    "        Test_results_net['Promo Gain'] = Test_results_net['Predictions_rf_self_promo'] - Test_results_net['Predictions_rf_edv']\n",
    "        Test_results_net['Cann Loss'] = Test_results_net['Predictions_rf_other_promo'] - Test_results_net['Predictions_rf_edv']\n",
    "        Test_results_net['Updated_Cann_loss'] = np.where(Test_results_net['Cann Loss'] > 0, 0, Test_results_net['Cann Loss'])\n",
    "\n",
    "        # Year\n",
    "        Test_results_net['Year'] = Test_results_net.Week.str.slice(0,4)\n",
    "        # Week number\n",
    "        Test_results_net['Week_no'] = Test_results_net.Week.str.slice(5,7)\n",
    "        # Converting into year-week-days\n",
    "        Test_results_net['year_week_ts'] = Test_results_net.apply(lambda row: year_week(row.Year, row.Week_no), axis=1)\n",
    "        Test_results_net['Month'] = pd.DatetimeIndex(Test_results_net['year_week_ts']).month\n",
    "        Test_results_net['Year_month'] = Test_results_net['Year'].astype(str) + '-' + Test_results_net['Month'].astype(str)\n",
    "\n",
    "        # Iteration 1   \n",
    "        if toggle_np2.value != 'Yes':\n",
    "            # Calculating net gain on year-month level\n",
    "            Test_results_net_gain1 = Test_results_net[Test_results_net.iter == 1].groupby(['Year_month','Month'])['Actuals','Predictions_rf_adcal','Predictions_rf_edv','Predictions_rf_other_promo','Predictions_rf_self_promo','Predictions_rf_dd_promo','Predictions_rf_page_promo','Promo Gain','Updated_Cann_loss'].sum().reset_index().sort_values('Month')\n",
    "            Test_results_net_gain1['Net Gain'] = Test_results_net_gain1['Updated_Cann_loss'] + Test_results_net_gain1['Promo Gain']\n",
    "            Test_results_net_gain1['% Net Gain'] = 100*Test_results_net_gain1['Net Gain']/Test_results_net_gain1['Predictions_rf_adcal']\n",
    "\n",
    "            Test_results_net_gain1['Page_Promo_Gain'] = round(Test_results_net_gain1['Predictions_rf_page_promo'] - Test_results_net_gain1['Predictions_rf_edv'], 2)\n",
    "            Test_results_net_gain1['Price_Promo_Gain'] = round(Test_results_net_gain1['Predictions_rf_dd_promo'] - Test_results_net_gain1['Predictions_rf_edv'], 2)\n",
    "            Test_results_net_gain1['Page_Promo_Gain'] = Test_results_net_gain1['Page_Promo_Gain'].abs()\n",
    "            Test_results_net_gain1['Price_Promo_Gain'] = Test_results_net_gain1['Price_Promo_Gain'].abs()\n",
    "            Test_results_net_gain1['Page Promo Gain %'] = round((Test_results_net_gain1['Page_Promo_Gain']/(Test_results_net_gain1['Page_Promo_Gain']+Test_results_net_gain1['Price_Promo_Gain']))*100,1)\n",
    "            Test_results_net_gain1['Price Promo Gain %'] = round((Test_results_net_gain1['Price_Promo_Gain']/(Test_results_net_gain1['Page_Promo_Gain']+Test_results_net_gain1['Price_Promo_Gain']))*100,1)\n",
    "\n",
    "            # Net Gain KPI\n",
    "            net_gain_kpi = {}\n",
    "            overall_net_gain_percent_np1 = round((Test_results_net_gain1['Net Gain'].sum()/Test_results_net_gain1['Predictions_rf_adcal'].sum())*100,1)\n",
    "            net_gain_kpi['New Product Overall Net Gain %'] = overall_net_gain_percent_np1\n",
    "            net_gain_kpi_df1 = pd.DataFrame(net_gain_kpi, index=[''])\n",
    "            display(HTML(net_gain_kpi_df1.to_html(index=False)))\n",
    "            ################## NET GAIN PLOT ##################                \n",
    "            x1 = np.arange(len(list(Test_results_net_gain1['Year_month'])))  # the label locations            \n",
    "\n",
    "            fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize = (25,8))\n",
    "            rects1 = ax1.plot(x1, list(Test_results_net_gain1['% Net Gain']),label='% Net Gain', marker = 'o')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('Net Gain %')\n",
    "            ax1.set_xlabel('Year Month')\n",
    "            ax1.set_title('New Product Net Gain')\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(Test_results_net_gain1['Year_month']), fontsize=15,rotation=45)\n",
    "            ax1.legend(loc = 'upper left')\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "            # ax1.bar_label(rects1, padding=5, labels=list(Test_results_net_gain1['% Net Gain'].round(1)),label_type = 'center')\n",
    "            for i,j in Test_results_net_gain1['% Net Gain'].reset_index()['% Net Gain'].round(0).items():\n",
    "                ax1.annotate(str(j) + \"%\", xy=(i, j))\n",
    "\n",
    "            plt.show();\n",
    "\n",
    "            #Promo Gain KPI\n",
    "            promo_gain_kpi = {}\n",
    "            promo_gain_kpi['New Product Total Promo Gain'] = human_format(Test_results_net_gain1['Promo Gain'].sum().astype('int'))\n",
    "            promo_gain_kpi_df1 = pd.DataFrame(promo_gain_kpi, index=[''])\n",
    "            display(HTML(promo_gain_kpi_df1.to_html(index=False)))\n",
    "\n",
    "            ## Promo Gain breakdown Plots ##\n",
    "            fig, ax1 = plt.subplots(figsize=(12,5))\n",
    "            a1=ax1.bar(Test_results_net_gain1['Year_month'], Test_results_net_gain1['Price Promo Gain %'], color='orange', bottom=Test_results_net_gain1['Page Promo Gain %'], label='New Product 1 Price Promo Gain %')\n",
    "            a2=ax1.bar(Test_results_net_gain1['Year_month'], Test_results_net_gain1['Page Promo Gain %'], color='yellow', label='New Product 1 Page Promo Gain %')\n",
    "            for i,j in Test_results_net_gain1['Price Promo Gain %'].reset_index()['Price Promo Gain %'].items():\n",
    "                ax1.annotate(str(j) + \"%\", xy=(i, 100-j/2),ha='center')            \n",
    "\n",
    "            for i,j in Test_results_net_gain1['Page Promo Gain %'].reset_index()['Page Promo Gain %'].items():\n",
    "                ax1.annotate(str(j) + \"%\", xy=(i, j/2),ha='center')\n",
    "            ax1.set_title('New Product Promo Gain breakdown')\n",
    "            ax1.set_ylabel('Promo Gain breakdown %')\n",
    "            ax1.set_xlabel('Year Month')\n",
    "            ax1.set_xticklabels(Test_results_net_gain1['Year_month'], rotation=45, ha='right')\n",
    "            ax1.set_ylim(0,130)\n",
    "            ax1.legend(loc='upper right')\n",
    "            ax1.get_yaxis().set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "            plt.show();\n",
    "\n",
    "            #Cannibalization loss KPI\n",
    "            cann_loss_kpi = {}\n",
    "            cann_loss_kpi['New Product Total Cannibalization loss due to Affecting Products Promotions'] = human_format(Test_results_net_gain1['Updated_Cann_loss'].sum().astype('int'))\n",
    "            cann_loss_kpi_df1 = pd.DataFrame(cann_loss_kpi, index=[''])\n",
    "            display(HTML(cann_loss_kpi_df1.to_html(index=False)))\n",
    "\n",
    "            print(\"Note: Cannibalization loss is measured due to Affecting Products Promotions\")\n",
    "\n",
    "            ## Cannibalization Contribution\n",
    "            Sim_results_final_BP1 = pd.merge(Test_results_net_cann[Test_results_net_cann['iter']==1][['Affecting_brand','Diff']], brand_mapping[['brand','Banner','Product']], how='left', left_on='Affecting_brand', right_on='brand')\n",
    "            cann_contribution1 = Sim_results_final_BP1.groupby('Product')['Diff'].sum().reset_index()\n",
    "            cann_contribution1['Diff_sum'] = cann_contribution1['Diff'].sum()\n",
    "            cann_contribution1['Cann_Contribution'] = round((cann_contribution1['Diff'] / cann_contribution1['Diff_sum'])*100,1)\n",
    "            final_cann_contribution1 = cann_contribution1.sort_values(by='Cann_Contribution', ascending=False).head(10)\n",
    "\n",
    "            ## Cannibalization Contribution Plots ##\n",
    "            fig, ax1 =  plt.subplots(figsize=(12,5))\n",
    "            a1=ax1.bar(final_cann_contribution1['Product'], final_cann_contribution1['Cann_Contribution'], color='dodgerblue')\n",
    "            for i,j in final_cann_contribution1['Cann_Contribution'].reset_index()['Cann_Contribution'].items():\n",
    "                ax1.annotate(str(j) + \"%\", xy=(i, j/2),ha='center')\n",
    "            ax1.set_title('New Product Cannibalization loss Top 10 Affecting Products Contribution %')\n",
    "            ax1.set_ylabel('Cannibalization loss Contribution %')\n",
    "            ax1.set_xlabel('Product')\n",
    "            ax1.set_xticklabels(final_cann_contribution1['Product'], rotation=45, ha='right')\n",
    "            ax1.get_yaxis().set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "            plt.show();                        \n",
    "        \n",
    "        # Iteration 1 & Iteration 2 combined\n",
    "        if toggle_np2.value == 'Yes':\n",
    "            # Calculating net gain on year-month level\n",
    "            Test_results_net_gain1 = Test_results_net[Test_results_net.iter == 1].groupby(['Year_month','Month'])['Actuals','Predictions_rf_adcal','Predictions_rf_edv','Predictions_rf_other_promo','Predictions_rf_self_promo','Predictions_rf_dd_promo','Predictions_rf_page_promo','Promo Gain','Updated_Cann_loss'].sum().reset_index().sort_values('Month')\n",
    "            Test_results_net_gain2 = Test_results_net[Test_results_net.iter == 2].groupby(['Year_month','Month'])['Actuals','Predictions_rf_adcal','Predictions_rf_edv','Predictions_rf_other_promo','Predictions_rf_self_promo','Predictions_rf_dd_promo','Predictions_rf_page_promo','Promo Gain','Updated_Cann_loss'].sum().reset_index().sort_values('Month')\n",
    "            Test_results_net_gain1['Net Gain'] = Test_results_net_gain1['Updated_Cann_loss'] + Test_results_net_gain1['Promo Gain']\n",
    "            Test_results_net_gain2['Net Gain'] = Test_results_net_gain2['Updated_Cann_loss'] + Test_results_net_gain2['Promo Gain']\n",
    "            Test_results_net_gain1['% Net Gain'] = 100*Test_results_net_gain1['Net Gain']/Test_results_net_gain1['Predictions_rf_adcal']\n",
    "            Test_results_net_gain2['% Net Gain'] = 100*Test_results_net_gain2['Net Gain']/Test_results_net_gain2['Predictions_rf_adcal']\n",
    "\n",
    "            Test_results_net_gain1['Page_Promo_Gain'] = round(Test_results_net_gain1['Predictions_rf_page_promo'] - Test_results_net_gain1['Predictions_rf_edv'], 2)\n",
    "            Test_results_net_gain1['Price_Promo_Gain'] = round(Test_results_net_gain1['Predictions_rf_dd_promo'] - Test_results_net_gain1['Predictions_rf_edv'], 2)\n",
    "            Test_results_net_gain1['Page_Promo_Gain'] = Test_results_net_gain1['Page_Promo_Gain'].abs()\n",
    "            Test_results_net_gain1['Price_Promo_Gain'] = Test_results_net_gain1['Price_Promo_Gain'].abs()\n",
    "            Test_results_net_gain1['Page Promo Gain %'] = round((Test_results_net_gain1['Page_Promo_Gain']/Test_results_net_gain1['Promo Gain'])*100,1)\n",
    "            Test_results_net_gain1['Price Promo Gain %'] = round((Test_results_net_gain1['Price_Promo_Gain']/Test_results_net_gain1['Promo Gain'])*100,1)\n",
    "\n",
    "            Test_results_net_gain2['Page_Promo_Gain'] = round(Test_results_net_gain2['Predictions_rf_page_promo'] - Test_results_net_gain2['Predictions_rf_edv'], 2)\n",
    "            Test_results_net_gain2['Price_Promo_Gain'] = round(Test_results_net_gain2['Predictions_rf_dd_promo'] - Test_results_net_gain2['Predictions_rf_edv'], 2)\n",
    "            Test_results_net_gain2['Page_Promo_Gain'] = Test_results_net_gain2['Page_Promo_Gain'].abs()\n",
    "            Test_results_net_gain2['Price_Promo_Gain'] = Test_results_net_gain2['Price_Promo_Gain'].abs()\n",
    "            Test_results_net_gain2['Page Promo Gain %'] = round((Test_results_net_gain2['Page_Promo_Gain']/(Test_results_net_gain2['Page_Promo_Gain']+Test_results_net_gain2['Price_Promo_Gain']))*100,1)\n",
    "            Test_results_net_gain2['Price Promo Gain %'] = round((Test_results_net_gain2['Price_Promo_Gain']/(Test_results_net_gain2['Page_Promo_Gain']+Test_results_net_gain2['Price_Promo_Gain']))*100,1)\n",
    "\n",
    "            # Net Gain KPI\n",
    "            net_gain_kpi = {}\n",
    "            overall_net_gain_percent_np1 = round((Test_results_net_gain1['Net Gain'].sum()/Test_results_net_gain1['Predictions_rf_adcal'].sum())*100,1)\n",
    "            overall_net_gain_percent_np2 = round((Test_results_net_gain2['Net Gain'].sum()/Test_results_net_gain2['Predictions_rf_adcal'].sum())*100,1)\n",
    "            net_gain_kpi['New Product 1 Overall Net Gain %'] = overall_net_gain_percent_np1\n",
    "            net_gain_kpi['New Product 2 Overall Net Gain %'] = overall_net_gain_percent_np2\n",
    "            net_gain_kpi_df2 = pd.DataFrame(net_gain_kpi, index=[''])\n",
    "            display(HTML(net_gain_kpi_df2.to_html(index=False)))\n",
    "\n",
    "            ################## NET GAIN PLOT ##################                \n",
    "            x1 = np.arange(len(list(Test_results_net_gain1['Year_month'])))  # the label locations            \n",
    "            x2 = np.arange(len(list(Test_results_net_gain2['Year_month'])))  # the label locations\n",
    "\n",
    "            fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize = (25,8))\n",
    "            rects1 = ax1.plot(x1, list(Test_results_net_gain1['% Net Gain']),label='% Net Gain (New Product 1)', marker = 'o')\n",
    "            rects2 = ax1.plot(x1, list(Test_results_net_gain2['% Net Gain']),label='% Net Gain (New Product 2)', marker = 'o')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax1.set_ylabel('% Net Gain', fontsize=20)\n",
    "            ax1.set_xlabel('Year Month', fontsize=20)\n",
    "            ax1.set_title('Net Gain for '+ product_fil, fontsize=25)\n",
    "            ax1.set_xticks(x1)\n",
    "            ax1.set_xticklabels(list(Test_results_net_gain2['Year_month']), fontsize=15,rotation=90)\n",
    "            ax1.legend(loc = 'upper left')\n",
    "\n",
    "            ax1.get_yaxis().set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "            for i,j in Test_results_net_gain1['% Net Gain'].reset_index()['% Net Gain'].round(0).items():\n",
    "                ax1.annotate(str(j) + \"%\", xy=(i, j))\n",
    "            for i,j in Test_results_net_gain2['% Net Gain'].reset_index()['% Net Gain'].round(0).items():\n",
    "                ax1.annotate(str(j) + \"%\", xy=(i, j))\n",
    "\n",
    "            plt.show();\n",
    "\n",
    "            #Promo Gain KPI\n",
    "            promo_gain_kpi = {}\n",
    "            promo_gain_kpi['New Product 1 Total Promo Gain'] = human_format(Test_results_net_gain1['Promo Gain'].sum().astype('int'))\n",
    "            promo_gain_kpi['New Product 2 Total Promo Gain'] = human_format(Test_results_net_gain2['Promo Gain'].sum().astype('int'))\n",
    "            promo_gain_kpi_df2 = pd.DataFrame(promo_gain_kpi, index=[''])\n",
    "            display(HTML(promo_gain_kpi_df2.to_html(index=False)))\n",
    "\n",
    "            ## Promo Gain breakdown Plots ##\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            a1=ax1.bar(Test_results_net_gain1['Year_month'], Test_results_net_gain1['Price Promo Gain %'], color='orange', bottom=Test_results_net_gain1['Page Promo Gain %'], label='New Product 1 Price Promo Gain %')\n",
    "            a2=ax1.bar(Test_results_net_gain1['Year_month'], Test_results_net_gain1['Page Promo Gain %'], color='yellow', label='New Product 1 Page Promo Gain %')\n",
    "            for i,j in Test_results_net_gain1['Price Promo Gain %'].reset_index()['Price Promo Gain %'].items():\n",
    "                ax1.annotate(str(j) + \"%\", xy=(i, 100-j/2),ha='center')            \n",
    "\n",
    "            for i,j in Test_results_net_gain1['Page Promo Gain %'].reset_index()['Page Promo Gain %'].items():\n",
    "                ax1.annotate(str(j) + \"%\", xy=(i, j/2),ha='center')\n",
    "            ax1.set_title('New Product 1 Promo Gain breakdown')\n",
    "            ax1.set_ylabel('Promo Gain breakdown %')\n",
    "            ax1.set_xlabel('Year Month')\n",
    "            ax1.set_xticklabels(Test_results_net_gain1['Year_month'], rotation=45, ha='right')\n",
    "            ax1.set_ylim(0,120)\n",
    "            ax1.legend(loc='upper right')\n",
    "            ax1.get_yaxis().set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "            a3=ax2.bar(Test_results_net_gain2['Year_month'], Test_results_net_gain2['Price Promo Gain %'], color='orange', bottom=Test_results_net_gain2['Page Promo Gain %'], label='New Product 2 Price Promo Gain %')\n",
    "            a4=ax2.bar(Test_results_net_gain2['Year_month'], Test_results_net_gain2['Page Promo Gain %'], color='yellow', label='New Product 2 Page Promo Gain %')\n",
    "            for i,j in Test_results_net_gain2['Price Promo Gain %'].reset_index()['Price Promo Gain %'].items():\n",
    "                ax2.annotate(str(j) + \"%\", xy=(i, 100-j/2),ha='center')            \n",
    "\n",
    "            for i,j in Test_results_net_gain2['Page Promo Gain %'].reset_index()['Page Promo Gain %'].items():\n",
    "                ax2.annotate(str(j) + \"%\", xy=(i, j/2),ha='center')\n",
    "            ax2.set_title('New Product 2 Promo Gain breakdown')\n",
    "            ax2.set_ylabel('Promo Gain breakdown %')\n",
    "            ax2.set_xlabel('Year Month')\n",
    "            ax2.set_xticklabels(Test_results_net_gain2['Year_month'], rotation=45, ha='right')\n",
    "            ax2.set_ylim(0,120)\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.get_yaxis().set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "            plt.show();\n",
    "\n",
    "            ## Cannibalization Contribution ##\n",
    "            Sim_results_final_BP1 = pd.merge(Test_results_net_cann[Test_results_net_cann['iter']==1][['Affecting_brand','Diff']], brand_mapping[['brand','Banner','Product']], how='left', left_on='Affecting_brand', right_on='brand')\n",
    "            cann_contribution1 = Sim_results_final_BP1.groupby('Product')['Diff'].sum().reset_index()\n",
    "            cann_contribution1['Diff_sum'] = cann_contribution1['Diff'].sum()\n",
    "            cann_contribution1['Cann_Contribution'] = round((cann_contribution1['Diff'] / cann_contribution1['Diff_sum'])*100,1)\n",
    "            final_cann_contribution1 = cann_contribution1.sort_values(by='Cann_Contribution', ascending=False).head(10)\n",
    "\n",
    "            #### ## Cannibalization Contribution\n",
    "            Sim_results_final_BP2 = pd.merge(Test_results_net_cann[Test_results_net_cann['iter']==2][['Affecting_brand','Diff']], brand_mapping[['brand','Banner','Product']], how='left', left_on='Affecting_brand', right_on='brand')\n",
    "            cann_contribution2 = Sim_results_final_BP2.groupby('Product')['Diff'].sum().reset_index()\n",
    "            cann_contribution2['Diff_sum'] = cann_contribution2['Diff'].sum()\n",
    "            cann_contribution2['Cann_Contribution'] = round((cann_contribution2['Diff'] / cann_contribution2['Diff_sum'])*100,1)\n",
    "            final_cann_contribution2 = cann_contribution2.sort_values(by='Cann_Contribution', ascending=False).head(10)\n",
    "\n",
    "            #Cannibalization loss KPI\n",
    "            cann_loss_kpi = {}\n",
    "            cann_loss_kpi['New Product 1 Total Cannibalization loss due to Affecting Products Promotions'] = human_format(Test_results_net_gain1['Updated_Cann_loss'].sum().astype('int'))\n",
    "            cann_loss_kpi['New Product 2 Total Cannibalization loss due to Affecting Products Promotions'] = human_format(Test_results_net_gain2['Updated_Cann_loss'].sum().astype('int'))\n",
    "            cann_loss_kpi_df2 = pd.DataFrame(cann_loss_kpi, index=[''])\n",
    "            display(HTML(cann_loss_kpi_df2.to_html(index=False)))\n",
    "\n",
    "            print(\"Note: Cannibalization loss is measured due to Affecting Products Promotions\")\n",
    "\n",
    "            ## Cannibalization Contribution Plots ##\n",
    "            fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,8), tight_layout = True)\n",
    "            a1=ax1.bar(final_cann_contribution1['Product'], final_cann_contribution1['Cann_Contribution'], color='dodgerblue')\n",
    "            for i,j in final_cann_contribution1['Cann_Contribution'].reset_index()['Cann_Contribution'].items():\n",
    "                ax1.annotate(str(j) + \"%\", xy=(i, j/2),ha='center')\n",
    "            ax1.set_title('New Product 1 Cannibalization loss Top 10 Affecting Products Contribution %')\n",
    "            ax1.set_ylabel('Cannibalization loss Contribution %')\n",
    "            ax1.set_xlabel('Product')\n",
    "            ax1.set_xticklabels(final_cann_contribution1['Product'], rotation=45, ha='right')\n",
    "            ax1.get_yaxis().set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "            a2=ax2.bar(final_cann_contribution2['Product'], final_cann_contribution2['Cann_Contribution'], color='dodgerblue')\n",
    "            for i,j in final_cann_contribution2['Cann_Contribution'].reset_index()['Cann_Contribution'].items():\n",
    "                ax2.annotate(str(j) + \"%\", xy=(i, j/2),ha='center')\n",
    "            ax2.set_title('New Product 2 Cannibalization loss Top 10 Affecting Products Contribution %')\n",
    "            ax2.set_ylabel('Cannibalization loss Contribution %')\n",
    "            ax2.set_xlabel('Product')\n",
    "            ax2.set_xticklabels(final_cann_contribution2['Product'], rotation=45, ha='right')\n",
    "            ax2.get_yaxis().set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "            plt.show();                      \n",
    "        \n",
    "    else:\n",
    "        print(\"Product is not present in that Region/Channel for selected period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
